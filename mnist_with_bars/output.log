Namespace(arch='res20s', batch_size=128, checkpoint=None, core_set_method='both', data='data/mnist', data_prune='zero_out', data_rate=0.2, dataset='mnist', decreasing_lr='91,136', eb_eps=0.08, epochs=182, gpu=0, lr=0.1, momentum=0.9, print_freq=50, prune_type='rewind_lt', pruning_times=16, queue_length=5, rate=0.2, resume=False, rewind_epoch=2, save_dir='mnist_with_bars', seed=None, split_file='npy_files/mnist-train-val.npy', threshold=0, warmup=0, weight_decay=0.0001)
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw/train-images-idx3-ubyte.gz
Extracting data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz
Extracting data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to data/mnist/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/mnist/MNIST/raw

return train dataset:  60000
build model: resnet20
Train number for dataset: 50000
######################################## Start Standard Training Iterative Pruning ########################################
******************************************
pruning state 0
* remain parameters = 267408
******************************************
* remain weight =  100.0 %
0.1
* training images number = 50000
Epoch: [0][0/391]	Loss 3.3591 (3.3591)	Accuracy 8.594 (8.594)	Time 0.12
Epoch: [0][50/391]	Loss 0.7275 (1.3631)	Accuracy 77.344 (54.948)	Time 2.73
Epoch: [0][100/391]	Loss 0.5242 (0.8907)	Accuracy 85.938 (70.831)	Time 2.70
Epoch: [0][150/391]	Loss 0.2559 (0.6863)	Accuracy 91.406 (77.571)	Time 2.80
Epoch: [0][200/391]	Loss 0.2427 (0.5666)	Accuracy 92.188 (81.549)	Time 2.72
Epoch: [0][250/391]	Loss 0.1310 (0.4883)	Accuracy 96.875 (84.160)	Time 2.70
Epoch: [0][300/391]	Loss 0.2583 (0.4323)	Accuracy 92.969 (86.041)	Time 2.61
Epoch: [0][350/391]	Loss 0.0993 (0.3914)	Accuracy 97.656 (87.378)	Time 2.61
train_accuracy 88.302
Test: [0/79]	Loss 0.1722 (0.1722)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.1683 (0.1973)	Accuracy 93.750 (93.857)
valid_accuracy 94.460
Test: [0/79]	Loss 0.1031 (0.1031)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0860 (0.2472)	Accuracy 96.875 (92.004)
valid_accuracy 93.110
Epoch #0: Train Accuracy: 88.302, Validation Accuracy: 94.46, Test Accuracy: 93.11
Apply Unstructured L1 Pruning
Remove Pruning
0.1
Epoch: [1][0/391]	Loss 0.0281 (0.0281)	Accuracy 98.438 (98.438)	Time 0.06
Epoch: [1][50/391]	Loss 0.2009 (0.1086)	Accuracy 94.531 (96.369)	Time 2.60
Epoch: [1][100/391]	Loss 0.1055 (0.1068)	Accuracy 96.094 (96.542)	Time 2.61
Epoch: [1][150/391]	Loss 0.1398 (0.1094)	Accuracy 97.656 (96.482)	Time 2.68
Epoch: [1][200/391]	Loss 0.0937 (0.1045)	Accuracy 95.312 (96.665)	Time 2.64
Epoch: [1][250/391]	Loss 0.1138 (0.1019)	Accuracy 96.094 (96.747)	Time 2.66
Epoch: [1][300/391]	Loss 0.1126 (0.1009)	Accuracy 96.094 (96.779)	Time 2.74
Epoch: [1][350/391]	Loss 0.0761 (0.0991)	Accuracy 96.875 (96.808)	Time 2.72
train_accuracy 96.886
Test: [0/79]	Loss 0.1288 (0.1288)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.1026 (0.1899)	Accuracy 96.094 (94.271)
valid_accuracy 94.700
Test: [0/79]	Loss 0.1832 (0.1832)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.0940 (0.2382)	Accuracy 98.438 (93.030)
valid_accuracy 94.070
Epoch #1: Train Accuracy: 96.886, Validation Accuracy: 94.7, Test Accuracy: 94.07
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.0848291739821434
0.1
Epoch: [2][0/391]	Loss 0.1265 (0.1265)	Accuracy 96.875 (96.875)	Time 0.06
Epoch: [2][50/391]	Loss 0.0340 (0.0811)	Accuracy 99.219 (97.564)	Time 2.50
Epoch: [2][100/391]	Loss 0.0874 (0.0788)	Accuracy 98.438 (97.502)	Time 2.60
Epoch: [2][150/391]	Loss 0.0678 (0.0777)	Accuracy 98.438 (97.506)	Time 2.56
Epoch: [2][200/391]	Loss 0.0219 (0.0759)	Accuracy 99.219 (97.540)	Time 2.51
Epoch: [2][250/391]	Loss 0.0305 (0.0730)	Accuracy 98.438 (97.610)	Time 2.58
Epoch: [2][300/391]	Loss 0.0844 (0.0725)	Accuracy 96.875 (97.617)	Time 2.61
Epoch: [2][350/391]	Loss 0.0626 (0.0742)	Accuracy 96.875 (97.594)	Time 2.54
train_accuracy 97.566
Test: [0/79]	Loss 0.0499 (0.0499)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1239 (0.0849)	Accuracy 96.094 (97.457)
valid_accuracy 97.710
Test: [0/79]	Loss 0.0319 (0.0319)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0150 (0.0981)	Accuracy 99.219 (97.013)
valid_accuracy 97.560
Epoch #2: Train Accuracy: 97.566, Validation Accuracy: 97.71, Test Accuracy: 97.56
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.06995303183794022
0.1
Epoch: [3][0/391]	Loss 0.1308 (0.1308)	Accuracy 96.094 (96.094)	Time 0.06
Epoch: [3][50/391]	Loss 0.0677 (0.0632)	Accuracy 98.438 (98.055)	Time 2.67
Epoch: [3][100/391]	Loss 0.1285 (0.0647)	Accuracy 97.656 (97.973)	Time 2.77
Epoch: [3][150/391]	Loss 0.0395 (0.0613)	Accuracy 98.438 (98.039)	Time 3.00
Epoch: [3][200/391]	Loss 0.0469 (0.0597)	Accuracy 98.438 (98.111)	Time 2.65
Epoch: [3][250/391]	Loss 0.0921 (0.0612)	Accuracy 96.875 (98.042)	Time 2.66
Epoch: [3][300/391]	Loss 0.0744 (0.0614)	Accuracy 96.875 (98.014)	Time 2.60
Epoch: [3][350/391]	Loss 0.0698 (0.0626)	Accuracy 97.656 (97.988)	Time 2.61
train_accuracy 97.974
Test: [0/79]	Loss 0.0350 (0.0350)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0423 (0.0840)	Accuracy 97.656 (97.411)
valid_accuracy 97.700
Test: [0/79]	Loss 0.0614 (0.0614)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0275 (0.0952)	Accuracy 98.438 (96.936)
valid_accuracy 97.420
Epoch #3: Train Accuracy: 97.974, Validation Accuracy: 97.7, Test Accuracy: 97.42
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.06526356935501099
0.1
Epoch: [4][0/391]	Loss 0.1039 (0.1039)	Accuracy 96.875 (96.875)	Time 0.05
Epoch: [4][50/391]	Loss 0.0404 (0.0573)	Accuracy 98.438 (98.085)	Time 2.57
Epoch: [4][100/391]	Loss 0.0888 (0.0617)	Accuracy 96.875 (98.051)	Time 2.63
Epoch: [4][150/391]	Loss 0.0209 (0.0589)	Accuracy 99.219 (98.137)	Time 2.64
Epoch: [4][200/391]	Loss 0.0188 (0.0586)	Accuracy 99.219 (98.169)	Time 2.71
Epoch: [4][250/391]	Loss 0.0589 (0.0588)	Accuracy 97.656 (98.164)	Time 2.64
Epoch: [4][300/391]	Loss 0.0353 (0.0584)	Accuracy 98.438 (98.168)	Time 2.75
Epoch: [4][350/391]	Loss 0.0724 (0.0583)	Accuracy 98.438 (98.190)	Time 2.66
train_accuracy 98.178
Test: [0/79]	Loss 0.0150 (0.0150)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0652 (0.0683)	Accuracy 96.875 (97.687)
valid_accuracy 97.930
Test: [0/79]	Loss 0.0292 (0.0292)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0019 (0.0851)	Accuracy 100.000 (97.166)
valid_accuracy 97.780
Epoch #4: Train Accuracy: 98.178, Validation Accuracy: 97.93, Test Accuracy: 97.78
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.060484353452920914
0.1
Epoch: [5][0/391]	Loss 0.0408 (0.0408)	Accuracy 97.656 (97.656)	Time 0.06
Epoch: [5][50/391]	Loss 0.1271 (0.0574)	Accuracy 96.094 (98.131)	Time 2.73
Epoch: [5][100/391]	Loss 0.0278 (0.0557)	Accuracy 99.219 (98.267)	Time 2.64
Epoch: [5][150/391]	Loss 0.0965 (0.0535)	Accuracy 96.875 (98.282)	Time 2.69
Epoch: [5][200/391]	Loss 0.1341 (0.0569)	Accuracy 96.875 (98.165)	Time 2.64
Epoch: [5][250/391]	Loss 0.0113 (0.0569)	Accuracy 100.000 (98.204)	Time 2.62
Epoch: [5][300/391]	Loss 0.0481 (0.0560)	Accuracy 99.219 (98.243)	Time 2.71
Epoch: [5][350/391]	Loss 0.0828 (0.0570)	Accuracy 97.656 (98.235)	Time 2.76
train_accuracy 98.270
Test: [0/79]	Loss 0.0616 (0.0616)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0311 (0.0585)	Accuracy 99.219 (98.192)
valid_accuracy 98.420
Test: [0/79]	Loss 0.0132 (0.0132)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0031 (0.0642)	Accuracy 100.000 (97.917)
valid_accuracy 98.380
Epoch #5: Train Accuracy: 98.27, Validation Accuracy: 98.42, Test Accuracy: 98.38
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.059220366179943085
0.1
Epoch: [6][0/391]	Loss 0.0428 (0.0428)	Accuracy 97.656 (97.656)	Time 0.07
Epoch: [6][50/391]	Loss 0.0747 (0.0434)	Accuracy 96.875 (98.483)	Time 2.70
Epoch: [6][100/391]	Loss 0.0599 (0.0463)	Accuracy 98.438 (98.453)	Time 2.56
Epoch: [6][150/391]	Loss 0.0561 (0.0475)	Accuracy 98.438 (98.443)	Time 2.68
Epoch: [6][200/391]	Loss 0.0275 (0.0515)	Accuracy 100.000 (98.333)	Time 2.65
Epoch: [6][250/391]	Loss 0.0547 (0.0517)	Accuracy 98.438 (98.338)	Time 2.58
Epoch: [6][300/391]	Loss 0.0514 (0.0506)	Accuracy 97.656 (98.388)	Time 2.73
Epoch: [6][350/391]	Loss 0.0204 (0.0508)	Accuracy 99.219 (98.393)	Time 2.69
train_accuracy 98.386
Test: [0/79]	Loss 0.0158 (0.0158)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1435 (0.0899)	Accuracy 96.094 (97.365)
valid_accuracy 97.420
Test: [0/79]	Loss 0.0352 (0.0352)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0052 (0.1234)	Accuracy 100.000 (95.925)
valid_accuracy 96.630
Epoch #6: Train Accuracy: 98.386, Validation Accuracy: 97.42, Test Accuracy: 96.63
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.05913809686899185
* remain weight =  100.0 %
* best SA=98.38
* find early bird tickets at epoch 7
Pruning with custom mask
* remain weight =  79.99985041584395 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  2695
******************************************
pruning state 1
* remain parameters = 213926.0
******************************************
* remain weight =  79.99985041584395 %
0.1
* training images number = 2695
Epoch: [0][0/22]	Loss 1.0169 (1.0169)	Accuracy 75.781 (75.781)	Time 0.08
train_accuracy 73.098
Test: [0/79]	Loss 0.1215 (0.1215)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1629 (0.1936)	Accuracy 94.531 (93.658)
valid_accuracy 93.700
Test: [0/79]	Loss 0.1193 (0.1193)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.0654 (0.2147)	Accuracy 98.438 (93.015)
valid_accuracy 93.590
Epoch #0: Train Accuracy: 73.09833024401833, Validation Accuracy: 93.7, Test Accuracy: 93.59
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/22]	Loss 0.6413 (0.6413)	Accuracy 71.875 (71.875)	Time 0.06
train_accuracy 69.647
Test: [0/79]	Loss 0.2550 (0.2550)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.2578 (0.3093)	Accuracy 89.062 (89.200)
valid_accuracy 90.070
Test: [0/79]	Loss 0.2687 (0.2687)	Accuracy 91.406 (91.406)
Test: [50/79]	Loss 0.1628 (0.3333)	Accuracy 96.094 (88.465)
valid_accuracy 89.500
Epoch #1: Train Accuracy: 69.64749536744297, Validation Accuracy: 90.07, Test Accuracy: 89.5
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0526537224650383
0.1
Epoch: [2][0/22]	Loss 0.9582 (0.9582)	Accuracy 66.406 (66.406)	Time 0.05
train_accuracy 69.314
Test: [0/79]	Loss 0.4803 (0.4803)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.5601 (0.4564)	Accuracy 85.156 (87.255)
valid_accuracy 87.760
Test: [0/79]	Loss 0.6800 (0.6800)	Accuracy 81.250 (81.250)
Test: [50/79]	Loss 0.3868 (0.4895)	Accuracy 91.406 (86.504)
valid_accuracy 87.120
Epoch #2: Train Accuracy: 69.31354360491977, Validation Accuracy: 87.76, Test Accuracy: 87.12
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04893280938267708
0.1
Epoch: [3][0/22]	Loss 0.9620 (0.9620)	Accuracy 68.750 (68.750)	Time 0.06
train_accuracy 64.712
Test: [0/79]	Loss 0.4396 (0.4396)	Accuracy 85.938 (85.938)
Test: [50/79]	Loss 0.4376 (0.4482)	Accuracy 84.375 (83.716)
valid_accuracy 84.150
Test: [0/79]	Loss 0.4836 (0.4836)	Accuracy 83.594 (83.594)
Test: [50/79]	Loss 0.2485 (0.5036)	Accuracy 92.188 (82.261)
valid_accuracy 83.610
Epoch #3: Train Accuracy: 64.71243042530067, Validation Accuracy: 84.15, Test Accuracy: 83.61
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.061488550156354904
0.010000000000000002
Epoch: [4][0/22]	Loss 0.8799 (0.8799)	Accuracy 71.094 (71.094)	Time 0.06
train_accuracy 71.725
Test: [0/79]	Loss 0.0861 (0.0861)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1062 (0.0847)	Accuracy 96.875 (98.085)
valid_accuracy 98.350
Test: [0/79]	Loss 0.0917 (0.0917)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0386 (0.0969)	Accuracy 98.438 (97.411)
valid_accuracy 97.950
Epoch #4: Train Accuracy: 71.72541744536504, Validation Accuracy: 98.35, Test Accuracy: 97.95
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007853182964026928
0.010000000000000002
Epoch: [5][0/22]	Loss 0.5726 (0.5726)	Accuracy 78.906 (78.906)	Time 0.10
train_accuracy 77.625
Test: [0/79]	Loss 0.0732 (0.0732)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0799 (0.0736)	Accuracy 98.438 (98.192)
valid_accuracy 98.460
Test: [0/79]	Loss 0.0703 (0.0703)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0198 (0.0800)	Accuracy 100.000 (97.901)
valid_accuracy 98.320
Epoch #5: Train Accuracy: 77.62523191377714, Validation Accuracy: 98.46, Test Accuracy: 98.32
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005777698941528797
0.010000000000000002
Epoch: [6][0/22]	Loss 0.5567 (0.5567)	Accuracy 80.469 (80.469)	Time 0.05
train_accuracy 79.332
Test: [0/79]	Loss 0.0691 (0.0691)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0631 (0.0620)	Accuracy 99.219 (98.575)
valid_accuracy 98.780
Test: [0/79]	Loss 0.0590 (0.0590)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0187 (0.0672)	Accuracy 100.000 (98.208)
valid_accuracy 98.570
Epoch #6: Train Accuracy: 79.33209647353814, Validation Accuracy: 98.78, Test Accuracy: 98.57
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.004683862440288067
0.0010000000000000002
Epoch: [7][0/22]	Loss 0.6218 (0.6218)	Accuracy 79.688 (79.688)	Time 0.05
train_accuracy 80.594
Test: [0/79]	Loss 0.0667 (0.0667)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0585 (0.0603)	Accuracy 99.219 (98.453)
valid_accuracy 98.700
Test: [0/79]	Loss 0.0607 (0.0607)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0173 (0.0653)	Accuracy 100.000 (98.254)
valid_accuracy 98.640
Epoch #7: Train Accuracy: 80.59369202084798, Validation Accuracy: 98.7, Test Accuracy: 98.64
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006263848044909537
0.0010000000000000002
Epoch: [8][0/22]	Loss 0.5472 (0.5472)	Accuracy 81.250 (81.250)	Time 0.06
train_accuracy 80.482
Test: [0/79]	Loss 0.0609 (0.0609)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0557 (0.0596)	Accuracy 100.000 (98.514)
valid_accuracy 98.750
Test: [0/79]	Loss 0.0589 (0.0589)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0166 (0.0652)	Accuracy 100.000 (98.269)
valid_accuracy 98.620
Epoch #8: Train Accuracy: 80.48237477375095, Validation Accuracy: 98.75, Test Accuracy: 98.62
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006076867575757205
* remain weight =  63.999955124753185 %
* best SA=98.57
* find early bird tickets at epoch 9
Remove Pruning
Pruning with custom mask
* remain weight =  63.999955124753185 %
* record size = (60000, 9)
zero all unforgettable images out, rest number =  1828
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  79.99985041584395 %
Test: [0/391]	Loss 0.0268 (0.0268)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0679 (0.0546)	Accuracy 98.438 (98.790)
Test: [100/391]	Loss 0.0528 (0.0559)	Accuracy 98.438 (98.755)
Test: [150/391]	Loss 0.0427 (0.0564)	Accuracy 99.219 (98.743)
Test: [200/391]	Loss 0.0638 (0.0561)	Accuracy 97.656 (98.799)
Test: [250/391]	Loss 0.0487 (0.0567)	Accuracy 98.438 (98.792)
Test: [300/391]	Loss 0.0696 (0.0566)	Accuracy 98.438 (98.832)
Test: [350/391]	Loss 0.0648 (0.0562)	Accuracy 98.438 (98.845)
valid_accuracy 98.818
[39444 14762  4039 ... 26560 38339 21900]
[  132   172   178   212   376   444   524   558   588   846   886   916
  1032  1121  1356  1404  1512  1552  1634  1784  1826  2158  2184  2302
  2410  2636  2661  2676  2694  2707  2720  2928  2942  3030  3254  3268
  3281  3390  3510  3532  3634  3638  3692  3696  3718  3756  3772  3986
  4024  4028  4039  4066  4158  4184  4460  4502  4542  4642  4692  4716
  4955  5035  5248  5314  5370  5536  5554  5624  5632  5704  5740  5771
  5806  5842  5868  5876  5912  5972  6102  6236  6246  6272  6347  6418
  6448  6582  6808  6830  6885  6905  6962  7112  7184  7259  7270  7498
  7638  7732  7768  7784  7798  7898  7917  7989  8029  8122  8200  8202
  8347  8428  8468  8488  8532  8688  8719  8721  8729  8730  8761  8785
  8867  8898  8904  8927  8940  8942  8978  9098  9114  9340  9344  9376
  9489  9504  9528  9627  9814  9952 10044 10048 10210 10211 10251 10297
 10334 10524 10544 10600 10622 10677 10752 10756 10800 11068 11104 11196
 11242 11438 11586 11632 11688 11693 11734 11759 11775 11781 11797 11949
 11950 12066 12183 12184 12272 12325 12377 12416 12422 12493 12522 12630
 12663 12764 12830 12834 12836 12940 12967 12984 13194 13318 13370 13428
 13452 13466 13494 13538 13572 13650 13658 13662 13703 13719 13735 13752
 13896 14072 14333 14406 14591 14635 14653 14655 14700 14756 14762 14764
 14796 14804 14819 14825 14886 15188 15190 15324 15338 15434 15450 15468
 15642 15728 15748 15766 15947 16116 16192 16376 16658 16957 17001 17061
 17220 17401 17551 17591 17592 17608 17728 17763 17764 17861 17940 17958
 18066 18166 18220 18228 18288 18378 18382 18419 18423 18557 18670 18755
 18776 18864 19360 19390 19412 19714 19782 19888 19934 19960 20029 20048
 20116 20210 20308 20350 20412 20522 20641 20784 20816 20882 20918 20930
 20959 21034 21114 21130 21302 21351 21445 21798 21952 21982 22130 22199
 22203 22217 22240 22263 22278 22284 22320 22400 22483 22562 22591 22607
 22763 22986 23038 23086 23136 23273 23336 23361 23385 23400 23406 23422
 23458 23588 23594 23710 23730 23819 23824 24036 24038 24046 24059 24210
 24310 24479 24501 24598 24613 24684 24730 24940 24997 25159 25232 25321
 25546 25614 25648 25678 25683 25714 25799 25854 25966 25986 25994 26017
 26061 26079 26135 26184 26266 26275 26376 26405 26443 26572 26622 26626
 26629 26636 26740 26756 26852 26940 27074 27105 27118 27121 27248 27251
 27292 27320 27514 27522 27602 27903 27912 27964 28152 28260 28279 28357
 28368 28413 28422 28471 28530 28567 28652 28710 28788 28898 28900 29036
 29048 29109 29180 29238 29434 29588 29672 29712 29930 30041 30130 30142
 30312 30356 30450 30508 30525 30646 30751 30770 30833 30925 30962 30968
 31000 31134 31173 31252 31517 31576 31591 31596 31606 31608 31650 31652
 31710 31736 31738 31760 31768 31854 31962 32002 32018 32108 32236 32280
 32288 32323 32345 32348 32372 32456 32507 32571 32573 32622 32774 32814
 33088 33242 33246 33250 33304 33318 33338 33448 33506 33552 33860 33906
 34010 34048 34234 34322 34377 34520 34614 34615 34644 34660 34676 34678
 34942 35062 35068 35464 35708 35734 35764 35851 35924 35956 35970 36141
 36446 36522 36620 36714 36760 36766 36835 36836 36866 36934 36982 37028
 37038 37048 37050 37140 37198 37256 37396 37427 37468 37469 37521 37574
 37672 37704 37921 38170 38283 38390 38399 38408 38628 38640 38680 38796
 38877 38964 39031 39260 39297 39327 39377 39487 39488 39575 39582 39614
 39662 39734 39778 39864 40022 40208 40280 40498 40612 40704 40708 40976
 41016 41027 41060 41113 41156 41200 41226 41270 41284 41366 41396 41408
 41438 41464 41538 41562 41594 41618 41688 41742 41904 42140 42262 42397
 42479 42604 42665 42687 42709 42715 42854 42866 42878 42986 43212 43234
 43328 43510 43574 43648 43670 43823 43850 43874 43898 44090 44253 44338
 44340 44383 44424 44442 44456 44480 44484 44598 44915 44944 45026 45048
 45056 45082 45294 45348 45408 45432 45616 45617 45654 45666 45760 45800
 45888 45944 45968 45980 46021 46066 46088 46122 46187 46269 46288 46368
 46394 46432 46476 46715 46794 46857 47034 47036 47159 47217 47227 47291
 47376 47387 47540 47798 47844 47888 47926 47936 48006 48228 48245 48288
 48356 48382 48402 48486 48594 48618 48750 48933 49026 49064 49070 49107
 49139 49164 49464 49535 49548 49624 49890 49904 49910 49945]
* Critical examples for prunings = 694
* Critical examples for training = 1828
* PrAC images = 2090
******************************************
pruning state 2
* remain parameters = 171141.0
******************************************
* remain weight =  63.999955124753185 %
0.1
* training images number = 2090
Epoch: [0][0/17]	Loss 0.9556 (0.9556)	Accuracy 67.969 (67.969)	Time 0.05
train_accuracy 68.421
Test: [0/79]	Loss 0.2075 (0.2075)	Accuracy 89.844 (89.844)
Test: [50/79]	Loss 0.3305 (0.2684)	Accuracy 86.719 (90.165)
valid_accuracy 90.200
Test: [0/79]	Loss 0.3225 (0.3225)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.2503 (0.3095)	Accuracy 92.188 (88.588)
valid_accuracy 89.250
Epoch #0: Train Accuracy: 68.42105262062766, Validation Accuracy: 90.2, Test Accuracy: 89.25
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/17]	Loss 0.8747 (0.8747)	Accuracy 65.625 (65.625)	Time 0.07
train_accuracy 67.177
Test: [0/79]	Loss 0.2234 (0.2234)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2466 (0.1947)	Accuracy 95.312 (94.210)
valid_accuracy 94.700
Test: [0/79]	Loss 0.1945 (0.1945)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.0596 (0.2173)	Accuracy 98.438 (93.260)
valid_accuracy 94.250
Epoch #1: Train Accuracy: 67.17703347822126, Validation Accuracy: 94.7, Test Accuracy: 94.25
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04932774603366852
0.1
Epoch: [2][0/17]	Loss 0.8256 (0.8256)	Accuracy 66.406 (66.406)	Time 0.05
train_accuracy 70.813
Test: [0/79]	Loss 0.0978 (0.0978)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1687 (0.1397)	Accuracy 92.969 (96.569)
valid_accuracy 96.640
Test: [0/79]	Loss 0.0926 (0.0926)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1179 (0.1465)	Accuracy 95.312 (95.910)
valid_accuracy 96.400
Epoch #2: Train Accuracy: 70.81339704887719, Validation Accuracy: 96.64, Test Accuracy: 96.4
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.040177397429943085
0.010000000000000002
Epoch: [3][0/17]	Loss 0.6632 (0.6632)	Accuracy 72.656 (72.656)	Time 0.05
train_accuracy 75.072
Test: [0/79]	Loss 0.0445 (0.0445)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0595 (0.0623)	Accuracy 99.219 (98.744)
valid_accuracy 98.760
Test: [0/79]	Loss 0.0592 (0.0592)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0205 (0.0675)	Accuracy 100.000 (98.453)
valid_accuracy 98.690
Epoch #3: Train Accuracy: 75.07177034952994, Validation Accuracy: 98.76, Test Accuracy: 98.69
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007198742590844631
0.010000000000000002
Epoch: [4][0/17]	Loss 0.5265 (0.5265)	Accuracy 84.375 (84.375)	Time 0.06
train_accuracy 78.278
Test: [0/79]	Loss 0.0386 (0.0386)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0522 (0.0569)	Accuracy 99.219 (98.713)
valid_accuracy 98.780
Test: [0/79]	Loss 0.0494 (0.0494)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0131 (0.0605)	Accuracy 100.000 (98.575)
valid_accuracy 98.830
Epoch #4: Train Accuracy: 78.27751194712077, Validation Accuracy: 98.78, Test Accuracy: 98.83
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005223762709647417
0.0010000000000000002
Epoch: [5][0/17]	Loss 0.5065 (0.5065)	Accuracy 79.688 (79.688)	Time 0.13
train_accuracy 80.478
Test: [0/79]	Loss 0.0383 (0.0383)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0514 (0.0561)	Accuracy 99.219 (98.683)
valid_accuracy 98.750
Test: [0/79]	Loss 0.0508 (0.0508)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0133 (0.0589)	Accuracy 100.000 (98.529)
valid_accuracy 98.800
Epoch #5: Train Accuracy: 80.47846882651297, Validation Accuracy: 98.75, Test Accuracy: 98.8
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0005258821765892208
0.0010000000000000002
Epoch: [6][0/17]	Loss 0.5084 (0.5084)	Accuracy 78.906 (78.906)	Time 0.05
train_accuracy 80.191
Test: [0/79]	Loss 0.0362 (0.0362)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0477 (0.0549)	Accuracy 99.219 (98.698)
valid_accuracy 98.770
Test: [0/79]	Loss 0.0501 (0.0501)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0125 (0.0578)	Accuracy 100.000 (98.560)
valid_accuracy 98.830
Epoch #6: Train Accuracy: 80.19138755250776, Validation Accuracy: 98.77, Test Accuracy: 98.83
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0005258821765892208
* remain weight =  51.200038891880574 %
* best SA=98.83
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  51.200038891880574 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  1143
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  63.999955124753185 %
Test: [0/391]	Loss 0.0396 (0.0396)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0652 (0.0472)	Accuracy 97.656 (98.989)
Test: [100/391]	Loss 0.0557 (0.0479)	Accuracy 98.438 (99.025)
Test: [150/391]	Loss 0.0260 (0.0474)	Accuracy 100.000 (99.058)
Test: [200/391]	Loss 0.0603 (0.0476)	Accuracy 97.656 (99.044)
Test: [250/391]	Loss 0.0503 (0.0478)	Accuracy 97.656 (99.057)
Test: [300/391]	Loss 0.0767 (0.0481)	Accuracy 99.219 (99.071)
Test: [350/391]	Loss 0.0528 (0.0483)	Accuracy 98.438 (99.065)
valid_accuracy 99.078
[44340    80   444 ...  7842 44480 42482]
[  132   172   178   212   340   376   444   482   524   588   748   832
   846   854   886   892  1032  1047  1097  1265  1356  1404  1512  1552
  1784  1826  2148  2158  2184  2302  2342  2426  2554  2576  2636  2661
  2676  2707  2720  2847  2942  3254  3268  3390  3510  3634  3692  3772
  3986  3998  4066  4122  4158  4460  4502  4562  4642  4692  4716  5062
  5084  5103  5177  5248  5314  5370  5430  5536  5554  5632  5704  5740
  5842  5876  5896  5972  6092  6102  6236  6269  6347  6418  6448  6524
  6582  6658  6670  6808  6905  7042  7242  7259  7270  7347  7528  7638
  7768  7784  7898  7917  7920  7989  8072  8122  8200  8202  8468  8488
  8532  8617  8680  8688  8701  8729  8730  8731  8785  8883  8898  8904
  8927  8940  8942  8978  9098  9114  9235  9340  9344  9376  9390  9396
  9504  9528  9583  9661  9727  9814  9952 10044 10146 10251 10297 10334
 10486 10524 10600 10677 10746 10752 10800 10894 10944 10995 11068 11104
 11196 11438 11565 11586 11631 11688 11693 11734 11775 11949 12066 12181
 12183 12184 12272 12325 12377 12416 12422 12493 12494 12630 12646 12663
 12764 12830 12834 12891 12940 12967 12984 13026 13138 13185 13194 13318
 13370 13428 13452 13494 13538 13572 13662 13703 13719 13735 13831 13896
 14333 14341 14406 14650 14700 14756 14760 14762 14764 14804 14886 14896
 15114 15188 15324 15338 15450 15642 15748 15766 15862 15975 15991 16072
 16192 16376 16862 17001 17061 17079 17220 17382 17401 17608 17645 17763
 17764 17772 17861 18056 18228 18288 18378 18930 19040 19244 19360 19390
 19404 19412 19502 19714 19782 19866 19888 19934 20018 20048 20097 20116
 20150 20204 20210 20308 20350 20412 20652 20784 20959 20962 21034 21302
 21351 21688 21906 21944 21952 21982 22130 22200 22240 22263 22278 22284
 22320 22483 22554 22559 22562 22591 22763 22830 23016 23038 23086 23126
 23136 23273 23361 23422 23458 23588 23594 23710 23730 23819 24006 24036
 24038 24046 24210 24310 24501 24560 24589 24598 24730 24940 24997 25159
 25232 25321 25546 25614 25648 25678 25683 25714 25966 25986 25994 26017
 26046 26061 26079 26184 26275 26358 26376 26392 26443 26504 26613 26629
 26740 26756 26842 26940 27074 27085 27105 27118 27248 27292 27293 27320
 27323 27426 27502 27514 27522 27526 27722 27732 27912 27964 28152 28162
 28175 28260 28357 28413 28422 28530 28567 28574 28684 28710 28788 28920
 28954 29048 29434 29530 29588 29672 29712 29713 29903 30041 30130 30142
 30312 30356 30416 30450 30508 30525 30566 30751 30770 30833 30925 30962
 30968 31000 31024 31134 31173 31576 31596 31608 31710 31736 31738 31760
 31854 31946 32002 32018 32108 32112 32202 32323 32345 32372 32394 32417
 32507 32519 32622 32774 32814 33242 33246 33250 33338 33448 33506 33552
 33612 33860 33906 34010 34048 34234 34322 34328 34377 34422 34614 34615
 34644 34660 34676 34843 34942 35062 35068 35234 35464 35708 35734 35764
 35777 35851 35887 35916 35924 36446 36522 36620 36714 36760 36766 36835
 36836 36866 36934 36936 36984 37038 37044 37050 37198 37256 37396 37414
 37450 37453 37468 37469 37551 37574 37589 37601 37672 37704 37719 37834
 37838 37962 38283 38390 38397 38399 38408 38526 38640 38796 38877 38884
 39074 39256 39260 39297 39327 39378 39423 39425 39488 39614 39662 39734
 39778 39832 39864 39875 39940 40022 40066 40208 40280 40498 40527 40612
 40704 40708 40886 40976 40977 41016 41027 41060 41094 41113 41200 41218
 41226 41276 41396 41408 41424 41438 41464 41538 41562 41594 41688 41742
 41904 42140 42262 42397 42479 42604 42665 42667 42878 42973 42986 43109
 43230 43234 43574 43575 43648 43670 43762 43823 43850 43874 44262 44338
 44340 44355 44424 44442 44456 44480 44484 44598 44736 44915 44939 44944
 45026 45048 45056 45082 45154 45294 45348 45616 45654 45666 45760 45917
 45968 45980 46066 46122 46187 46269 46288 46298 46300 46316 46331 46368
 46394 46432 46476 46715 46857 47034 47152 47159 47160 47217 47340 47376
 47387 47421 47538 47540 47597 47600 47759 47798 47844 47888 47926 47936
 48006 48112 48228 48242 48245 48323 48344 48356 48382 48512 48538 48594
 48706 48750 48856 48945 49070 49143 49164 49212 49464 49535 49548 49573
 49626 49890 49904 49910 49945]
* Critical examples for prunings = 677
* Critical examples for training = 1143
* PrAC images = 1440
******************************************
pruning state 3
* remain parameters = 136913.0
******************************************
* remain weight =  51.200038891880574 %
0.1
* training images number = 1440
Epoch: [0][0/12]	Loss 0.9034 (0.9034)	Accuracy 67.188 (67.188)	Time 0.08
train_accuracy 60.278
Test: [0/79]	Loss 0.1545 (0.1545)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.2084 (0.1953)	Accuracy 94.531 (95.879)
valid_accuracy 96.180
Test: [0/79]	Loss 0.2282 (0.2282)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.0837 (0.2067)	Accuracy 99.219 (95.190)
valid_accuracy 95.780
Epoch #0: Train Accuracy: 60.27777777777778, Validation Accuracy: 96.18, Test Accuracy: 95.78
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/12]	Loss 1.0290 (1.0290)	Accuracy 64.844 (64.844)	Time 0.06
train_accuracy 58.264
Test: [0/79]	Loss 0.2177 (0.2177)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.2011 (0.1806)	Accuracy 96.875 (96.094)
valid_accuracy 96.390
Test: [0/79]	Loss 0.1216 (0.1216)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1093 (0.1945)	Accuracy 97.656 (95.251)
valid_accuracy 95.600
Epoch #1: Train Accuracy: 58.263888888888886, Validation Accuracy: 96.39, Test Accuracy: 95.6
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.05064529925584793
0.010000000000000002
Epoch: [2][0/12]	Loss 1.0423 (1.0423)	Accuracy 60.938 (60.938)	Time 0.05
train_accuracy 63.056
Test: [0/79]	Loss 0.0824 (0.0824)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0880 (0.0885)	Accuracy 98.438 (98.438)
valid_accuracy 98.540
Test: [0/79]	Loss 0.0664 (0.0664)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0586 (0.0977)	Accuracy 99.219 (97.886)
valid_accuracy 98.080
Epoch #2: Train Accuracy: 63.05555555555556, Validation Accuracy: 98.54, Test Accuracy: 98.08
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007143222261220217
0.0010000000000000002
Epoch: [3][0/12]	Loss 0.9096 (0.9096)	Accuracy 63.281 (63.281)	Time 0.06
train_accuracy 66.042
Test: [0/79]	Loss 0.0750 (0.0750)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0848 (0.0854)	Accuracy 98.438 (98.529)
valid_accuracy 98.630
Test: [0/79]	Loss 0.0647 (0.0647)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0494 (0.0945)	Accuracy 99.219 (97.978)
valid_accuracy 98.160
Epoch #3: Train Accuracy: 66.04166666666667, Validation Accuracy: 98.63, Test Accuracy: 98.16
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008910768083296716
0.0010000000000000002
Epoch: [4][0/12]	Loss 0.8241 (0.8241)	Accuracy 67.188 (67.188)	Time 0.07
train_accuracy 67.847
Test: [0/79]	Loss 0.0738 (0.0738)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0801 (0.0839)	Accuracy 98.438 (98.514)
valid_accuracy 98.620
Test: [0/79]	Loss 0.0639 (0.0639)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0471 (0.0932)	Accuracy 99.219 (98.024)
valid_accuracy 98.220
Epoch #4: Train Accuracy: 67.84722222222223, Validation Accuracy: 98.62, Test Accuracy: 98.22
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0010225471341982484
* remain weight =  40.95988152934841 %
* best SA=98.16
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  40.95988152934841 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  867
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  51.200038891880574 %
Test: [0/391]	Loss 0.0771 (0.0771)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.1112 (0.0821)	Accuracy 96.875 (98.637)
Test: [100/391]	Loss 0.0964 (0.0855)	Accuracy 97.656 (98.654)
Test: [150/391]	Loss 0.0884 (0.0849)	Accuracy 98.438 (98.650)
Test: [200/391]	Loss 0.0773 (0.0835)	Accuracy 98.438 (98.644)
Test: [250/391]	Loss 0.0807 (0.0837)	Accuracy 97.656 (98.662)
Test: [300/391]	Loss 0.1086 (0.0845)	Accuracy 97.656 (98.656)
Test: [350/391]	Loss 0.0611 (0.0841)	Accuracy 99.219 (98.658)
valid_accuracy 98.624
[ 4290  7833 36982  7530 22302  4460 10995  7528 34404 30356 37521 18864
 48975 22607 29410  4221 37552 18609 44262 15486   340 37551 15862 29712
 15070 41113 22615 48402 22284 21906 15016 29672 37704 19124 29713 37921
 40498 30041 25598 19244  8488 11631   558 11693 41060 25246 37952 11044
 22436 22200  7994 11688 25799 48245 48921 37414 48242 22531 48512 37468
 15660 22203 37076   576 37469 30262 11104 25412 44984 44456 15728 44090
 34095 34328 22483 25854   500 29588  8468 22497 14804 40452 48288 41054
 22169 37140 29238  8154 14796 29573 45048   132 29185 29832 15188  4506
  8428 45056 40434 11396    80 25790 15338 37834 48649 25466 48945 34377
 41094 18776 37816 14711 21944 11234 15094 19215 40948  4562   212  7758
  3998  8347 45143 25294  7768 14700 34234 37507 40527 40386 29903  8532
 33612 15114 48706 37962 14762  4692 11702 37601 14760 44480   588 37966
 14896 44484    70 40977 48933 40976 37198 33906 14740 25546 15642  4153
  7920 29530   178 37589 37044 37672 37838  7885 40886 49285  6658 26749
 24490 46298 27898 26740 35062 46288 47421 12493  9489 46274 13604 46269
 19888  1920 16862 20784 38884 42943 24526 38877 17958  9450 31481 19866
  5456 42973 13662 20816 49543  6524 36089 46316 31576 46432 20652 17003
 35916 39734 35924 31664 35124  9583  2014 12566 46394 16959 41894 26756
 16957 32702 17879 47376  6448 13538 31606  5536 47387 17890 42866 46331
  6466 13558 23588 17869 13673 42986 49535  1634 13831 20959 28180 23336
 46021 20962 38700 31252  5332 46002 12302 38680 26522 19782 38672 26516
 34870  5306 26504 38650 28260  6808 43230 31173 21034 10600 34843 34830
 34829 49416 32934 18066 16678 39848  6582  1784 28014 13703 23458  5422
 26636 43040 41742 24587 38798  5408 26622 20882 28072 23422 20930 18056
 20918 23384 43098  3290 13452  3281 23406 32888 26613  3268 47538 13752
  6670  3030 39031 20641  9952 27331  2720 47036 49890 27121 24198 32112
  5821 32372 46806 24210 39311 12830 20322 42429 13109 32397 39297 23962
 27085 20150 42454 17316 27426  6092 27431 35632 20412 32018  5806 27323
 46857 27320 12940 49960 12934  5912 10044 32276 42262 39425 39423 49945
 27243 27248 27254 27180 24084  2576  2676 24036 24038 13026 39488 24052
 20204 39260 27293 24059  5972 42334 12891 20210  5866 27292 38605 17645
 17648 20029 20572 42687 12651  2148 12646 20018 42702 42703  5638 42709
 32604 23730 27700  5656 35851 27722 46466  6347 27732 42748 13428 20628
 39708 10315 35882 26852 31710 35887  5602 47291 17112 47237  6269 13185
 13198  2338 17663 23886 20097 12752 27514 27522 47159 27526 31934 17213
 39186 10215 32511  5704 10256 31840 31850  9727 31854 47219 32002  6246
 27602 12692 42616 31884 42604 23824 47217 47689  8072 14333 28567 33242
 28574  3811 30856  8945 33250 21445 24997 45408 24990 22986 38374 14238
  8761 41276 36783 41284  8772 38365  8927  5103 28840 22779 26266  9009
   886 36835 19590 38433 30925 30566 30903 18423 10746 49212 10750 38408
   892 14484  8729 38403 49090 34678 10752  8966 38399 38409 38359  8777
  5084 10800  8867 28684 33362 33318 18397 34614 36363 45516 11949 49139
 38252 49164 28710  1047 22884 19502 49143 33338 36714 38256 43670 14341
 36836 12000 30672 24887 30811  1143 21302  8904 36642 43575 33388 40240
 38339 28672  4986 28654 22830 36760 43592  8883  8799  3692 41366 33304
   995 14292 36522 28533 47828 28984  9104 15924 21130  7434 49026 18228
 36446 18557 10944  3907 26376 28407 45792 28417 14072 19390 48102 30482
 28422 23126 26392 36425  1404 36934 40022 47710 18190 41562 14650 25966
 33552 38052 22633 14635 33484 23194 28357 43328 12184 19360 25102 41156
 12181 19369 49014 36936 14008  1356 33506  3532 41226  5174  8701 14078
 34740 49070 23089 23086 30530 41478 28471 33448 45348 30962 10894  7009
  7347 49264 30544 31000  3748 21164 49064 19412   832  6962 45306 15975
 43826 43818 43823  5908 39074 29048 32417 32248 32280 16072 17772  7989
 47260  7242 44383 10146 17608 22130 22559 32345  4502 42447 25976  2158
 29662 31962 17728 10884  2694 22679  6050   966 16100  9814  2342 22554
 20287 33378 24972 42042  2764 11734 42234 27468 34520   635 43804 30416
 18930 15324 32571  6848 14972 48856 12153  3536  5062 31622 49626 18220
 22278 31591 34660 45012 48811 47488 40874 46314 13854  4741 30156 30142
 23021 34942 47558 31301 45026 49464 27912 39832  9235 41438 45968 44939
  7784 24589 18196 26629 41334 15766 19714  6418 29311 49674 16198 24432
  1512 21952 33296 12183 14653 35051 47340 20171 12377 20350 32108 32342
  4476 43109 46122 12836 19404  8670 24798 45332 34750 41218 19808 28078
 19328  1330  9344 21382 48966  7732 41538 47646 41618 39424 35464 22643
 10048  6885  2622 16560  4446  1444 26471 45917  5314 21124 22062    28
 14582 49910 26560   376 37574 40514 47600 34758 38526 32323 34048  9098
 22675 10994 39940  4129 25306  1604 12416 41016 10251 26940 17739 41904
  7270 39752 28652 17747 16956  7146 31596 38370 35246 45615 39184 15402
 20048   266  7259 30770 10297 29881 47290   584 37680 43658 18405  4030
 16130 17819  8226 22083 36686 15434 32573 44758 17817 21900 37427 36006
 16030 42482  9433 30184 47094 13138 43454 37450 25318 35616  9396 13719
  2426  7010 16376  9376  5430 35310  1222   902 46300  7842 25678 33412
  3136 14468  5740 25683 27503 27502 40144 25159  7080 44098  5110 41424
 23911 32394 21598]
[   70   132   178   180   266   318   376   502   524   588   598   644
   832   846   854   892  1001  1032  1047  1097  1117  1148  1259  1341
  1356  1365  1375  1400  1404  1495  1512  1552  1634  1662  1766  1784
  1826  2014  2148  2158  2184  2302  2342  2410  2554  2576  2636  2661
  2676  2707  2720  2892  2942  3030  3136  3254  3268  3281  3290  3390
  3510  3532  3634  3692  3756  3772  3808  3814  3986  3998  4028  4039
  4066  4122  4150  4158  4460  4502  4642  4692  4694  4716  5084  5216
  5248  5314  5370  5422  5430  5466  5477  5536  5554  5618  5624  5632
  5718  5740  5771  5806  5821  5842  5876  5896  5912  5972  6092  6102
  6197  6212  6236  6347  6418  6466  6582  6658  6670  6808  6830  6885
  6905  7009  7042  7242  7259  7264  7270  7528  7631  7638  7767  7784
  7898  7917  7920  8072  8093  8122  8200  8202  8347  8383  8468  8480
  8488  8532  8680  8688  8729  8730  8777  8785  8799  8883  8898  8904
  8940  8942  8978  9098  9104  9114  9302  9340  9344  9376  9390  9396
  9504  9528  9534  9627  9661  9689  9717  9727  9876  9952 10044 10089
 10146 10205 10215 10225 10239 10243 10251 10253 10257 10285 10297 10301
 10309 10315 10334 10486 10600 10677 10746 10752 10756 10894 10944 11068
 11104 11395 11565 11586 11632 11688 11693 11702 11734 11737 11759 11775
 11781 11949 11953 12066 12183 12184 12272 12302 12325 12377 12404 12416
 12422 12493 12494 12522 12630 12646 12663 12692 12764 12808 12830 12834
 12836 12934 12940 12967 12984 13025 13087 13156 13177 13194 13213 13318
 13428 13452 13494 13538 13558 13662 13703 13829 13831 13896 14072 14078
 14201 14333 14341 14406 14484 14700 14756 14760 14762 14764 14796 14804
 14819 14852 14886 15016 15070 15132 15188 15257 15278 15324 15338 15402
 15450 15476 15728 15778 15779 15893 15975 15991 16130 16146 16173 16192
 16376 16408 16875 16957 17001 17011 17061 17079 17129 17207 17220 17326
 17401 17608 17739 17747 17763 17764 17861 17876 17890 17958 18056 18066
 18196 18220 18228 18247 18288 18302 18322 18378 18386 18536 18598 18713
 18930 19040 19124 19215 19360 19404 19412 19502 19590 19643 19714 19782
 19814 19866 19888 19934 20048 20116 20150 20190 20204 20210 20226 20287
 20308 20350 20412 20572 20628 20641 20652 20784 20918 20930 20959 20970
 20976 21034 21128 21302 21351 21688 21852 21906 21982 22130 22139 22200
 22240 22263 22302 22320 22400 22470 22483 22554 22559 22561 22562 22563
 22591 22607 22675 22679 22759 22763 22766 22767 22830 22838 22884 23016
 23059 23086 23126 23136 23273 23332 23385 23422 23458 23538 23588 23594
 23710 23730 23819 23826 23946 23962 24006 24038 24046 24210 24250 24310
 24432 24490 24501 24587 24589 24598 24630 24722 24940 24997 25020 25159
 25232 25297 25321 25406 25562 25614 25648 25657 25678 25683 25714 25813
 25854 25966 25975 25986 25994 26046 26061 26079 26184 26275 26358 26376
 26392 26443 26444 26613 26622 26629 26636 26749 26756 26842 26940 27074
 27085 27105 27118 27122 27248 27292 27293 27320 27431 27502 27522 27526
 27732 27912 27964 28152 28162 28228 28260 28279 28357 28391 28413 28422
 28530 28533 28567 28574 28603 28654 28684 28710 28788 28898 28979 28984
 29041 29048 29055 29180 29238 29341 29434 29530 29573 29616 29672 29711
 29712 29713 29843 29903 30041 30123 30130 30142 30312 30356 30450 30508
 30525 30566 30751 30770 30833 30856 30870 30962 30968 31000 31068 31070
 31134 31173 31216 31304 31576 31606 31608 31622 31650 31672 31710 31736
 31738 31748 31760 31806 31840 31884 31958 31962 32002 32018 32108 32112
 32129 32202 32288 32335 32345 32372 32394 32411 32413 32417 32419 32449
 32455 32507 32508 32519 32571 32573 32622 32774 32792 32814 32835 33242
 33246 33250 33296 33318 33338 33361 33388 33425 33448 33612 33680 33752
 33768 33860 33906 34010 34048 34234 34322 34328 34377 34520 34615 34644
 34660 34703 34824 34942 35062 35068 35234 35464 35574 35675 35708 35721
 35734 35764 35851 35873 35887 35916 35924 36104 36141 36446 36591 36620
 36714 36760 36765 36766 36835 36836 36934 37038 37044 37050 37076 37139
 37213 37225 37256 37396 37450 37468 37469 37514 37552 37601 37606 37672
 37680 37952 37962 38050 38170 38252 38256 38283 38304 38390 38397 38408
 38435 38526 38640 38680 38700 38796 38798 38846 38877 38884 39074 39184
 39260 39281 39284 39297 39299 39327 39377 39378 39423 39424 39425 39441
 39486 39528 39614 39662 39734 39752 39778 39801 39832 39863 39864 39875
 39940 40208 40280 40304 40509 40592 40612 40704 40708 40718 40874 40976
 41016 41027 41060 41094 41113 41156 41200 41218 41226 41276 41284 41358
 41382 41396 41408 41424 41438 41464 41538 41540 41549 41562 41594 41618
 41688 41718 41742 41830 41843 41897 41904 41933 41959 42140 42142 42234
 42262 42312 42337 42397 42479 42632 42648 42665 42667 42703 42805 42866
 42878 42973 42986 43109 43176 43234 43439 43575 43670 43823 43850 43874
 44054 44078 44327 44340 44355 44383 44424 44442 44456 44480 44484 44598
 44728 44758 44852 44915 44939 44944 45018 45026 45048 45056 45082 45266
 45294 45306 45348 45408 45488 45566 45616 45631 45652 45654 45662 45666
 45760 45792 45868 45888 45917 45930 45944 45968 45980 46023 46040 46066
 46088 46122 46154 46167 46187 46246 46269 46288 46298 46300 46316 46368
 46432 46440 46466 46580 46715 46724 46806 46857 46958 47008 47034 47143
 47152 47159 47160 47219 47255 47260 47288 47321 47340 47373 47376 47387
 47421 47540 47597 47651 47710 47759 47798 47844 47888 47912 47926 47936
 47940 48006 48102 48242 48245 48270 48288 48344 48356 48382 48402 48486
 48512 48594 48638 48750 48933 48945 49139 49143 49164 49212 49255 49416
 49464 49487 49498 49501 49548 49573 49582 49626 49890 49904 49910 49945]
* Critical examples for prunings = 924
* Critical examples for training = 867
* PrAC images = 1387
******************************************
pruning state 4
* remain parameters = 109530.0
******************************************
* remain weight =  40.95988152934841 %
0.1
* training images number = 1387
Epoch: [0][0/11]	Loss 0.8580 (0.8580)	Accuracy 65.625 (65.625)	Time 0.07
train_accuracy 60.490
Test: [0/79]	Loss 0.1634 (0.1634)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1709 (0.1772)	Accuracy 98.438 (96.676)
valid_accuracy 96.840
Test: [0/79]	Loss 0.1974 (0.1974)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0971 (0.1874)	Accuracy 96.875 (96.186)
valid_accuracy 96.880
Epoch #0: Train Accuracy: 60.49026677929934, Validation Accuracy: 96.84, Test Accuracy: 96.88
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/11]	Loss 0.9643 (0.9643)	Accuracy 59.375 (59.375)	Time 0.06
train_accuracy 63.230
Test: [0/79]	Loss 0.1293 (0.1293)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.2032 (0.1487)	Accuracy 96.094 (96.002)
valid_accuracy 96.260
Test: [0/79]	Loss 0.1485 (0.1485)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1004 (0.1551)	Accuracy 97.656 (95.941)
valid_accuracy 95.800
Epoch #1: Train Accuracy: 63.22999288095531, Validation Accuracy: 96.26, Test Accuracy: 95.8
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.05247877538204193
0.010000000000000002
Epoch: [2][0/11]	Loss 0.7870 (0.7870)	Accuracy 71.094 (71.094)	Time 0.13
train_accuracy 67.340
Test: [0/79]	Loss 0.0590 (0.0590)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0755 (0.0738)	Accuracy 97.656 (98.284)
valid_accuracy 98.460
Test: [0/79]	Loss 0.0712 (0.0712)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0259 (0.0827)	Accuracy 100.000 (98.100)
valid_accuracy 98.500
Epoch #2: Train Accuracy: 67.33958152325444, Validation Accuracy: 98.46, Test Accuracy: 98.5
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.006865698844194412
0.0010000000000000002
Epoch: [3][0/11]	Loss 0.7234 (0.7234)	Accuracy 70.312 (70.312)	Time 0.05
train_accuracy 69.358
Test: [0/79]	Loss 0.0554 (0.0554)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0726 (0.0709)	Accuracy 97.656 (98.300)
valid_accuracy 98.480
Test: [0/79]	Loss 0.0696 (0.0696)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0233 (0.0791)	Accuracy 100.000 (98.192)
valid_accuracy 98.570
Epoch #3: Train Accuracy: 69.35832736366673, Validation Accuracy: 98.48, Test Accuracy: 98.57
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009129918762482703
0.0010000000000000002
Epoch: [4][0/11]	Loss 0.7783 (0.7783)	Accuracy 68.750 (68.750)	Time 0.06
train_accuracy 70.296
Test: [0/79]	Loss 0.0542 (0.0542)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0708 (0.0696)	Accuracy 97.656 (98.330)
valid_accuracy 98.520
Test: [0/79]	Loss 0.0683 (0.0683)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0225 (0.0775)	Accuracy 100.000 (98.192)
valid_accuracy 98.580
Epoch #4: Train Accuracy: 70.29560217276355, Validation Accuracy: 98.52, Test Accuracy: 98.58
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006756139919161797
* remain weight =  32.767905223478735 %
* best SA=98.58
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  32.767905223478735 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  726
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  40.95988152934841 %
Test: [0/391]	Loss 0.0534 (0.0534)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0711 (0.0642)	Accuracy 97.656 (98.790)
Test: [100/391]	Loss 0.0678 (0.0650)	Accuracy 98.438 (98.855)
Test: [150/391]	Loss 0.0517 (0.0648)	Accuracy 99.219 (98.857)
Test: [200/391]	Loss 0.0756 (0.0640)	Accuracy 98.438 (98.892)
Test: [250/391]	Loss 0.0572 (0.0647)	Accuracy 98.438 (98.867)
Test: [300/391]	Loss 0.0984 (0.0656)	Accuracy 96.875 (98.827)
Test: [350/391]	Loss 0.0771 (0.0653)	Accuracy 97.656 (98.840)
valid_accuracy 98.838
[22083 37589    28 37952   576 48638 25546 37521 29672 37507 14756 48594
 37076 14762 48649 25562  7994 37551 34010 29711 48933 29311 40704 14972
  7767  7768 44852  7784 14852 11631 29434   266 22284 40948  7638 44340
 15486 25854 25683  7842 33860 22470 25246 44355 22263 40886   340 48402
 18776 40514 37198 48356 44939 14886  8347 22400 48856 25790 30123 11104
 33752 48344 40977 40976 37816  7732 34234 33768 25318 40498 37256 41016
 37704  8093 15070  8202  4506 29832 44456 37450 15728 45056 22169  8122
 19040 18930 45048 44090 44480 44484 34328 37469  4039 48270  4741  4460
    70 11044 15188  7920 15402 22497 21944   212 18864 44383 22240 21952
 14819 45026   500 11234 15434  7885 29530   180   178 48512 37921 14804
 25232 41060 37414 21982 15132 37140 15778 46958 13854 19888 38884  6524
 39832 49573 36089 28014 19866 32814 36104  5430 17958  5422 13703  6582
 32835 12422 13719 12416  5408 24587 36141 24589 39875 38798  1826 31481
  5466 42973 31622  5536 13538 31596 31576 13558 41904  9489 46316 49626
 27912 46300 24598 46288 47421 17890 13604 42943 39801 46274 26756 46269
 26749  3136 20784 24526 16862 26852 20882 43098  3390  6808 19714 31134
 34829 47689 38605 34824  6830 36363 10600 40022  9114 41594 45868 23194
  9098 31070 31068  1400 36425 21124 12184 12183 28407 26504 45930 49416
 26516 26636 23406  5370 19808  6658 34942 26622 26613 20930 28162  3290
 19782 46122 47597 18056 31252   598 38700 39940 20976  5306 28260 38650
 26522 23273 45944 32934 47759 31650 39734 49890 39311 32372  2410 42482
 23946 27468 12808 32002 17608 27085 39260  6102 13185  2338  9814 27502
 27503  5740 27522 42142 27526 17220 20097 23886  2426 17326 42447  2720
 27243 27248 39441  5876 20226  2576 35464 42334 32202 49960 27292 39488
 31934 47008 17401 24059 39377  5972  2676 46857 24036 47036  2694 32112
 42429 20171 12891 10315 17213  5718 17739 20572 17079 26940 10251 10253
 17061 10256 27722 31748 39074 23730 27732 31736 17763 17772 47291 46466
 39708 10285 13452 46440 10301  2014 24432  6269 10243 42703 42702 47159
 39184 42616 32507 10205 23824  9717 31854 42648 35234 32519 10215 17645
 20048  2184 42667  5656 10225 17112 12692  6246 31806 20029 47237 42042
  5638  2892  1356  6962 24990 36642 43592 47926 15924 25102 19369 33484
 30770 43874 14292 16198 30482 41382 33304 41226 48102 43850 33506 28710
 10944 38359 24887 34660 25994 22986 30416 24997  7434 49014 18557 18322
 45616 49026 30811 22675 26266  5062 43575 34644  3692 19404 21351  8777
 36836 30566   886   892 41284  8729  8730  8761 36760 14406 49143 18423
  3756 36783 16072 33378  8785 22766 18405 38170 18378 10894 41366 19502
   832  1032 19412 43826  5084 30530 28898 49090 14341 41358 36714 18397
 49164 21382   846 45652  8904  7259 14653  7528 25975  8966 38435 23126
 30356 22607 41478 47844 45143  7009 19590 47828 47798 38052 23089 37038
  3510 26358  3536 14700   635  3532  8532 45760 18609 48966 32248 11734
 25159 19328 28471 33612 38399 38403 30870 48242 22554 30856 48945  8940
 34404 28567 14201 33552 29048 15862 34678 41464  3998 31000  8927  7530
 15779 30903 22561 45792 20628 20322 10048 39378 29238 47288 47260   588
  8154 12651 29712 17747 37606  5821 31958 23911 37468 13198 38050 44078
  3907 37601 19360 32455 15338 41156  5704 42604   558 40386 27602 45266
 32394 15324 14764 31850 14760 46806 30262 37574 11693 48288  8072 32323
  6236 14072  6347 11632 33318 16173 23538  5456  9390 47912 38365 45615
 38796 47538 35882 35068  4129  6670  1634 28180  8942  8945  5314 46002
  5216 14008 26376 41562 26392 16678 29903  8488 47376 48006 49139 20641
 10297 39752 25412 40527 45917 20350 22531 10994  9009 48975 49910 32108
  4150 24198 32342 43454  1512 16376 49264  5602 47094 41540 35616 13138
 37834 26560  5110 37838 47646 16560 34048 12836 12830 26471 22062  1444
 34758 24798 42312 28413  2622 21130 41538  5912 14078  1330 37552 34750
  8480  2554 48811 37044 18190   584  6885  1404 15766 18220 41113 47290
 34740 20652 42234  7758   376 25306 10044  5103 47600 36982 35246 49212
 38339 13662  7917 30142 25678 28672 44098  9433 41218  7146 32573 19390
 36686 22779  9450 15450 36006 37427 43658 41334  2148 22679 35062  8670
 10800 45332 33448 47217  1047 37680  1222 24310  8226  1604  4030  7270
 35310 17817  3030 21598 45408 40144   902 16959 33412 43109 16957 21900
 22643 16030 47160  4476 14468 12752 26842 17819 28652  1143 15642 28078
  9344 38374 16956  7080 38370 12377]
[  132   178   180   376   444   506   524   558   588   598   854   886
  1032  1047  1097  1400  1404  1512  1552  1634  1784  1826  1940  2052
  2158  2302  2338  2554  2576  2636  2661  2694  2707  2720  2824  2942
  3030  3254  3268  3388  3390  3510  3634  3692  3696  3772  3986  3998
  4039  4066  4122  4158  4460  4502  4542  4642  4692  4716  4741  5062
  5084  5177  5216  5248  5370  5456  5536  5554  5624  5632  5704  5718
  5738  5740  5842  5896  5912  5972  6027  6092  6102  6236  6251  6269
  6304  6347  6397  6418  6448  6524  6582  6658  6670  6714  6755  6808
  6848  6878  6905  6974  7042  7114  7264  7270  7347  7490  7584  7638
  7768  7784  7885  7898  7917  7920  8029  8086  8104  8122  8200  8202
  8347  8468  8488  8532  8637  8680  8688  8701  8729  8730  8777  8785
  8832  8898  8927  8940  8966  8978  9098  9114  9235  9340  9344  9376
  9396  9489  9504  9528  9583  9627  9661  9717  9814  9952 10044 10146
 10205 10251 10256 10282 10297 10315 10334 10486 10600 10677 10746 10750
 10752 10800 10831 10894 10944 11068 11104 11242 11438 11565 11586 11631
 11632 11639 11688 11693 11734 11775 11781 11894 11949 11953 12066 12183
 12184 12272 12325 12377 12416 12422 12493 12494 12522 12630 12646 12663
 12764 12808 12830 12834 12940 12950 12957 12967 12984 13026 13100 13194
 13252 13318 13370 13428 13452 13494 13650 13662 13703 13719 13735 13850
 13896 14072 14333 14341 14406 14540 14582 14653 14700 14756 14760 14762
 14764 14796 14804 14819 14886 14896 15114 15132 15188 15324 15338 15402
 15412 15450 15574 15642 15748 15766 15778 15779 15862 15913 15991 16072
 16173 16192 16376 16766 16862 16957 17001 17061 17112 17220 17326 17401
 17494 17592 17608 17645 17648 17698 17736 17739 17747 17763 17764 17861
 17958 18056 18072 18166 18228 18288 18322 18378 18423 18436 18510 18514
 18598 18609 18650 18776 18864 19138 19244 19322 19360 19390 19404 19502
 19590 19714 19866 19888 19934 20016 20018 20048 20054 20110 20116 20171
 20204 20210 20308 20322 20350 20412 20460 20522 20572 20641 20652 20784
 20792 20882 20918 20930 20959 20976 21034 21130 21302 21351 21436 21445
 21688 21888 21906 21944 21982 22130 22169 22199 22200 22203 22217 22240
 22263 22278 22284 22320 22483 22554 22561 22562 22591 22607 22763 22766
 22830 23016 23086 23136 23206 23273 23336 23361 23385 23406 23422 23458
 23538 23594 23710 23730 23819 24006 24036 24038 24046 24166 24210 24432
 24501 24506 24560 24587 24589 24598 24609 24798 24940 24990 24997 25102
 25178 25232 25294 25321 25614 25648 25657 25678 25683 25714 25797 25833
 25863 25954 25966 25976 25986 25994 26061 26079 26184 26275 26358 26376
 26398 26443 26516 26544 26613 26622 26629 26636 26740 26749 26842 26890
 27026 27074 27085 27105 27118 27180 27248 27252 27292 27293 27316 27320
 27323 27431 27503 27514 27526 27538 27706 27732 27912 27964 28072 28152
 28162 28175 28231 28260 28279 28357 28413 28422 28512 28525 28530 28567
 28574 28710 28778 28788 28898 28954 28984 29048 29055 29121 29122 29180
 29238 29672 29711 29712 29713 29881 29906 29930 30041 30130 30184 30312
 30356 30416 30450 30482 30508 30525 30574 30600 30604 30659 30751 30770
 30792 30833 30856 30870 30925 30962 30968 31000 31108 31134 31173 31576
 31608 31629 31650 31710 31736 31738 31760 31840 31854 31946 32018 32108
 32112 32345 32348 32372 32394 32417 32419 32469 32519 32622 32774 32814
 33182 33242 33246 33250 33318 33338 33364 33378 33448 33506 33552 33612
 33768 33860 33906 34234 34322 34328 34377 34404 34422 34552 34614 34615
 34660 34676 34824 34861 34942 35062 35068 35246 35464 35574 35622 35688
 35694 35708 35734 35764 35777 35787 35851 35916 35924 36089 36141 36363
 36446 36522 36620 36714 36730 36760 36766 36835 36836 36866 36934 36982
 37038 37050 37062 37076 37198 37256 37396 37414 37427 37450 37468 37469
 37491 37507 37521 37551 37552 37589 37606 37672 37680 37704 37900 37952
 37962 38050 38170 38230 38283 38287 38345 38390 38397 38399 38408 38433
 38439 38526 38553 38577 38640 38680 38877 39256 39260 39297 39309 39311
 39327 39377 39378 39423 39425 39473 39482 39488 39614 39662 39734 39778
 39801 39832 39864 39875 39940 40066 40208 40280 40527 40589 40612 40704
 40708 40874 40976 41016 41027 41060 41066 41113 41200 41218 41226 41270
 41276 41284 41334 41396 41408 41424 41464 41538 41562 41594 41618 41742
 41890 42140 42262 42397 42479 42482 42566 42604 42616 42665 42667 42772
 42866 42878 42964 42986 43207 43234 43510 43574 43575 43670 43818 43823
 43826 43850 43874 43934 44030 44090 44338 44340 44424 44442 44456 44480
 44484 44598 44736 44845 44915 44939 44944 45026 45048 45056 45082 45143
 45294 45348 45408 45414 45491 45606 45615 45616 45654 45666 45760 45761
 45868 45965 45968 45980 46002 46066 46122 46187 46269 46288 46298 46368
 46432 46466 46538 46715 46806 46857 46996 47034 47152 47159 47240 47260
 47340 47376 47387 47421 47488 47540 47597 47759 47826 47844 47888 47912
 47926 47936 48006 48228 48242 48245 48344 48356 48382 48402 48469 48706
 48750 48856 48881 48933 48945 49070 49083 49090 49139 49143 49164 49212
 49285 49464 49514 49548 49573 49626 49676 49890 49904 49945]
* Critical examples for prunings = 838
* Critical examples for training = 726
* PrAC images = 1203
******************************************
pruning state 5
* remain parameters = 87624.0
******************************************
* remain weight =  32.767905223478735 %
0.1
* training images number = 1203
Epoch: [0][0/10]	Loss 1.1905 (1.1905)	Accuracy 51.562 (51.562)	Time 0.07
train_accuracy 57.606
Test: [0/79]	Loss 0.2764 (0.2764)	Accuracy 89.844 (89.844)
Test: [50/79]	Loss 0.3071 (0.3316)	Accuracy 92.188 (89.874)
valid_accuracy 90.650
Test: [0/79]	Loss 0.2842 (0.2842)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.1473 (0.3647)	Accuracy 97.656 (89.246)
valid_accuracy 90.200
Epoch #0: Train Accuracy: 57.60598498984168, Validation Accuracy: 90.65, Test Accuracy: 90.2
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/10]	Loss 1.0942 (1.0942)	Accuracy 57.812 (57.812)	Time 0.06
train_accuracy 57.107
Test: [0/79]	Loss 0.1393 (0.1393)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1564 (0.1567)	Accuracy 96.875 (97.687)
valid_accuracy 97.880
Test: [0/79]	Loss 0.1504 (0.1504)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1195 (0.1569)	Accuracy 98.438 (97.564)
valid_accuracy 97.670
Epoch #1: Train Accuracy: 57.1072318472668, Validation Accuracy: 97.88, Test Accuracy: 97.67
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.05763261392712593
0.010000000000000002
Epoch: [2][0/10]	Loss 0.8581 (0.8581)	Accuracy 66.406 (66.406)	Time 0.06
train_accuracy 62.843
Test: [0/79]	Loss 0.0897 (0.0897)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0963 (0.1014)	Accuracy 99.219 (98.422)
valid_accuracy 98.430
Test: [0/79]	Loss 0.0796 (0.0796)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0511 (0.1050)	Accuracy 100.000 (98.177)
valid_accuracy 98.440
Epoch #2: Train Accuracy: 62.842892806131644, Validation Accuracy: 98.43, Test Accuracy: 98.44
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.006573541555553675
0.0010000000000000002
Epoch: [3][0/10]	Loss 0.7526 (0.7526)	Accuracy 66.406 (66.406)	Time 0.13
train_accuracy 64.505
Test: [0/79]	Loss 0.0786 (0.0786)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0814 (0.0882)	Accuracy 98.438 (98.591)
valid_accuracy 98.640
Test: [0/79]	Loss 0.0703 (0.0703)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0349 (0.0902)	Accuracy 100.000 (98.545)
valid_accuracy 98.740
Epoch #3: Train Accuracy: 64.50540322218949, Validation Accuracy: 98.64, Test Accuracy: 98.74
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008673422853462398
* remain weight =  26.214249386704957 %
* best SA=98.74
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  26.214249386704957 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  672
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  32.767905223478735 %
Test: [0/391]	Loss 0.0807 (0.0807)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.0929 (0.0836)	Accuracy 97.656 (98.652)
Test: [100/391]	Loss 0.0828 (0.0858)	Accuracy 99.219 (98.677)
Test: [150/391]	Loss 0.0676 (0.0847)	Accuracy 99.219 (98.743)
Test: [200/391]	Loss 0.1033 (0.0845)	Accuracy 97.656 (98.702)
Test: [250/391]	Loss 0.0864 (0.0855)	Accuracy 97.656 (98.646)
Test: [300/391]	Loss 0.0947 (0.0853)	Accuracy 97.656 (98.700)
Test: [350/391]	Loss 0.0674 (0.0848)	Accuracy 99.219 (98.713)
valid_accuracy 98.698
[34234 42986  8945 22497 46300  2338 27522  5972 44355 20204 24036 45792
 27526  8942 10315  8202  5456  5422  1356 20171  6418 25854 27243 35622
 39297 38526  4030 10297 37816 48856 18166  5084 23194 30142  7638 22470
  3390 30123 32345 10048 37838 20792 11104 40589  4039 32348 36446 35068
  8966 25863 32323 39311 12184 41016 12183 44939 33752 21906 42566 37256
 17213  1330 40612 22483 48881  5430 31000 24887 26522 41618 13138 25412
 23206  5718  6236 16766 20412 13850 46996  6658 15324 23385 39875 31252
 25683 22217 29832 42447 24198 48594 20322 37589 13854 21034  6670 41904
 37414 32934  7842 17890 25546  5896 40886 28260 23361 38680 20882 22240
  1512 32248 48706 49626 23538 24609 43234  7917 47538 22130  6582 32835
 37552 26749  5306 12967 48649 25614 35464  5314  3136 46958 48638  9235
 32814    28 34942  9952  7920 37491 18930 25657 37574 39940 12416 16678
 13735 29711 29712 29713 43207 38798 38796 20976 46806 22263 46002  7758
 15450 28413 21982  4460 28407 26852 37704  7767  7768 36363 19714 15434
 38884  6755 46274 31481 23273  1400 21944 14972 19934 15486 27514  9376
  5740 28072  5408  9814 44383 31108 20210  6448 27503 42312  7732 18056
 47036 34824   180 49464 15070 17326 26629 22284 12272 22278  6714 38650
 11234 32112 29903  8086 26636 11438 40022  9340 26622  5177   212 47689
 28078 34829 45965 26613 31958  1444 15412 27293 18072 13650 40704 37672
 15402 45968  7584 30962 27602 35916 46440 34404 17736  6304 27722 45616
 20097 31650 17739 26358 47926  4741 11775 31840 43670 38339  1047 29055
  8532 10146 10752 30751 33448 45332 10750 14292 18322 21598 19360 13318
 32469 35777 29048 33242 41478 27732 16173 31806 36642 49026 17608  5638
 26061 42703 27912 31629 48228  8468  7434 26392 15779 15778 34644 47340
 33552  3510  7009 16198 45654 49014 30416 38050 43658 21351 30811 33506
 38359 17747 28710 18557 36982 31854 44030 22679  5632 38365 29121 19328
 19369 22763  5602 17698  3696 12651  6251 20628  9583 39708 26266 46538
 18436 27026 36836 10800 12646 13370 21445   854  8637 49139  8688 18405
 43826 43818 28898 33338   892 10831 18397 25102 47260 38230 20054 49143
 18423 11894 39074  3692 33304 28954   832 19412 42866 16956 42772 49212
 15924 19502 39662  3756 35234 10894 10215 47217 43934 20018 45348 28778
 20016 36714 23911 30530 28984 41424 10205 34552  7259 36730 47290 35882
 19404  7264 32507 42142 14341  6269  8729 12692  5536 20029 48102 44078
 41226  1634  5656  9717 35688 38403 31596 38399 18609 25975 14760 14764
 19244 23946  7528 38435  7530 38439 41562 40498 37050 48945 19590 11693
 33612 15728  7490 37962 42667 38408 21302 45143 34660 43592 49904 17764
 37062 17592 25994 23086  6092 42943 42234  6347 17772 26398   558 30925
 20522 37140 11631 13538 31576 30856 14804 47828  2184 22531  8927 48245
  6102 47376 24310 29311 24526 32419   444  8940 15642 23126 44090 14201
 37044 18228 39778 18598 27180 32417 11044 48933  5466   598 11639 34328
  8347 25954 37900 29180 44480 25294 19866 11953 32394 23336 15188  8730
 28014 38700 20930 22830 49960 22203 46288  9627 33860  1143 10253  8488
 48512 31070 25246 39752 47912 24059  4692 28422 30356 34614  5738  9661
 48270  1784 47159 49890 26940 47160 47094 37834  8670 34740 48975 13719
 49264 44098 41218 39801 40527 22200 40948 17817   584  2148   902 24798
  2676 37469  9433  7146 35062 47600 30184 23824  8200 41334  8480  7270
 39184 10251 48811  1604 25790 39256 12377 14072 27502 26560 22169 17958
  4476  3030 18190 40144 28672 22643 48242 28180  4150 14468 49573 10243
 12752 37198 38374 20048 28567 16030  1404 35310 41113 10994 18220 26471
 36006 10256 33412 10746  7080 36425 35616 21382  9396 48966 13662 44456
 28471  8072 22607 20350 29881  5103   132 19040 21900 46316 42482  5912
 32108 28357 28652  8226 41156 35246 41538 17079 18864 14582 45868 30770
  1940  5110 44484 34758  9098  2554  5216 46857 16560 24560 32573 27323
 12891 46466 49285  2622 17819 25159 16957 16959 22083 25318   376 32342
 22561 16376 25678 48402 43109 25306 37427 48288  3772  9344 29238 37680
 34750 22062 24990 36686  2576 27706 32002 34048 12836 43454 40514 12830]
[   28    80   132   178   180   266   340   370   391   524   558   588
   644   775   846   966  1032  1047  1097  1121  1148  1222  1239  1269
  1356  1495  1512  1552  1634  1662  1826  1920  2078  2184  2302  2342
  2410  2550  2554  2576  2622  2636  2661  2694  2707  2720  2847  2901
  2942  3118  3136  3254  3390  3470  3510  3536  3634  3638  3692  3719
  3772  3791  3879  3986  4028  4031  4039  4066  4122  4158  4184  4446
  4457  4502  4542  4642  4692  4716  4955  5013  5035  5084  5188  5216
  5248  5250  5306  5314  5370  5422  5536  5554  5602  5624  5632  5638
  5656  5659  5704  5718  5738  5740  5806  5842  5876  5896  5900  5912
  5972  6018  6092  6102  6236  6246  6269  6347  6418  6540  6582  6636
  6658  6670  6725  6755  6764  6808  6830  6905  6974  7005  7042  7207
  7259  7270  7528  7584  7606  7638  7719  7758  7768  7801  7842  7898
  7920  8122  8154  8226  8403  8468  8488  8532  8617  8637  8680  8688
  8701  8729  8730  8785  8853  8883  8898  8904  8940  8978  9009  9098
  9104  9114  9340  9344  9376  9390  9396  9471  9489  9504  9528  9583
  9627  9661  9717  9814  9952 10044 10048 10073 10146 10194 10207 10215
 10243 10251 10253 10287 10297 10309 10312 10334 10382 10486 10570 10600
 10677 10746 10752 10800 10944 11068 11104 11196 11298 11306 11565 11576
 11586 11632 11688 11693 11702 11718 11734 11738 11759 11775 11786 11949
 12066 12183 12184 12232 12272 12302 12325 12377 12416 12422 12493 12494
 12501 12514 12522 12551 12559 12585 12587 12603 12630 12646 12663 12679
 12692 12764 12830 12834 12940 12957 12967 12984 13026 13138 13194 13318
 13428 13452 13494 13662 13703 13896 13914 14072 14078 14230 14333 14341
 14406 14468 14490 14650 14653 14700 14756 14760 14762 14764 14765 14804
 14819 14852 14866 14896 15013 15039 15070 15114 15132 15188 15243 15324
 15338 15402 15403 15450 15500 15642 15660 15699 15728 15766 15799 15827
 15829 15862 15924 15991 16116 16130 16192 16198 16328 16376 16461 16488
 16574 16676 16716 16956 16959 17001 17061 17079 17112 17129 17209 17216
 17220 17336 17401 17480 17592 17739 17747 17763 17764 17772 17861 17958
 18003 18066 18072 18220 18228 18288 18356 18378 18382 18419 18460 18511
 18514 18598 18776 18864 18930 19040 19345 19360 19412 19450 19494 19502
 19590 19748 19764 19782 19808 19888 19960 20018 20048 20050 20116 20169
 20171 20179 20204 20210 20265 20308 20350 20412 20522 20641 20784 20792
 20816 20847 20882 20930 20959 20967 21034 21124 21135 21164 21197 21302
 21351 21445 21527 21548 21601 21688 21906 21944 21952 21982 22124 22130
 22169 22199 22203 22217 22240 22263 22272 22278 22284 22320 22335 22436
 22442 22470 22483 22559 22561 22562 22591 22607 22615 22633 22679 22763
 22779 22830 22958 23028 23038 23040 23086 23136 23194 23273 23336 23385
 23422 23458 23546 23588 23594 23642 23710 23730 23770 23782 23814 23819
 23962 24006 24038 24046 24059 24101 24118 24210 24310 24360 24501 24560
 24587 24589 24598 24630 24716 24722 24730 24805 24849 24940 24972 24990
 24997 25159 25232 25294 25308 25321 25412 25546 25614 25648 25657 25677
 25678 25683 25714 25799 25854 25863 25954 25966 25986 25994 26017 26061
 26079 26138 26146 26184 26266 26275 26358 26376 26392 26443 26455 26504
 26572 26622 26629 26740 26751 26756 26842 26940 27074 27085 27105 27113
 27118 27171 27181 27239 27252 27293 27320 27331 27413 27431 27451 27502
 27522 27537 27641 27643 27700 27898 27964 28152 28162 28357 28417 28471
 28530 28533 28567 28603 28778 28780 28788 28795 28801 28839 28844 28954
 29109 29152 29180 29238 29287 29434 29496 29519 29573 29588 29611 29672
 29711 29712 29713 29843 29893 29930 30041 30123 30130 30142 30312 30356
 30443 30450 30452 30508 30525 30534 30724 30751 30770 30811 30833 30856
 30870 30968 31000 31016 31100 31134 31173 31185 31252 31275 31546 31596
 31608 31617 31672 31710 31736 31738 31760 31854 31958 32002 32018 32035
 32065 32108 32202 32288 32345 32348 32372 32405 32427 32438 32475 32571
 32622 32774 32814 32835 32888 33088 33246 33250 33296 33338 33361 33362
 33506 33552 33612 33768 33906 33941 34010 34048 34234 34322 34328 34377
 34484 34520 34615 34644 34660 34843 34942 35051 35062 35112 35464 35708
 35734 35764 35851 35883 35916 35924 36089 36104 36141 36172 36214 36363
 36425 36446 36522 36620 36714 36760 36766 36835 36836 36866 36934 36982
 37038 37050 37198 37256 37396 37468 37491 37574 37704 37838 37900 37952
 37962 37966 38050 38170 38256 38283 38319 38381 38390 38397 38399 38403
 38408 38409 38526 38650 38700 38796 38798 38877 39031 39032 39074 39256
 39260 39289 39296 39297 39327 39342 39357 39378 39423 39424 39425 39488
 39614 39662 39734 39752 39778 39810 39832 39864 39875 40022 40172 40182
 40208 40280 40514 40527 40569 40589 40592 40612 40704 40708 40874 40886
 40964 40976 40977 41027 41060 41094 41113 41200 41218 41226 41335 41408
 41438 41464 41540 41541 41593 41594 41668 41688 41718 41742 41897 42042
 42140 42234 42262 42273 42334 42397 42429 42479 42532 42665 42854 42878
 42986 43068 43089 43098 43109 43176 43234 43328 43368 43439 43449 43670
 43823 43826 43850 43852 43874 43898 44098 44133 44294 44340 44383 44424
 44442 44456 44484 44598 44852 44915 44939 44944 45026 45048 45056 45082
 45143 45185 45294 45306 45348 45408 45432 45531 45615 45616 45666 45760
 45779 45792 45889 45894 45968 45980 46040 46066 46076 46088 46122 46187
 46269 46277 46288 46298 46368 46432 46689 46715 46857 46887 46938 47034
 47152 47217 47219 47237 47288 47291 47340 47376 47387 47421 47540 47597
 47646 47650 47710 47759 47798 47844 47888 47926 47936 47965 48006 48228
 48242 48245 48270 48344 48356 48382 48402 48525 48527 48594 48681 48706
 48750 48881 48921 48933 48966 48968 49009 49070 49090 49139 49164 49212
 49252 49416 49434 49464 49501 49524 49535 49537 49548 49573 49626 49790
 49792 49890 49916 49945]
* Critical examples for prunings = 952
* Critical examples for training = 672
* PrAC images = 1305
******************************************
pruning state 6
* remain parameters = 70099.0
******************************************
* remain weight =  26.214249386704957 %
0.1
* training images number = 1305
Epoch: [0][0/11]	Loss 1.4175 (1.4175)	Accuracy 53.125 (53.125)	Time 0.09
train_accuracy 57.701
Test: [0/79]	Loss 0.4173 (0.4173)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.3959 (0.4099)	Accuracy 92.188 (91.452)
valid_accuracy 92.260
Test: [0/79]	Loss 0.4243 (0.4243)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2882 (0.4279)	Accuracy 96.094 (90.548)
valid_accuracy 91.100
Epoch #0: Train Accuracy: 57.701149425287355, Validation Accuracy: 92.26, Test Accuracy: 91.1
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/11]	Loss 1.0771 (1.0771)	Accuracy 60.156 (60.156)	Time 0.05
train_accuracy 61.226
Test: [0/79]	Loss 0.2253 (0.2253)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.2570 (0.2289)	Accuracy 95.312 (96.094)
valid_accuracy 96.150
Test: [0/79]	Loss 0.1708 (0.1708)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1416 (0.2266)	Accuracy 96.875 (95.864)
valid_accuracy 95.990
Epoch #1: Train Accuracy: 61.22605363984675, Validation Accuracy: 96.15, Test Accuracy: 95.99
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06225481256842613
0.010000000000000002
Epoch: [2][0/11]	Loss 0.8873 (0.8873)	Accuracy 67.969 (67.969)	Time 0.07
train_accuracy 64.674
Test: [0/79]	Loss 0.1017 (0.1017)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1195 (0.1103)	Accuracy 99.219 (98.223)
valid_accuracy 98.290
Test: [0/79]	Loss 0.1108 (0.1108)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0676 (0.1162)	Accuracy 98.438 (97.855)
valid_accuracy 98.080
Epoch #2: Train Accuracy: 64.67432950191571, Validation Accuracy: 98.29, Test Accuracy: 98.08
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00801723264157772
0.0010000000000000002
Epoch: [3][0/11]	Loss 0.7786 (0.7786)	Accuracy 67.188 (67.188)	Time 0.07
train_accuracy 66.437
Test: [0/79]	Loss 0.0891 (0.0891)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1044 (0.0987)	Accuracy 98.438 (98.254)
valid_accuracy 98.300
Test: [0/79]	Loss 0.0996 (0.0996)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0623 (0.1042)	Accuracy 98.438 (98.009)
valid_accuracy 98.240
Epoch #3: Train Accuracy: 66.4367816091954, Validation Accuracy: 98.3, Test Accuracy: 98.24
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009415255626663566
* remain weight =  20.971324717285945 %
* best SA=98.24
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  20.971324717285945 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  649
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  26.214249386704957 %
Test: [0/391]	Loss 0.0756 (0.0756)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.0973 (0.0935)	Accuracy 98.438 (98.499)
Test: [100/391]	Loss 0.0884 (0.0958)	Accuracy 99.219 (98.523)
Test: [150/391]	Loss 0.0768 (0.0972)	Accuracy 98.438 (98.489)
Test: [200/391]	Loss 0.0863 (0.0952)	Accuracy 99.219 (98.562)
Test: [250/391]	Loss 0.1165 (0.0958)	Accuracy 97.656 (98.568)
Test: [300/391]	Loss 0.1316 (0.0967)	Accuracy 95.312 (98.502)
Test: [350/391]	Loss 0.1072 (0.0973)	Accuracy 97.656 (98.478)
valid_accuracy 98.452
[48975 29434 47798 11438  7009 26443 40886 31252 14333 41060 33088 31000
 40527 41618  4476 10600 12183 12184   340 26504 28260  8898  3390 13850
 28778  5103 16116 41464 11196  4502 14078 34660 33242 41478  5035 49264
 47888 40182 26358 11639 12066 49285  3536 21906 38390  7005 26392 23336
 26398 24887 41540 48512 47828  7758 37574 23361 47912  1143 16676 28162
  4030 16678 12325 47600 36760  8883 23126 13735   966 23458 22436 34942
 48649 39940 37469 49535 13719 43109 43592  6582 20882 47538 48594 12302
 30856 36214 34824 22203 38409 11044 49416 41668 25657 22130 22200  6670
 26560 23385 45616 19714 40022 19040 46076  7042 11632 48706 18066  6636
 37834  1634 18930 18056  4446  6755 15486 45968 41226 25159 15402 11786
   178   180 48881 36425  8853 19369 31100 49070 43658 10894 28417 15412
 37396 49090  7842 44480 25318 26138 44424  8945 11104 31108   132  8154
 37680 10944 43439 41156 15114 25994 34377 48228 30968 18598  8966  1330
  8072 28471 26061 23194 36446 37672 29588 49014 25232 38359 19328    80
 25863 33612 23206 45868 16173  6905 43670  3756  4955  8942  8940 25246
 23273 48270  8927 30751 38365 40948 33304 48945 11953  7898 34615  3634
 49212 22320  4692  8200 24972 41438 38439   266 47936 10746 34614 34520
 18378 22169 49139 37704 44484 30770   212 43368 48856 15434 21351 36642
 11894 10831  6974 18423 14292 33768 15188 36522  4122 25966 21952  3692
 16198 10800 18382  1444 34234  4542 28984 14866 45185 37044 42234  2694
 45408 27180 31672 37050  6347 42866 15778 39031 32394  2720 43826 32372
 17592 42854 37038 27243 22633 13428 47036 29121 32323 35916 10044  8488
 23710 16959 32342 16957 32348 20171 12836   854 32419 15766 24360 44098
 31608  7528  5632   558  2847 22562 22561 46368 20048  9471 37966 13538
 27026  8680 43823  7259 44090 32427 36836 22591 17608 31650 13494 44078
 43818   588 29180  7264 10146 32475 27085  9489 27912  6418 20641 20210
  8617 27522  2338 27514 43934  9814 27503 22763 35616 23824 27706 39342
 46806 39357 24036 32018 42482 27526 30530 43898 46689 39256 31958 39260
  6092 35688 17213 17216 46715 30525 23946  2302 42566 45348 39297 39074
 24038  5972  9952  9583 27320 46958  7434 35464 38050 12940 31738 42334
   846 43852 42312 20628 24198  2078 14582  5896  5912 29048 32112 43874
 39378 42447 29055 21601 31806 13026 22679  6236 15862  9627 39423 39424
  6251 22830 24310 41904 45056 17763 42042  8701 39832 47291 20784 22483
 39752  8403 49626 31546 28072 39778   444 17890 26940 28014 36104 17817
 46269  5554 29287 22497 20816 38230 42986 23538 26852 47376 46300 10309
 14819 24526 10256 38884 47421 19888  2942 47340 22470 37198 24598  8730
 43089 46187 37952 10215 17772  6448 27964 26749 24609 10207  7584 14406
 10205 32835 47260 23594  3136 14760 43068  9344 17736 46316 20016  5408
  8729  9433 20018 24587  9340 24589 37140 12651 39875 32571 14764 45048
 47759 41424 31736 30142 15924 20522 29713 26636  8347  4150  2410 24730
 24059 46088 25854 13650 37256  9376 12957 36866 14072 15991 37552 14700
 32417 34644 38680 48006 25102 37062  6830 34552 21382 42943 12752   584
 28672 25790  5430  1222 40977 49143  1512 25412 28078 24560 25799 10243
 21598 17958 33412 41016 47646 32108 41334 35062 22062 41718 47290 48402
   376 47288 46122 20350 27502 34750 22083  7146 32888 38650 35234 12416
 31576 36006 12377 49573 47237 34048 25546 45332 28652 23911 44852 16130
 47217  4460 35310 47160  1404 13138  9098 16030 15450 47159 32573 39662
 25683 32507 35246 38397 39184  7638 29711  2148 46857  7732 10994 34740
 10297 41218  2622 38700 36714  9009 48966 21124 41538 40144 17819 46274
 43454 14650 22643 14468 28710 16574 16956 11734  8480 34758 44456  5110
 37427  4741 18190 19960 12830 26471 46466 38526 48242 43234 47094 17001
 22779 33448 25306 36982 38374 26842 23588  7080 22531 44939  5456 19404
 38339 13662  1604 14972  8226  7917 14341 33506  2576  8670 34328  5084
 24798 19244  6658 21900  2554 15660  8086  3030  7270 16560 29930 36686
 16376]
[   70   132   178 ... 49890 49904 49945]
* Critical examples for prunings = 1081
* Critical examples for training = 649
* PrAC images = 1359
******************************************
pruning state 7
* remain parameters = 56079.0
******************************************
* remain weight =  20.971324717285945 %
0.1
* training images number = 1359
Epoch: [0][0/11]	Loss 1.1852 (1.1852)	Accuracy 58.594 (58.594)	Time 0.07
train_accuracy 57.910
Test: [0/79]	Loss 0.5072 (0.5072)	Accuracy 85.938 (85.938)
Test: [50/79]	Loss 0.5494 (0.5304)	Accuracy 85.938 (87.944)
valid_accuracy 88.400
Test: [0/79]	Loss 0.5371 (0.5371)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.4071 (0.5742)	Accuracy 92.188 (85.034)
valid_accuracy 86.480
Epoch #0: Train Accuracy: 57.91022818749928, Validation Accuracy: 88.4, Test Accuracy: 86.48
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/11]	Loss 0.8228 (0.8228)	Accuracy 65.625 (65.625)	Time 0.06
train_accuracy 64.459
Test: [0/79]	Loss 0.2324 (0.2324)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.2516 (0.2538)	Accuracy 96.094 (95.665)
valid_accuracy 96.120
Test: [0/79]	Loss 0.2568 (0.2568)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1855 (0.2870)	Accuracy 96.875 (94.378)
valid_accuracy 95.270
Epoch #1: Train Accuracy: 64.45916119281469, Validation Accuracy: 96.12, Test Accuracy: 95.27
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.05984415113925934
0.010000000000000002
Epoch: [2][0/11]	Loss 0.8783 (0.8783)	Accuracy 66.406 (66.406)	Time 0.07
train_accuracy 66.814
Test: [0/79]	Loss 0.1474 (0.1474)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1428 (0.1568)	Accuracy 98.438 (97.335)
valid_accuracy 97.560
Test: [0/79]	Loss 0.1440 (0.1440)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0717 (0.1817)	Accuracy 99.219 (96.032)
valid_accuracy 96.780
Epoch #2: Train Accuracy: 66.81383374616273, Validation Accuracy: 97.56, Test Accuracy: 96.78
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00823837798088789
0.0010000000000000002
Epoch: [3][0/11]	Loss 0.7015 (0.7015)	Accuracy 72.656 (72.656)	Time 0.06
train_accuracy 70.714
Test: [0/79]	Loss 0.1271 (0.1271)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1152 (0.1305)	Accuracy 98.438 (97.763)
valid_accuracy 97.980
Test: [0/79]	Loss 0.1214 (0.1214)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0525 (0.1498)	Accuracy 99.219 (96.967)
valid_accuracy 97.430
Epoch #3: Train Accuracy: 70.71375998861215, Validation Accuracy: 97.98, Test Accuracy: 97.43
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009629273554310203
* remain weight =  16.77698498175073 %
* best SA=97.43
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  16.77698498175073 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  595
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  20.971324717285945 %
Test: [0/391]	Loss 0.1595 (0.1595)	Accuracy 94.531 (94.531)
Test: [50/391]	Loss 0.1374 (0.1321)	Accuracy 96.875 (97.488)
Test: [100/391]	Loss 0.1228 (0.1342)	Accuracy 98.438 (97.641)
Test: [150/391]	Loss 0.0870 (0.1337)	Accuracy 99.219 (97.718)
Test: [200/391]	Loss 0.1070 (0.1322)	Accuracy 97.656 (97.699)
Test: [250/391]	Loss 0.1007 (0.1340)	Accuracy 99.219 (97.647)
Test: [300/391]	Loss 0.1731 (0.1339)	Accuracy 95.312 (97.693)
Test: [350/391]	Loss 0.1162 (0.1329)	Accuracy 99.219 (97.761)
valid_accuracy 97.786
[38577 36522  7530 15412  7920 22199  6808  8942 14972 35764 23194  8122
 39734 22203 36783  5538 37396 20016   418 20534 15434 22217 21982  6830
  5554 24432  9104 20784  7584  6347  7898 21124  9340  7009 37491  8966
  9046 22763 37557  7989   212 37552 36446 23730 22531 38170   132 15660
  7264 35068 29287  6251 36642 20652 39752 22169 37450  6236  9583 37468
 37469  8701 37574 22062 36425 20641 23710 21130  5602 22240    80   376
  6636  9814  9471 22436  8731 39488  7732 36760  7758 35632 44939  6418
  9376  6670 39482  8883 44338  6578  5898 44845 39424  6582 37838  5876
    70  6448 20882 37834 20224 37966 44480 23946 21900 38052 36984 20171
  9390 21601 10207  5632 38399 37680 39625  8854  7842 38401 23273 20522
 20816  8347 21952 39342  6755 38640  9436 20976 49904  6092 39297 38680
   178  9009   180 39378 37140 10146 23868 22633 22470 38700 39184 29711
 19888 32507 32519 47260 17739 12651 12646 47291 16072 47318 26842  3030
 41912 41904 45408 30574 12494 17817 47217  2817 17648 12940 27248 27243
 42262 32288 47036 27180 42234 32323 12836 12834 28778  2694 32348   966
 32372 17592 26749 42312 26740  3136 41562 33088 15961 18228 26376 47828
  3510 43850 26358 12066 47888 41438 47912 33246  3634 33304 33182 18190
 12183 12184 14468 12416 26636   854 41742 24526 32888 28898 41718 30530
 18066 12302 47646 41668 26515 26504 41618 32776  2554 14341 43670 43510
 14172 46066 28574 46088 16698 46122 46023 28078 42973 46269 16836 46274
 45654  1826 46288 28014 16574 45980 28279 43439 30968 28471 14078 45792
 14072 28422 28413 14008 45868 28368  1404 45888 28357 31100  1222 43310
 42943 46300  1143 16875  2184 27556 14292 13194 27522 46715 27514 27502
 28710  2338 32002 42482  1032 46806  2410 32112 27323 42687 15924 31806
 28672 27912 13538 31579 31608 42854 46406  1978 45616 31650 31672 46466
 13428 31736 27732 31738 17061 30770 17079  3692 43818 49090 11104 14796
 14653 10800 33768 45056 48402 34740 11565 25294 40976 25246 11044 25799
 29055 48933 18405 48344 40066  5035 41016 25318 25683 48512 25648 11196
 45749 10746 49264 48706 25466  4446 30312 34660 29121 29048 48649 48638
 34614 40144 45154 25566  4542  5084 18930  4562 40829 40182 34352 48966
 48975 49014 24990 25994 33506 11781 11786 39875 24589 19369 33448  4822
 40280 11882 19390  3756 14582 24560 19412 10894  5430 18423 41156 34942
 10944 14760 29041  3998 18650 48270 34552 14764 19328 33612 19808 11693
 11688 49006 18598 49139 49143 15766 25412 19666 17213  2290  5656 36934
 21445 10831 24972 34234  8898 37044 22561 30833 19244 28984 30856 45348
 19960  4066 36214 36141 29672 30482 30925  3772 32419 41424 23588  9489
  2078 34750 44484 35310 13452 28652 43592 17001 24798 34758 34802 29712
 24730 37062 30184 19934 35246 24609 16376 27706 24587 34520 22083  1330
 21906 16957 49212 46298 10256 16676 44098 16678 19502 43109 49890  8200
 10243 20018  8226  7784 49285 46316  1604 31546  1444  7917 37427 35234
 16560  5536 14866 43234  1940  5638 44090 10297 20048 31252  5110 31108
 45917 34328 44078 47600 23824 29930 25790  9098 12377 32814 17958 40948
 48432 23911 17890 39256 16030 15450 40824 17819 47340 35616 47290 32573
  3268 43826 25854 26560 41334 36006 12000  9433 23538 33412 41226  3536
  9344 41218 26940 18220 45332 41540 43852  6658 18609 48242 47759 26471
  3390  4030 41538  8480   340 38409  7270 43658   584 10994 22779  8670
 44456 13138 25156 10048 27503  1047 21598 42566 22643 24198 30751 22607
 31840 16198  2148 46857  7259 42334 48945 47237 24036 32475 40654 12752
 47160 47094 12830  7080 32342 34048 38339 40527 25306  7146  5912 36686
 21382 35464 15402 16130  2576  2676 43454]
[   25    28    67 ... 49910 49916 49945]
* Critical examples for prunings = 1275
* Critical examples for training = 595
* PrAC images = 1600
******************************************
pruning state 8
* remain parameters = 44863.0
******************************************
* remain weight =  16.77698498175073 %
0.1
* training images number = 1600
Epoch: [0][0/13]	Loss 1.4322 (1.4322)	Accuracy 45.312 (45.312)	Time 0.07
train_accuracy 60.000
Test: [0/79]	Loss 0.9571 (0.9571)	Accuracy 67.969 (67.969)
Test: [50/79]	Loss 1.0643 (0.9981)	Accuracy 64.844 (67.004)
valid_accuracy 67.930
Test: [0/79]	Loss 1.0191 (1.0191)	Accuracy 64.062 (64.062)
Test: [50/79]	Loss 0.8866 (1.0099)	Accuracy 74.219 (65.748)
valid_accuracy 66.260
Epoch #0: Train Accuracy: 60.0, Validation Accuracy: 67.93, Test Accuracy: 66.26
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/13]	Loss 0.8714 (0.8714)	Accuracy 67.969 (67.969)	Time 0.07
train_accuracy 67.750
Test: [0/79]	Loss 0.3399 (0.3399)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.3782 (0.3423)	Accuracy 91.406 (91.621)
valid_accuracy 92.020
Test: [0/79]	Loss 0.3570 (0.3570)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.3293 (0.3475)	Accuracy 89.062 (90.748)
valid_accuracy 90.350
Epoch #1: Train Accuracy: 67.75, Validation Accuracy: 92.02, Test Accuracy: 90.35
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07132826745510101
0.010000000000000002
Epoch: [2][0/13]	Loss 0.6526 (0.6526)	Accuracy 74.219 (74.219)	Time 0.06
train_accuracy 72.688
Test: [0/79]	Loss 0.1234 (0.1234)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1474 (0.1413)	Accuracy 95.312 (97.059)
valid_accuracy 97.310
Test: [0/79]	Loss 0.1511 (0.1511)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1020 (0.1514)	Accuracy 96.875 (96.691)
valid_accuracy 97.110
Epoch #2: Train Accuracy: 72.6875, Validation Accuracy: 97.31, Test Accuracy: 97.11
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008113590069115162
0.010000000000000002
Epoch: [3][0/13]	Loss 0.7651 (0.7651)	Accuracy 66.406 (66.406)	Time 0.14
train_accuracy 72.250
Test: [0/79]	Loss 0.0951 (0.0951)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1157 (0.1129)	Accuracy 96.094 (97.809)
valid_accuracy 97.950
Test: [0/79]	Loss 0.1192 (0.1192)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0560 (0.1208)	Accuracy 99.219 (97.549)
valid_accuracy 97.900
Epoch #3: Train Accuracy: 72.25, Validation Accuracy: 97.95, Test Accuracy: 97.9
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008693132549524307
0.0010000000000000002
Epoch: [4][0/13]	Loss 0.5724 (0.5724)	Accuracy 78.125 (78.125)	Time 0.06
train_accuracy 74.875
Test: [0/79]	Loss 0.0872 (0.0872)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1050 (0.1043)	Accuracy 97.656 (97.963)
valid_accuracy 98.060
Test: [0/79]	Loss 0.1138 (0.1138)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0539 (0.1116)	Accuracy 99.219 (97.580)
valid_accuracy 97.950
Epoch #4: Train Accuracy: 74.875, Validation Accuracy: 98.06, Test Accuracy: 97.95
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006241223309189081
* remain weight =  13.421438401244545 %
* best SA=97.95
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  13.421438401244545 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  663
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  16.77698498175073 %
Test: [0/391]	Loss 0.0807 (0.0807)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1147 (0.0995)	Accuracy 97.656 (98.162)
Test: [100/391]	Loss 0.0990 (0.1018)	Accuracy 98.438 (98.151)
Test: [150/391]	Loss 0.0879 (0.1017)	Accuracy 99.219 (98.220)
Test: [200/391]	Loss 0.0994 (0.1009)	Accuracy 98.438 (98.165)
Test: [250/391]	Loss 0.1021 (0.1008)	Accuracy 98.438 (98.182)
Test: [300/391]	Loss 0.1186 (0.1005)	Accuracy 97.656 (98.160)
Test: [350/391]	Loss 0.1346 (0.1008)	Accuracy 95.312 (98.141)
valid_accuracy 98.132
[29672   212 25566 44340 37704 25232 29711  7898 34234  8226 22278  4692
   132 25188 18864 40527 15728 30142 25520 44456 25657 48638  7758   340
 25412 48538 37256   418  8202 37491  7732 19244 37834    70 44598 48811
 25678  8384 48524 22483 22263  4542 29712 37396  4150 11693 48594 25159
 21906  4618 37546   180 44534 29180 25799 48706   178 18904 29930 29832
 37450  4502  4428 34352 29434 37921 40977 29843  4506 48432  4822 15766
 25318 37672  7917 19040 45048 13709 49464 46154  6658 16836 20816  9471
  1826 12416  6670 42973 42943 39875 46122 19808 12377 46088 38796 49416
 24587 24589 20882 36214 28078 18066 13703  9489 41718 46316 17958 36006
 49535 46298 10458  9583  1978 39801 46274 12494 19888 23538 47421 42854
  5536  6582 31546  3268  1920 35051 31523 26622 20784 24526  9396 31650
  9390 34942 31100 16530 24730 19666 38577 45800  6974 34802 12184 12183
 49264 31108 28368 41464 49252  7009 45749  1374 38526 21130 31000 41438
  1356 23086 23126 40022 43234  3536 24609  9379  9376 20918 43109 31275
 16678 38700 20970 24662 14790 38680 45930 41562 18190 19714 43176 41538
 38640 28260  6905 18220  1512  9235 23194  6755 23594 32776 24432  5868
 46806 24198 27316 10205 23946 12808 35616 32475 32112 10215  2576 17698
 39284 13185 32519 12764 10251 46689 39256 10253 17316 32018  2410 23911
 42312 47034 39378 12932 20210 32323 12940  2720 35464 39482  5972 49904
 17592 49910  2694 39488 32288 27180 39425 39424  5912 24059 10048  5898
  5896 27243 27248  2817 47152 47160 39625  6236  5656 42687 46440  6418
 17890 47318 47321 17079  3136 39734 35916  5632 31738 46406 17061 17059
  6448 23642  5624  9661 20641 47340 31710 13494 20652 13428 40076 35882
 47291 17763 32573 27490 17772 31962 26890 27522  9814  3030 17817 20522
 20018 12663 47237 20534 12651 23730 41912 41904 20572 23710  9727  9721
 35873 47288 46466  3634  6848 37044 28984 18650 14700 41218 38339 47912
  1117 30356 30770 48006 18423  8966  5035 22763 45348 33338 49143 37050
 21445 18405 11759 43874 26017 21302 11894 43510 22562 30574 34676  7584
 38052 34552 22633 14653 26138 49090 16173 36984  8854 36760  4039 38256
  8883 19369 14406 34615 48945 49070  8898 28778 28710  7347 26061  1341
 12000 16130 19360  7259 33364 11781  4028 41284 33612 43592  7530 41276
  7264 11882 30646 21598 23016 47828 16298 41396 34740 14762 25854 10982
 19328 11734  7490 28471 33242  7510  7638 43823 30450 25945  9098 41113
 29121 21636 26266  1330  9114 43826 41424 10750 48102 22531 45654 41382
 49014 34520 38170  8730 15862 19412 38399  8772  7146 10944 14582 33318
 22679 14756 33506 36642 14760 45616 48966  8731 41366 31958 13558 48288
  8532 18609 24938  5740 39074 11104 49548 48649 15961  8729  8488 12752
 27431 14598 41226 37468 47219 22169    80 39260 43818   846 41156 30416
   854 49626  5103  2338 34660 47844 49139 48933 14072  5408  7920 12325
 37966 34678  8617 14172 36425 39423   650 41054 19590 32280 42234 47651
 38401 14972 12967 32372 33088 38650 25863 10800 14804  9436 30870 22470
  9104  5084 45154  7784  6578 28413 22830 28684 37952  1032 21351 11632
 46269 47798 47759  8942 31591 24560 13138   584 32342 41016 25790 22643
 44424 12957 22062 10994 19390 32348 37557 37574 23824  8670 25466 47159
 32507 35310 47094 17728  4030 20350 20048 15948 12830 40654  2554 42334
 34048 22083 17648  8637 20171 46857 26940 17747 22607 44480  2676 32002
 42482 44484 25683 33448  6269  9009 38397 40144  1604  6830 31252 16676
 41334 41594 47600 25306  9344 26471 23336 38374 38370   266 10831 40824
 36686 28652  9433 23406 15660 26560 47646 48402  7842 14124 34758 21124
 34750 44845  1404  5216 24798 14078  7080 49212 28357   376 44852  1444
 16560  3692 16574 43454 45868  1222 30856 45917 10297 30751  5110 14468
 22779 29238 37680 27706 45332 16072 15450  2148 47290 16030 15434 30482
  6347 37469 20016 17819 47217 35234 35246 24310 39184 48975 49573 21900
 44939  5602 28672 40182 13719  7270 32888  1047 13662 26636 33412 35062
 31576 34328 33768 43658 16956 16959  8480 44098 46300 10894  7989 23588
 13524 37427 40066]
[    0    80   132 ... 49904 49945 49960]
* Critical examples for prunings = 1158
* Critical examples for training = 663
* PrAC images = 1493
******************************************
pruning state 9
* remain parameters = 35890.0
******************************************
* remain weight =  13.421438401244545 %
0.1
* training images number = 1493
Epoch: [0][0/12]	Loss 1.6636 (1.6636)	Accuracy 40.625 (40.625)	Time 0.06
train_accuracy 53.516
Test: [0/79]	Loss 1.4525 (1.4525)	Accuracy 42.188 (42.188)
Test: [50/79]	Loss 1.4848 (1.4425)	Accuracy 40.625 (44.164)
valid_accuracy 45.260
Test: [0/79]	Loss 1.4396 (1.4396)	Accuracy 44.531 (44.531)
Test: [50/79]	Loss 1.3212 (1.4845)	Accuracy 54.688 (43.689)
valid_accuracy 44.520
Epoch #0: Train Accuracy: 53.51641009178085, Validation Accuracy: 45.26, Test Accuracy: 44.52
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/12]	Loss 0.9219 (0.9219)	Accuracy 64.844 (64.844)	Time 0.06
train_accuracy 64.367
Test: [0/79]	Loss 0.4750 (0.4750)	Accuracy 85.938 (85.938)
Test: [50/79]	Loss 0.5112 (0.5285)	Accuracy 84.375 (85.263)
valid_accuracy 85.730
Test: [0/79]	Loss 0.4951 (0.4951)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.4277 (0.5840)	Accuracy 91.406 (82.996)
valid_accuracy 83.810
Epoch #1: Train Accuracy: 64.3670464456281, Validation Accuracy: 85.73, Test Accuracy: 83.81
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07188631594181061
0.010000000000000002
Epoch: [2][0/12]	Loss 1.0214 (1.0214)	Accuracy 63.281 (63.281)	Time 0.07
train_accuracy 69.324
Test: [0/79]	Loss 0.1944 (0.1944)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1908 (0.1979)	Accuracy 97.656 (96.553)
valid_accuracy 96.930
Test: [0/79]	Loss 0.1693 (0.1693)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1100 (0.2175)	Accuracy 100.000 (95.680)
valid_accuracy 96.480
Epoch #2: Train Accuracy: 69.32350991639369, Validation Accuracy: 96.93, Test Accuracy: 96.48
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00835887435823679
0.010000000000000002
Epoch: [3][0/12]	Loss 0.6823 (0.6823)	Accuracy 75.781 (75.781)	Time 0.06
train_accuracy 68.721
Test: [0/79]	Loss 0.1695 (0.1695)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1581 (0.1639)	Accuracy 97.656 (96.293)
valid_accuracy 96.690
Test: [0/79]	Loss 0.1334 (0.1334)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0678 (0.1718)	Accuracy 100.000 (96.155)
valid_accuracy 96.850
Epoch #3: Train Accuracy: 68.7206967629128, Validation Accuracy: 96.69, Test Accuracy: 96.85
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00941766519099474
0.0010000000000000002
Epoch: [4][0/12]	Loss 0.6664 (0.6664)	Accuracy 71.094 (71.094)	Time 0.13
train_accuracy 70.596
Test: [0/79]	Loss 0.1414 (0.1414)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1332 (0.1415)	Accuracy 98.438 (96.936)
valid_accuracy 97.210
Test: [0/79]	Loss 0.1175 (0.1175)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0519 (0.1503)	Accuracy 100.000 (96.645)
valid_accuracy 97.260
Epoch #4: Train Accuracy: 70.59611556199438, Validation Accuracy: 97.21, Test Accuracy: 97.26
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008916132501326501
* remain weight =  10.737150720995636 %
* best SA=97.26
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  10.737150720995636 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  713
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  13.421438401244545 %
Test: [0/391]	Loss 0.1865 (0.1865)	Accuracy 93.750 (93.750)
Test: [50/391]	Loss 0.1489 (0.1398)	Accuracy 97.656 (97.135)
Test: [100/391]	Loss 0.1283 (0.1399)	Accuracy 96.875 (97.293)
Test: [150/391]	Loss 0.1181 (0.1408)	Accuracy 97.656 (97.216)
Test: [200/391]	Loss 0.1164 (0.1398)	Accuracy 97.656 (97.233)
Test: [250/391]	Loss 0.1171 (0.1422)	Accuracy 96.875 (97.127)
Test: [300/391]	Loss 0.2219 (0.1419)	Accuracy 94.531 (97.205)
Test: [350/391]	Loss 0.1511 (0.1414)	Accuracy 97.656 (97.247)
valid_accuracy 97.228
[  524 33906 10570 19888 34406 48975 22240 35062 49416 15114 19360 37574
 18066 12302 22199 15338 35051 12325 37491 25976 22062 37704 49426 25614
 47421 49064 44340 26560 25954 44338 11781 22203 40654 49464  5430 26622
 44098 10982 25520 49535 18598 29711 25945 11378 49026 32835  3907 10944
 37358 41742 32814 10458 34422 48649 11759 41718 33506 48706 30184 48524
 24589 29672 41688 19404 37124  7917 47538  4542 11802 19369 18930 22083
 29238 48594 26017 24501 24560  4355  4986 49070 22335 33318 15188 18405
 44383 49153 40076 22217 34740  4150 37450  4039 25232 48933  7842  3536
  4030  4028 12153 26358 10677 18228 48356 25868 12181 29843  7784 41366
 48912 25159 25178 25829 34322 49212 29832 47912  3692 41382 22169 34676
 41396 47936 26266 11639 44845 26275 37256 47844 40976 49252 47828 10831
 49139  4692  5278  8202 37838 34824 37198  3998 41562 34644   178   418
 49090 26458   180 44915 34841    70 25657 25318  4618 15660 41226 34520
 48966 33812 25683 15434 37469 33088 18776 22130 33768 26398 11565 40022
 48945 34234 41276 33359 41618  5248 24730 19666 34802 18190 40208 37834
 41540 29434 48402  8384  8200 29311 47798 12692 45332 35916 46406 23642
 20628 13452 35882 35873 13428 31736 17001 17059  8772 22779  9661 17079
  2158 42665 23730 17120 14468 17061  6269  6418 46368 21351 46274  9504
 23538  1920 23546 46298 42854 38256  2014  1940 46316 13558  7259 43658
 16130 23588 30646 23594  7270 22830  6524 30574  4460 32108  9952 17401
 27323 24006 42334  8680 27293  8676 14540  5972 35480 32202 35464 15961
 39424 12967 49960 10044 10048 24059 31840 46806 23946 42566 17216 17220
 27522  8730  9814 27490 31962 14819 20350 46689  6130 39256 32018  2410
 17316 17326 39284 39297   846 13185 49945  9489 30692 38397  9104 45800
 38526 23126  1404 38390 28357  6905 36446  1444 31100 31108 45868 16530
 30833  1495  6848 38374  1512 16298  7114 38399 30870 30925 36522 16376
 28471 14172 14124  7009  9046 43439 21124 30983  1341 28422 45654 45652
 31000 28413 14084 14078 14072 45749 31523 43510  8940 28072 43574  6670
  9379 38796 13719 28672 23422 46154 28078 13709 28684  1058 20816 23458
  8854  1047 16173  6578  9471  1784 31173 20882 21302 31185 38640 38359
 31197 43176 38650 45965 45968 20976 30751 20970 38680 20959  6755 28162
  1117 46024 30770 16676  8898 45980 15948     0 14764 37044  8532 12834
 14653 22531  7490 37050 38050 12836 47227 20016 17764 12663 12830 29121
 37966 38052 32345 39662 10282  5656  5718 39683   584  2720  5638 20169
  8617 17772 20018 27085  5806   650 32475 32248  7528 32507  2901 39625
 10205 24250 10241 30312 42078  5704  5771 45154 32519  2817 24310 17739
 10253 30356 17747 47217 47219 32417 47094  7510 12808 10156 20171 47288
 22562 14796 41904 14790 42221 12522 47340 17861 49904 47376 27180 36936
 29180 10315 39473  5896 39734  2676  5554 32288 19934  8480 32280 22483
 49626 26740 30416 12932 12934 47290 37921  5624  2707 45056 37952 39488
 26842  3030 43850 39482  3136 17819 14804 42234 22470 45048  7638 15766
 28984 11693 34843 29712 45761 10600 47651  4428 15728  5536  8853 30962
 35068 25648 30123 14238 48432 47291  3268 26636 40514  6808 32888 24587
 30142 36425 47488 18904 17763  4153 20918 36214 46066 15324  4129 40977
 21636 24609 24526 48538 21130 14886  9390 12940  8468 17592 49143  2554
 20641  6102 17426 17645 21445 46440 36934 17648 24938  6246 41438 35851
 42687 23710 43874 20522 48102 41156  6236 31650 34771 24997 26061 40144
 16116 35688 10746 27502 19328 14700 37396 12957 48006  5110 29930  7920
 28260 49285 19590 41284 22633 42482 40066 23911 34552 12416 32002 13138
 35616 17817 24798 16560 18864 49890 33364 44090 47160  5035 18423 43454
 43818  8670  2576 49573  1330 30968 41334  5084 34048 14598  5103 14582
  4446 43852 34660  5912 41912 12000 42312 20322 44484 41894  9098 25306
 41424 49910 10800  7080  1374 27316 19502 46857 22643 40752 17958 38401
 45917 12377 44456 25156 41538 26940 10251 41113 10256 43592 36686 34328
  9396 35310 37427 47600   132 15402 29041  8226 25863 35234 17698 36006
 20048 31576 10215 26471 21598 12752 10994 36714  7264 28710 18609 15450
 13662  9433 33412 27706 46466 39184  7146 34750 49067 14760 21900 27503
  1143 32342 10297 25294 40527 25678 41218 16030  1604 46122 32573 38700
 12183 25799  2148 25790 46088 19244 18220 28652  9344 47759  7758  5216
 22607 43109 34758 38339 35246]
[   25    28    29 ... 49945 49955 49985]
* Critical examples for prunings = 1582
* Critical examples for training = 713
* PrAC images = 1939
******************************************
pruning state 10
* remain parameters = 28712.0
******************************************
* remain weight =  10.737150720995636 %
0.1
* training images number = 1939
Epoch: [0][0/16]	Loss 1.6352 (1.6352)	Accuracy 39.844 (39.844)	Time 0.06
train_accuracy 60.650
Test: [0/79]	Loss 1.5846 (1.5846)	Accuracy 30.469 (30.469)
Test: [50/79]	Loss 1.6472 (1.5393)	Accuracy 25.000 (34.758)
valid_accuracy 35.530
Test: [0/79]	Loss 1.5008 (1.5008)	Accuracy 33.594 (33.594)
Test: [50/79]	Loss 1.4305 (1.5607)	Accuracy 45.312 (32.705)
valid_accuracy 33.880
Epoch #0: Train Accuracy: 60.6498194709766, Validation Accuracy: 35.53, Test Accuracy: 33.88
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/16]	Loss 0.7795 (0.7795)	Accuracy 73.438 (73.438)	Time 0.06
train_accuracy 69.881
Test: [0/79]	Loss 0.4295 (0.4295)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.4504 (0.4303)	Accuracy 90.625 (89.844)
valid_accuracy 90.520
Test: [0/79]	Loss 0.4652 (0.4652)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.3079 (0.4333)	Accuracy 92.188 (89.721)
valid_accuracy 90.260
Epoch #1: Train Accuracy: 69.88138212033803, Validation Accuracy: 90.52, Test Accuracy: 90.26
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08839510381221771
0.1
Epoch: [2][0/16]	Loss 0.8455 (0.8455)	Accuracy 69.531 (69.531)	Time 0.07
train_accuracy 72.305
Test: [0/79]	Loss 0.1895 (0.1895)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.2144 (0.2122)	Accuracy 98.438 (96.002)
valid_accuracy 96.160
Test: [0/79]	Loss 0.2062 (0.2062)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1973 (0.2264)	Accuracy 93.750 (95.297)
valid_accuracy 95.470
Epoch #2: Train Accuracy: 72.30531200076453, Validation Accuracy: 96.16, Test Accuracy: 95.47
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06450264900922775
0.010000000000000002
Epoch: [3][0/16]	Loss 0.6466 (0.6466)	Accuracy 79.688 (79.688)	Time 0.07
train_accuracy 75.245
Test: [0/79]	Loss 0.1461 (0.1461)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1335 (0.1581)	Accuracy 95.312 (95.343)
valid_accuracy 95.660
Test: [0/79]	Loss 0.1646 (0.1646)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0782 (0.1666)	Accuracy 99.219 (95.129)
valid_accuracy 95.870
Epoch #3: Train Accuracy: 75.24497161912451, Validation Accuracy: 95.66, Test Accuracy: 95.87
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009960992261767387
0.010000000000000002
Epoch: [4][0/16]	Loss 0.7144 (0.7144)	Accuracy 71.094 (71.094)	Time 0.07
train_accuracy 76.844
Test: [0/79]	Loss 0.1114 (0.1114)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1204 (0.1308)	Accuracy 96.094 (96.461)
valid_accuracy 96.730
Test: [0/79]	Loss 0.1395 (0.1395)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0596 (0.1371)	Accuracy 99.219 (96.415)
valid_accuracy 96.960
Epoch #4: Train Accuracy: 76.84373388344507, Validation Accuracy: 96.73, Test Accuracy: 96.96
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009752020239830017
0.0010000000000000002
Epoch: [5][0/16]	Loss 0.7956 (0.7956)	Accuracy 75.781 (75.781)	Time 0.12
train_accuracy 78.236
Test: [0/79]	Loss 0.1083 (0.1083)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1126 (0.1246)	Accuracy 96.875 (96.584)
valid_accuracy 96.830
Test: [0/79]	Loss 0.1322 (0.1322)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0581 (0.1299)	Accuracy 98.438 (96.599)
valid_accuracy 97.090
Epoch #5: Train Accuracy: 78.23620424078813, Validation Accuracy: 96.83, Test Accuracy: 97.09
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0011145166354253888
0.0010000000000000002
Epoch: [6][0/16]	Loss 0.5013 (0.5013)	Accuracy 82.812 (82.812)	Time 0.06
train_accuracy 77.669
Test: [0/79]	Loss 0.1053 (0.1053)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1106 (0.1217)	Accuracy 97.656 (96.737)
valid_accuracy 96.920
Test: [0/79]	Loss 0.1290 (0.1290)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0552 (0.1268)	Accuracy 98.438 (96.752)
valid_accuracy 97.260
Epoch #6: Train Accuracy: 77.66890150742041, Validation Accuracy: 96.92, Test Accuracy: 97.26
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009752020705491304
* remain weight =  8.58987016095255 %
* best SA=97.26
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  8.58987016095255 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  855
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  10.737150720995636 %
Test: [0/391]	Loss 0.0723 (0.0723)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.1158 (0.1118)	Accuracy 96.875 (97.304)
Test: [100/391]	Loss 0.1095 (0.1160)	Accuracy 96.094 (97.123)
Test: [150/391]	Loss 0.0741 (0.1162)	Accuracy 100.000 (97.118)
Test: [200/391]	Loss 0.1368 (0.1157)	Accuracy 96.094 (97.097)
Test: [250/391]	Loss 0.1115 (0.1154)	Accuracy 96.875 (97.136)
Test: [300/391]	Loss 0.1658 (0.1152)	Accuracy 94.531 (97.124)
Test: [350/391]	Loss 0.1341 (0.1154)	Accuracy 97.656 (97.122)
valid_accuracy 97.118
[ 5624 26398 10600 47597 41668  8680  5718  5632  8676  5536 11364 15338
 14866  7758 12302 15862 24360 26376  4542 47421 18166 18930 40704 18066
 11242 48402  3536 29041 25306 35068 37966 40728 48594 48524 25159 30123
 41464 12321 25102  3510 44340 47488 47383 18982 11306 40592  8688 35062
  7768 25232 41618 24804 24497 11786 29311 25954 24722 25948 15209 49070
 25945 22188 49067 11797 48912 11992  3998 34520 47888 11802  5084  7994
 33362  3907 41226 11068 19360 44098 33722  8384   180 19369 22212 25683
 48899 15652 40076 22199 33359 24887 47926 37396 25868  5216 18609 48975
  4129 40171 18598 22130 25817 41060 24798 34615 25829 48006 34644 24742
 37358 47966 15114  5103 40976 48102 19404 22169  4028 19412 19502 37347
 34552 44598 11953 33612  8468  4153  8532 18423 47828 37198 15766 41382
  3719 49233 24560 19666 18832 21636 37491 11639 34406 24997  4986 33812
  3696 24990 19244 47651 49285  8202 11565 41438 19714 48356 48344 41424
 24526 33906  8617 49252 48750 22320 18864 15779 15748 18322 34422 37468
 29238 11104 41276 37256 19328 15494 49153 26061  5035 40022 33752 34771
 49139 11734 37469 41284 18378 24630 26138 34843 40858  3772 22278 34841
  7917 18220 34829  3794 34824 12153 44046 24609 47759 49164  3791 26515
 26749 19888 23730 27265 30692 42312 22763 14406 13185 39256 32108 46689
 17426 20308 49960 17440  6269  2636 27180 24310 10048 46724 49945 16298
  1143 35734 32018 16376 46432 28471 35882 31840 35873 23593 23594  9104
 36653 39289 46466 27413 23642  6448 30751 46538 31954 20412 31962 35777
 43439 14337 13428 43510  6246  7498 28710 36836 46857 32323 17592 23946
 32342 32348 22643 20169 32372 22633 12940   934 21351  8908 35464 17645
 36875  2901 16173 23911  8942 43574 35622 42221 36783  6236  2694 32202
 49910  1032 49904  2720  2661  8966 32236 49890 28672 22679 23886 16192
 45294 36822 32280  7490 10089 16116 27514 30811  9489  1495 27964  7114
 23016 13828 36214  6848 20784 46021 46023 14124 27912 21034 38526 42854
 16530 13752 30983 13735 43234  6905 36446  1512 23194 28162  1612  9376
 45777 23126  9390 38650 31185 31100 20970  6755 23136 20976 45868 28078
 36425  7080 45749 38680 42992 14072 38700 23140 27522 13703 14172 16444
 13558 14238 28399 30870 46298 21130 28413 31710  7259  6582 31738  7264
 30833 38408 23538 42566  2250 23546 35916 13452 17059  1330 46274 31650
 36104  1400 30952 30951 16957 16488 23385 13662  2014 13650  1404  9661
 27722 28357 23422 27706 31596 20641 31609 20637 42687 46269 31576 10215
  4712 36982 32507 15948 10287 14760 22483 26852 47034 10334 12834 35234
 49535 37038  3210 14764  5806 10282  7584 12836 45026 43818 43823 24059
 49548 10273 47219 47288 12646 45056 24101 12792 10309 17763 22561 15961
 26622 36936  5972 26629 37026 47237  3030 32519 38050 47260 28956 17747
 19960 41912 26842  8853 41904 47241 22470 49644 17772 10458 32604 37062
 20018 10243  3136 32417 10241 26756  7638 21445 38170  2942 47340  6050
 15913  8881 20016 43658 43670  5912 45185   788 47217 15924 20146 14582
 43852 49626 41718 30450 24479 26740 20663 33088 17003 48945 17216 23406
 48933  5554  4150 11693 45048 27503  1356  5598 40948 13524 46316 37962
 17112  2158 37704 49573 30142  1269 15991 47160 47159 38397 19782 20522
  8772   547 43328 31608 27502 41772 12325 17079  3470  5821 28984 26458
 48432 25156  9433 14790 28072 32888 43874 45761  5771 31197 32814 40514
 40612  8729 14804  4716 23086 11378 45800 32835 12522 14084 20930 41688
 39734  7005 47376 28180 44424  9340 12416 45944 20816 18776  5638 48242
 46154 36522  9627 12422 13709  9235 26275 30962 30968 29713 13719  1940
 45654 10570 19934 43826 13831 29048 24250 44338  6885 37044 40752  3268
 44484 40386 46076  9583 42866 25799 22284 35851 39327 12752 33378 27085
 26017 30574 28684 32276 27316 20226 25854 39260 37838 10831 42140 27292
   854  3692 36714 14653 45143 34676 10315   506 41156 36760 32475  2576
 27251 45154 22335 32168 42234 38256  8854  5278  2488 26184 20322 28778
 36866  2410 30770 27323 17648 39425 33250 10205 16130 27431 29121  4122
  6466 12936 12934  3810  5430 22664  8898 30448 17814 19590 39482 39424
  6130 14540 39473 12957 39423 35480 31958 46991 19322 12000 20959 47291
  7920 29731 45348 35051 33860 22203 40527 22217 37834    70 39801 29711
 13138  6578 34942 39074 40182 33552 38339 42482 25863 22830  6670 36730
  8226 34234 10256  7732  6524 20882 26471 49143 14078 17958 39683 40654
 20210 18190 36686 24198 16560 34758 22779 35310 40144 21900 14700 47290
 39184 16574 31301 39488 18405 10251 25412 43592 40977   266 16676  7009
 17819 38374 12181 43109  1222 25790  1604 48966 41334  9098 26940 37574
  9344 17817 12183 10994 10750  6974 37427  7842 26560  5110 15402 39875
 20171 24587 34802 22531  6418 22607 23588 34048 34750 47094 26636  2148
   132 23824  2554 25678 10297 19808 12377 29930  2676 38370 11632 35616
  5602  1047 32573 16956 37450 47646  8480 24504 20628   584 44852  4030
 41538 14468  9009 36006 22062 47912  8670 34660 35688 16959 10894  7270
 21124  5656 21598  7146 41218 44939 34328 32248 16030 34740  8200  6830
 46806 22083 17890 47600 35246 43454 12830  4460 15450  4428 33506 49090
 29712 20048 33412 20350 45332 39625 23336 15660  1920 28654 17739 42334
  1444 28652 25546]
[  132   178   182 ... 49945 49960 49993]
* Critical examples for prunings = 1659
* Critical examples for training = 855
* PrAC images = 2116
******************************************
pruning state 11
* remain parameters = 22970.0
******************************************
* remain weight =  8.58987016095255 %
0.1
* training images number = 2116
Epoch: [0][0/17]	Loss 1.4831 (1.4831)	Accuracy 39.844 (39.844)	Time 0.06
train_accuracy 61.531
Test: [0/79]	Loss 1.8704 (1.8704)	Accuracy 38.281 (38.281)
Test: [50/79]	Loss 1.9455 (1.7893)	Accuracy 33.594 (42.233)
valid_accuracy 42.810
Test: [0/79]	Loss 1.7099 (1.7099)	Accuracy 45.312 (45.312)
Test: [50/79]	Loss 1.7540 (1.8057)	Accuracy 45.312 (40.441)
valid_accuracy 41.830
Epoch #0: Train Accuracy: 61.5311909118537, Validation Accuracy: 42.81, Test Accuracy: 41.83
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/17]	Loss 0.8551 (0.8551)	Accuracy 67.969 (67.969)	Time 0.07
train_accuracy 68.478
Test: [0/79]	Loss 0.4380 (0.4380)	Accuracy 91.406 (91.406)
Test: [50/79]	Loss 0.5007 (0.4288)	Accuracy 89.844 (91.866)
valid_accuracy 91.900
Test: [0/79]	Loss 0.4410 (0.4410)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.3401 (0.4615)	Accuracy 97.656 (89.614)
valid_accuracy 90.440
Epoch #1: Train Accuracy: 68.47826088398752, Validation Accuracy: 91.9, Test Accuracy: 90.44
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08358728885650635
0.1
Epoch: [2][0/17]	Loss 0.8823 (0.8823)	Accuracy 63.281 (63.281)	Time 0.07
train_accuracy 71.692
Test: [0/79]	Loss 0.4674 (0.4674)	Accuracy 82.812 (82.812)
Test: [50/79]	Loss 0.3539 (0.3517)	Accuracy 87.500 (87.745)
valid_accuracy 88.270
Test: [0/79]	Loss 0.2594 (0.2594)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.2346 (0.3944)	Accuracy 92.969 (85.447)
valid_accuracy 86.780
Epoch #2: Train Accuracy: 71.69187146999886, Validation Accuracy: 88.27, Test Accuracy: 86.78
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06878536939620972
0.010000000000000002
Epoch: [3][0/17]	Loss 0.7169 (0.7169)	Accuracy 75.781 (75.781)	Time 0.07
train_accuracy 74.575
Test: [0/79]	Loss 0.0983 (0.0983)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1315 (0.1183)	Accuracy 96.094 (96.967)
valid_accuracy 97.400
Test: [0/79]	Loss 0.1067 (0.1067)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0443 (0.1314)	Accuracy 99.219 (96.691)
valid_accuracy 97.400
Epoch #3: Train Accuracy: 74.57466924483474, Validation Accuracy: 97.4, Test Accuracy: 97.4
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.010883761569857597
0.010000000000000002
Epoch: [4][0/17]	Loss 0.6657 (0.6657)	Accuracy 75.781 (75.781)	Time 0.06
train_accuracy 78.025
Test: [0/79]	Loss 0.0985 (0.0985)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1212 (0.1050)	Accuracy 98.438 (97.702)
valid_accuracy 97.950
Test: [0/79]	Loss 0.0995 (0.0995)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0406 (0.1127)	Accuracy 100.000 (97.335)
valid_accuracy 97.930
Epoch #4: Train Accuracy: 78.02457471245404, Validation Accuracy: 97.95, Test Accuracy: 97.93
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00844579841941595
0.0010000000000000002
Epoch: [5][0/17]	Loss 0.6891 (0.6891)	Accuracy 78.125 (78.125)	Time 0.06
train_accuracy 76.512
Test: [0/79]	Loss 0.0944 (0.0944)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1138 (0.1007)	Accuracy 98.438 (97.748)
valid_accuracy 97.960
Test: [0/79]	Loss 0.0926 (0.0926)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0370 (0.1095)	Accuracy 100.000 (97.457)
valid_accuracy 98.050
Epoch #5: Train Accuracy: 76.51228734901586, Validation Accuracy: 97.96, Test Accuracy: 98.05
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008707009255886078
0.0010000000000000002
Epoch: [6][0/17]	Loss 0.5630 (0.5630)	Accuracy 71.875 (71.875)	Time 0.06
train_accuracy 79.159
Test: [0/79]	Loss 0.0931 (0.0931)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1089 (0.0987)	Accuracy 98.438 (97.733)
valid_accuracy 97.980
Test: [0/79]	Loss 0.0908 (0.0908)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0357 (0.1071)	Accuracy 100.000 (97.442)
valid_accuracy 98.070
Epoch #6: Train Accuracy: 79.15879019897692, Validation Accuracy: 97.98, Test Accuracy: 98.07
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0007836308213882148
* remain weight =  6.871896128762045 %
* best SA=98.07
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  6.871896128762045 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  986
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  8.58987016095255 %
Test: [0/391]	Loss 0.1070 (0.1070)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1222 (0.0970)	Accuracy 97.656 (98.146)
Test: [100/391]	Loss 0.0973 (0.0973)	Accuracy 98.438 (98.252)
Test: [150/391]	Loss 0.0868 (0.0984)	Accuracy 97.656 (98.148)
Test: [200/391]	Loss 0.0786 (0.0969)	Accuracy 98.438 (98.158)
Test: [250/391]	Loss 0.0660 (0.0976)	Accuracy 98.438 (98.114)
Test: [300/391]	Loss 0.1643 (0.0975)	Accuracy 94.531 (98.147)
Test: [350/391]	Loss 0.1102 (0.0975)	Accuracy 96.094 (98.170)
valid_accuracy 98.116
[  584 12522 48242 14866 42312  5103  5062  2488 43826 37962 30184 25546
  8731 30123  4460 31962 31958 28954   551 43874  8729 10486  6524 25498
 47288 43823 37062 15913 40268 22284 23594 35851 37966 41594 24198 33812
 24210  5806 44954 17326  8772   588  5084  4428 15991 43818 27292 46432
 44939 48245 48228 47291 20412 28956 31954 27323 41562  6582 42337  8701
  4562   544 19040 10205   178  8200 24014 10215 26896 29410  4692  2901
  8207 44480 26890 34234 32571 37468 24006 29713 29711 37672 21982 20169
 12934 32348 20048 24052  4909 22062  4642  2818 29341 15209 48432 23824
 37413 32604 17645  8470  2855 32323 17861 37704 32342 21772 20171 32345
 11306 40434  4716 39488  6102 10282 37546 17763 32475 29530 37552 15450
  6092 10287 29538 11364 21906 26749 10297 25159 15412 10309 12834 32511
 32507 10277 10273 39473 48524  2934 35480 10241 10334 40527 26852 48594
 32417 39482 23911 20224 15338  6130 48512 21944 10256 17819 17736 17739
 19089 39425 20116 35464   547 46806 22083 22212 47237 19960 22203 22200
 22199 11196   444 35708 15778 13194  7994 37256 15766  8617 32108 10044
 29180 29181 32112  4542 33906 35688 40704 35234 19934  7920 48270 35777
 32002 26504  6448 43898 35770  8680 32018 11632  5876 26515 15826 49514
 47260 35224 24940 42262 43934 17440 22169 21636 49548 49890 12692 32202
  5954 11242  3210 47159 27026 39289 39297  6269 25318  8532 15660 32236
 35310  5972 18982 39309 17591 17592  2720 39260   340 10089 37838 46689
  2661 26572 17494 23710  6347 11565 15728 24990 20226 39573  2694 15114
 29238 22130 49910 46724 27074 19244 44046 42140 22139 32892 40208 47488
 18228  6808  9661  7347 23086 45501  5656  1032 22779 12000 24742  9471
 19782  1330 30846 43510 24630 45868 36104 42687 20663 31517 27706 39801
  7528 22607 41396 27732 13719  6830 21130 14307 13709 38370 22595 41276
 11992 36642 38374 46122 23273 28357 36836 24722 13662 25854 39864 38640
 18190  4024 49067 22531 21324 28684 42616 31608 10884 20628 20882  8940
 12184 19808 39752  9727 35051 40144 36620  6755 31596 47888 36866 24479
  7584 12377 38397 21302 25683 28652 20652 30525 28654 31576  8966 14582
 43234 17061 16766 16488 28672 25648 28574 22763  4122 36214 42793 47651
 42866 23126 38283  3696 23194 46021 22664 46023 28413 47597 47926  3692
  6974 33552 27912  5554 31301 45992 36730 31345 49233  9104  9114 25790
 26138 45980  9098 47646 33506 40066 45944 49009 23136 47936 13828 28471
  1190 10944 36268 34615 39875 42878 14468  4158 36783 25829 41284 13752
 22633 45472 12325 40976 46076 41382 21134 30810 30811 36653 48968 43454
 38365 38798 14341 16959 41060  1269 13896 10746  5602  6885 10750 30794
 41366  4066  5536 26184 22750  1940  4153 21232 49285 16956  7374 16298
 38256 41464 18322 47759 34520  8881 28778 27526 46298  8854 45048 33752
 45777  5155  7722  8853 12494  1604 25976 19369  1444 23011 47844 30952
 46269 40858 47383 16716 38050 18405 49090 47798  7259 33722 27472 22483
 11786  3510 36425 11775 34829 35102  1552 46316  7719 36984 46314 36982
 49398 43670 43648 31736 26358 14760 28078  9384 36446 14762 22453 31024
 33288 16574 17220   846 38553 11781 14084  7112 14700 41538 49153  5718
 22838 24501  1400 28141 48899  5174 37050 49121 21351 43592 18166 31185
 30450 13494 11044  5704 17112 12416 31197 14796  2250 28152 28898 31200
 49139 14238 24609 24250 21445  7146  3907 27431  1404 11734 23016 35068
 40171  7264  1495 25954 28260 20970 41226 28162  1512  7270 20976 20534
 16130 14655 41156 49083 30925 19412 26392 22845 38526 42986 14650 33359
 47376 14790  7732 16530 41113 45026 23886 17772 39940 45917 14078  6251
  6236 48681  1222 26636 34676 44338 48402 12836 44098 37574 42854 10831
 14124 20816 21900 32519 17890  6246 29594 20018  5430 28180 16408 32280
 13026 32288 38680 20210 42992   180 40022 30856 29672  8202 10243 15324
 45586 38650 38399 33448 29731 44484 30870  4028 25102 41094 29712 18382
 12940 49070 11438 32419   132 22830 33242 43198 31216 45815  3810 20146
 26842 30951 31252 20110   212 41904  9376 49212  9489 12183 47912  3772
 34824  8488 45654 24526 40592  8480 37396  8468 48638 30833 12153  8384
 26017 30962 17817 17648 49164    70  9390 16444 17814  7005 47828 30968
 27413 12830 24432 27254 15862  6466 34552 31650 47290 36934 22278 18864
 29048 34406 36936 46538 29041 14653  3536 43852  8908 43850  9952 48102
 37952  5035   934  7917 24310 33615 46154 41424 31546 42665 22561 10458
 29121 17426  6418 22554 48288 41438  2554 38877 20641   506  8676 17079
 45185 16173  8942 11639 42334 11802 16116 41618 49464 24887 48811 14740
 15961 20522 26376 35873 14764 47387 37076 35882 13452 35916 31840 11111
  6578 39683  7768 11104 19360 14804  4362 44915 33612  2410 15948 30416
  8898 30142 24804 13558 39734 36006 22470 46288 13538 23406   854 27522
  6670 28788 19328 27316 13524 16084  7842 23422 18832  5821 23538 49535
 23336 45348  6905 21688  4955 24101 47219 10048  4150   376 29881  2676
  9627 33088  4129 18598 40977 19502 16957 48006 37834 46066 12321  3268
 26622 41742 48344 15779 39832 47538 24997 39031  1920 17001 23730 32168
 49252 45968 37347  8093 27722 49904 18609 36714 20322 27085 41718 46274
 10146  2158 32814 17747 48356 46991  1784 29930 28422  8122 15924 14172
 30692 36760 19322 11378 40948 25863 44598 17059  5632 32888 10894 22263
 42234 37044 20016 41912 47160  9344  6050 44090  5110 25156 13831 39256
 34802 34328 23140 10800 39423 38700 38339  7758 33378 15402 14072 46466
 49416 25306 36686 43109 42482 47340 16676 34660 47600 35616 41540 37026
 18423 16030 30751 43658 30770 27503  3030 47966 24059 41334 35246 25294
 25678  5638  8670 26560 48966 32573 46857 47217 16376 31609 26940 27248
 15748 28710  9433  4030  8226 38401 17958 10994  2014 25412 21598 18220
  1058 47094 37450 39184  5912 20350 49960 12752  2576  5216 37427 41218
  7638 45154 24798 13138 40752  1143 21124 38772 40654 46828 26471 40514
 34758 24587 11797 16560 34644 34750  2148 14008 23588 48975 34048 44456
 38359  1121 34740 22643  3998 45332  7080 35062 40386  4446 33412 12302
 42566 10251]
[   28   109   132 ... 49908 49945 49985]
* Critical examples for prunings = 1125
* Critical examples for training = 986
* PrAC images = 1642
******************************************
pruning state 12
* remain parameters = 18376.0
******************************************
* remain weight =  6.871896128762045 %
0.1
* training images number = 1642
Epoch: [0][0/13]	Loss 1.4645 (1.4645)	Accuracy 37.500 (37.500)	Time 0.09
train_accuracy 55.177
Test: [0/79]	Loss 2.2695 (2.2695)	Accuracy 10.938 (10.938)
Test: [50/79]	Loss 2.3396 (2.2353)	Accuracy 8.594 (11.229)
valid_accuracy 11.440
Test: [0/79]	Loss 2.1513 (2.1513)	Accuracy 11.719 (11.719)
Test: [50/79]	Loss 2.2599 (2.2210)	Accuracy 12.500 (12.286)
valid_accuracy 12.250
Epoch #0: Train Accuracy: 55.176613890151884, Validation Accuracy: 11.44, Test Accuracy: 12.25
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/13]	Loss 0.9553 (0.9553)	Accuracy 58.594 (58.594)	Time 0.06
train_accuracy 61.754
Test: [0/79]	Loss 1.0599 (1.0599)	Accuracy 74.219 (74.219)
Test: [50/79]	Loss 1.1101 (1.0295)	Accuracy 75.000 (75.827)
valid_accuracy 76.900
Test: [0/79]	Loss 1.0160 (1.0160)	Accuracy 77.344 (77.344)
Test: [50/79]	Loss 0.9116 (1.0465)	Accuracy 83.594 (76.072)
valid_accuracy 76.550
Epoch #1: Train Accuracy: 61.753958601028124, Validation Accuracy: 76.9, Test Accuracy: 76.55
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08989986777305603
0.010000000000000002
Epoch: [2][0/13]	Loss 0.9813 (0.9813)	Accuracy 62.500 (62.500)	Time 0.06
train_accuracy 66.626
Test: [0/79]	Loss 0.2871 (0.2871)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.3002 (0.2690)	Accuracy 96.875 (96.339)
valid_accuracy 96.630
Test: [0/79]	Loss 0.2818 (0.2818)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.1596 (0.2773)	Accuracy 98.438 (96.186)
valid_accuracy 96.570
Epoch #2: Train Accuracy: 66.62606571769017, Validation Accuracy: 96.63, Test Accuracy: 96.57
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009904222562909126
0.010000000000000002
Epoch: [3][0/13]	Loss 0.6604 (0.6604)	Accuracy 75.781 (75.781)	Time 0.13
train_accuracy 68.270
Test: [0/79]	Loss 0.1502 (0.1502)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1654 (0.1486)	Accuracy 96.875 (97.442)
valid_accuracy 97.660
Test: [0/79]	Loss 0.1694 (0.1694)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0692 (0.1533)	Accuracy 99.219 (97.335)
valid_accuracy 97.580
Epoch #3: Train Accuracy: 68.27040197672129, Validation Accuracy: 97.66, Test Accuracy: 97.58
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.010883761569857597
0.0010000000000000002
Epoch: [4][0/13]	Loss 0.8290 (0.8290)	Accuracy 67.188 (67.188)	Time 0.06
train_accuracy 68.879
Test: [0/79]	Loss 0.1266 (0.1266)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1412 (0.1304)	Accuracy 98.438 (97.472)
valid_accuracy 97.680
Test: [0/79]	Loss 0.1548 (0.1548)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0550 (0.1342)	Accuracy 99.219 (97.381)
valid_accuracy 97.680
Epoch #4: Train Accuracy: 68.8794153006736, Validation Accuracy: 97.68, Test Accuracy: 97.68
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009795385412871838
* remain weight =  5.497591695087656 %
* best SA=97.68
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  5.497591695087656 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  860
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  6.871896128762045 %
Test: [0/391]	Loss 0.1151 (0.1151)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1297 (0.1173)	Accuracy 97.656 (97.886)
Test: [100/391]	Loss 0.1272 (0.1231)	Accuracy 97.656 (97.703)
Test: [150/391]	Loss 0.1024 (0.1244)	Accuracy 99.219 (97.708)
Test: [200/391]	Loss 0.1102 (0.1234)	Accuracy 97.656 (97.742)
Test: [250/391]	Loss 0.1144 (0.1234)	Accuracy 97.656 (97.771)
Test: [300/391]	Loss 0.1407 (0.1230)	Accuracy 96.875 (97.796)
Test: [350/391]	Loss 0.1464 (0.1234)	Accuracy 97.656 (97.788)
valid_accuracy 97.752
[39734 47340 24479 32888 24630 49548  7722  3510 37900  5624 39804 41688
 39752 37952 22453 47479 24560 41562  5632  8468 49488 15778   444 41742
 39914 41718 26516 26515 19888  8488 19782 47646  3210 26392 14804 49398
 44078 49386 19808   547 24432  3390  8470 47651 47488 37966 22470 41618
 12272 32892 14790 45026 37140 12325 29238 39875 29185 29121 30142 14796
 14072 44098  4122  4129 11693  4150 34377 22139 15434  4153 48242 11068
 33722 40948 37450 48288 11632 33752 15188 15190 21944   178 48933 29839
 11734 49006 22199 33552 41094 11802 40280  4024  4028 15486 11104 15114
 48968 19369 41060 37396 19360  4066 25854 48945 21906 10982 25156 37632
 11110 40720 19089  4460 40704 48538 34010 18982 15324 11298 48512 48638
 34099  4502 34048 29672 40654 25412 44484 48594 37552 11378 41113 25498
 48681 21952 37468 48811 33812 40874 25232  8072 48402 29594 15338 18864
    70 15243 37502 37601  4362 40514 44534 25318  4428 11196 33088 19404
 49014 49233 40066 10746 47828  3692 41366 29311  3696 49212 26184 37256
 33288  5174 18378 24804 33304 18382  3756  7842 37834 18322 41382 34740
 26358 18196  3536 34829 34824 18228  5315 12184 49285 33318 40022 15660
 47759 41424  7798 49252  5248 44850 47798 24742 12153 10800  5155 29341
 26017 10884  3907 40208 29930 49070 18514  5035 34552 48006  7917  7920
 25975 37347 18557 29906 34520  5000 49026 22212 24938 29434  5062  8347
   340  8384 34660 33338 22284 41284 12000  3794 22278 40182 47926  5132
 33359 34644  3811 49143 49139 19502  5103 40171 18423 26061 47160 17220
  8898  6418 30692 27624 35873 20572 17112 31806 35851 39074 42616 36730
  9727 20534 31840  2250 28684  6347 16173  1032 43592 42566 27522 31760
 28654 42665 28652 20641 27722 17012 46368 14307 20628 14314 23594 45501
 31710 35916 23730  9661  7259  8908 17061  7264 13428 22830 14341  8902
  2158 39031 42667 22838 27503 21351 17216  2488 23886 28778 17365 14468
 17370 39289   934 16084 35616 35614  7374 46770  9948 27293 45332 36835
 36836 46806  6102 42312  8772 27248  6092 39297 30751 13138 46724 14762
 31934 27472  9814 23770  6269 31958 31962 35734 42482 16130  6170 22763
 27431  7347  6236  2410 32002 22750 32018   966 35688 43658 39256  6251
  8940 20663 30764 28413 16408 30952 30951 31197 23016 38640 28180 20970
 31216 23011 31185 45944  6830 36268 13828 31301 14172 28078 46021 46023
 46024 16716 20882 28141 28471  6885 30962 28357 31024  7005 14084 45777
  1444 45792  6974 31000 38553 16444 28200 14008 21130 23140 21134 28260
 16549 21034  1512  1356 45868  9235  6905  1374 46828 36214  9379  9489
 28533 36620 16298 38359 42866 30811 30810 38877 42854 31546 13662 46269
 31576 31594 38339 46298 31608 42793 16956 16958 46314 30770  1143 36642
  6755 27912 28525  1269  9384  9390 31366 38408 46088 38772 16766 20847
  6710 45616 36104 46122 38390 30856 23385 20816 13709  6670 46154 23406
 36120  9471 23422  1239 22690  4542 27085 35324 10215 22554  8617 39488
 43804 14653 17777 17728  5912 24101 26842 10241  5806 20169 15862 32519
 17592 29041 14582 42140 32348 22561 43850 28898 30450 32342 22591 35310
 22607 32475 10243 20150 12812  8676 32571 28944 17763 20110 17645 43826
 15947 15948 10251 12830 17764 26890 22633 39546 32419 24250 17736 32417
 28956 43898 43818 12836  2901 30416 47159 20146 35246 27026  8680 49904
 10256 10321   854 35480 22664 39378 29048 20224 32280 38050 12967 14528
 16030 30356 39573  7638 12752  2934 24006 15913 10044 32236 15826 14655
 47291 36982 26749 39683 26740 24038 39423 17714   846 15991 24059 10086
  5972 49908 49910 37044 22643   886 24052 41912 20210 45048 27121  6050
 45082 15848 17814 41904 37050 39425  5821 12940 36866 43934 12744 45154
 36984 49960 11639  6582 29530 37413 26896 30794 34406 23458 48228 11044
 29180 44338 48270 13026 40858 26572 28162 40527 25546 29713 11438 17940
  5602 37962 47260  7080 39656 47288 49573 15827 47376 36425 19040 19960
  1400 14764  5704 48432 20018 16676 13719 38798 11111 39801 13752 19244
 38401 15766  8532  8207 17772 25657  5554 29573 37026  8200 49535   558
 35234 47217 45654 13703 46316 33615 37704 36783 23911  3772 49067 14886
 15924  6448 26405 45266 24587  2694 32108  6466 34676 24997 46991  5278
 18166 31850  9627 17440 19590 41156 41226  7898   250 45185  7758 10944
 22240  8853 15961 31954  2928 20652 17001 17326 43510 42703 17958 46857
 40144 38650 47237 43648 25306  1604 39184 12651  5110 43109 10297 20171
 31252 29731 18190 22779 16376  9340  9344 28710 33412  3030 26636 23824
 39260 29711 41334 17494 47290 42334  2554 10750 22679  4562 34750  8777
  2576 44480 36446   376  5216 17890 14078  2676 24798 31108 24310 22062
 12183 47912 35464  8731 22083 18405 23194 29712 17817   584 44456 16574
  9098 20350 17819 16560 34758 21598 26376 14700 10282 23588 37427 27706
 36686  1920 44939 26940 38365 14238 43454  1404 42687 38370 22130 26560
 17739 12416 12377 38374 26471  4030 21900 22169 16959 33612 13538  2014
  6524 32507 14650 36006 41062 48966 19328 15450 24198 48975  1121 25790
 47094  8670  7732   266 39940 29410 25678 28672 46078 10894 22531 41538
 33448 10994 10146  3268 49090 35062 41218  7112  8480 12692 38397 17079
  2148 32880 47600  1222 17747   132 20048 34328 27526  8226  7270 36714
 30184  7146 32573 33506  9433 15402  9009  5656]
[   70    80   132 ... 49908 49945 49960]
* Critical examples for prunings = 1346
* Critical examples for training = 860
* PrAC images = 1790
******************************************
pruning state 13
* remain parameters = 14701.0
******************************************
* remain weight =  5.497591695087656 %
0.1
* training images number = 1790
Epoch: [0][0/14]	Loss 1.6872 (1.6872)	Accuracy 35.156 (35.156)	Time 0.07
train_accuracy 53.352
Test: [0/79]	Loss 2.3318 (2.3318)	Accuracy 10.156 (10.156)
Test: [50/79]	Loss 2.3907 (2.2786)	Accuracy 10.156 (11.596)
valid_accuracy 11.890
Test: [0/79]	Loss 2.2202 (2.2202)	Accuracy 11.719 (11.719)
Test: [50/79]	Loss 2.2633 (2.2620)	Accuracy 12.500 (12.699)
valid_accuracy 12.560
Epoch #0: Train Accuracy: 53.351955452178444, Validation Accuracy: 11.89, Test Accuracy: 12.56
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/14]	Loss 0.9440 (0.9440)	Accuracy 63.281 (63.281)	Time 0.07
train_accuracy 63.017
Test: [0/79]	Loss 1.3622 (1.3622)	Accuracy 59.375 (59.375)
Test: [50/79]	Loss 1.4592 (1.3534)	Accuracy 55.469 (61.795)
valid_accuracy 62.410
Test: [0/79]	Loss 1.3072 (1.3072)	Accuracy 67.969 (67.969)
Test: [50/79]	Loss 1.2713 (1.3719)	Accuracy 71.875 (59.835)
valid_accuracy 61.680
Epoch #1: Train Accuracy: 63.016759921452184, Validation Accuracy: 62.41, Test Accuracy: 61.68
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.09019795060157776
0.1
Epoch: [2][0/14]	Loss 0.8892 (0.8892)	Accuracy 64.062 (64.062)	Time 0.07
train_accuracy 63.631
Test: [0/79]	Loss 0.4933 (0.4933)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.5270 (0.5252)	Accuracy 83.594 (84.283)
valid_accuracy 84.710
Test: [0/79]	Loss 0.5891 (0.5891)	Accuracy 82.031 (82.031)
Test: [50/79]	Loss 0.3684 (0.5495)	Accuracy 92.969 (83.961)
valid_accuracy 84.590
Epoch #2: Train Accuracy: 63.63128483948095, Validation Accuracy: 84.71, Test Accuracy: 84.59
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0714237168431282
0.010000000000000002
Epoch: [3][0/14]	Loss 0.9202 (0.9202)	Accuracy 66.406 (66.406)	Time 0.05
train_accuracy 67.486
Test: [0/79]	Loss 0.2776 (0.2776)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2443 (0.2619)	Accuracy 96.094 (93.336)
valid_accuracy 93.780
Test: [0/79]	Loss 0.3010 (0.3010)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.1416 (0.2902)	Accuracy 97.656 (91.774)
valid_accuracy 93.120
Epoch #3: Train Accuracy: 67.48603345135737, Validation Accuracy: 93.78, Test Accuracy: 93.12
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.010203387588262558
0.0010000000000000002
Epoch: [4][0/14]	Loss 0.6920 (0.6920)	Accuracy 72.656 (72.656)	Time 0.12
train_accuracy 70.056
Test: [0/79]	Loss 0.2292 (0.2292)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.1873 (0.2139)	Accuracy 97.656 (94.776)
valid_accuracy 95.240
Test: [0/79]	Loss 0.2503 (0.2503)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1163 (0.2416)	Accuracy 97.656 (93.199)
valid_accuracy 94.460
Epoch #4: Train Accuracy: 70.05586595588557, Validation Accuracy: 95.24, Test Accuracy: 94.46
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0010883613722398877
0.0010000000000000002
Epoch: [5][0/14]	Loss 0.7125 (0.7125)	Accuracy 76.562 (76.562)	Time 0.05
train_accuracy 69.832
Test: [0/79]	Loss 0.2131 (0.2131)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.1739 (0.2013)	Accuracy 98.438 (95.037)
valid_accuracy 95.480
Test: [0/79]	Loss 0.2331 (0.2331)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1078 (0.2298)	Accuracy 97.656 (93.581)
valid_accuracy 94.760
Epoch #5: Train Accuracy: 69.83240237102828, Validation Accuracy: 95.48, Test Accuracy: 94.76
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.000952316215261817
* remain weight =  4.398148148148151 %
* best SA=94.76
* find early bird tickets at epoch 6
Remove Pruning
Pruning with custom mask
* remain weight =  4.398148148148151 %
* record size = (60000, 6)
zero all unforgettable images out, rest number =  954
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  5.497591695087656 %
Test: [0/391]	Loss 0.2344 (0.2344)	Accuracy 94.531 (94.531)
Test: [50/391]	Loss 0.1953 (0.2040)	Accuracy 95.312 (95.159)
Test: [100/391]	Loss 0.2259 (0.2069)	Accuracy 95.312 (94.972)
Test: [150/391]	Loss 0.1708 (0.2072)	Accuracy 96.094 (95.059)
Test: [200/391]	Loss 0.1708 (0.2061)	Accuracy 96.875 (95.040)
Test: [250/391]	Loss 0.1838 (0.2088)	Accuracy 96.094 (94.867)
Test: [300/391]	Loss 0.2975 (0.2084)	Accuracy 89.844 (94.913)
Test: [350/391]	Loss 0.1794 (0.2067)	Accuracy 97.656 (95.032)
valid_accuracy 94.960
[14868 37900 22302 10800 29713 37838 21944 40022  7842  8207 21982  8532
  8226 49212   558 10746 49573 37589 44939 43874 40008  5278  8200 39683
 37198 29065 49233  8384 49464 15324 29048 43934 37347 24533 19752 10600
 39801 34942 22169  8347 49398  5554 19782 37680 15848 24490 22130 39832
 22139 19814 24479  8029 37396 49426 22203  8072 37450 19714 49535  8470
 37601 49252 37606  8468 15826 19666 24630  8122 34824 39734 34829  5315
    70 35062 14790 29672 49289 22240 19866 10677  5602  7920 49488 29036
 22217 39914 22062  6905 39488 15434 22884 38365  6347 38374 38390 38397
 38399 21164 23730  6418 29410 39031  6466 23594 23588 38433 38439 20572
 20534 35916 36642  7264 29238 22766 23824 21302 44078 14982  9948  6236
 22845  6251 29434  8978 29839 44086 35688  6272 44090 22838 38283 29286
 15660 20628 20816  7005 36214  6808 29341  6974 23140 29930 23336 20962
  6843  9390 20959 38672 38640 23194  9344 36303 29906 38796 36425 23361
 36522 20641 20652  6578 36006 40066  9583   250  7112 23016 29880 29881
 23406 29967  6670 36104 36446  6710 38526 39256 19960 35614 23868 44338
 10282 20054 22470 37044 30130 22483  5868 22453 35310 10251 10241  7638
   132 20110  8676 10215 15766 15778 24101 20048 39573 29731 44534 29121
  5740  7758 37921 22400 39614 10297  7732 35234  8617 37962 14886 10321
 44340 24210 20029 10334 38050 39487 39482 29206 44598  8853   178  6102
 36783 15190  8902 44294 36766   444 10044 10042 39327 49960 23886  8908
  6170  8904 39377 49892  6050 20146 22531 38052  8730 20150 36934 22584
 22591  8772  7528 39425  8777 39423  8785 24006 35464 20204 20210 35480
  8927  1612 11196 18056 47260 17890 14307 32702 47318  3210 17940 26636
 26629 17958 41742  1222 45501  3268 47376 14333 30770 47387 32814 26572
 30764 14341 26516 45056 32776 26740 12663 12692 42042 32419 47034 12836
  2928  2934 12812 32475 17739 30858 30856 32511 32519 26863 17763 17772
 26842 28471 41912 47159 30833 47160 41904 17817 17819 32888 18066 47479
 12422 12137 33242 43544 47798 18382  3756 47828  3772 41284 18398 12076
 47844 28658  3794 41263 33318 26079 47888 28672 33359 47912  1032 12000
 14468 26017  3692 12898 41358 12181 47488 41594 47538  1143 26405 41538
 26392 18166 47595  3510 12302  3532  3536 43510 12272 18228 47646 33088
 47651 41424  3634 41396 30692  1097 41382 18322 28422  2817 46964 17001
 45792 31596 17003 46274 17012 27732 27722 27706 42703 31652 46314 42687
  1495 16549 17079 28260 14078 31710 17093 14084 27624 45749 31738 17112
 31591 42616  2014 16959 45868 31185 43098 31197 31216 28162 31252 28078
 45944 13896 45968 45980 46021 28180 46023  1784 31366  1826 46078 27912
  1552 13703  1940 42793 31108 16984 45348 31760 17188  2576 46770 30952
 30951 42262 46806 17494 32202 46828  2661 13021 32236 27121 27085 32288
 42140 32324 12940 17608 45617 32342 28413 46958 32348 16408 14172 46432
 27248  2554 31850 27503  1444 17220 46524 42482 27431 31954 31962  2426
 17316 17326 32002 42359 17365  2488 30983 42337 42334 27293 17401 46715
 42317 42312 28357 17420 25994 37551 33722 11693 25546  4714 40514 34520
 30350 48402 30532 49006  4692 48228 25722 25232 15948 49014 34552 15947
  4428 43826 25498 40976 30508  4446 16116 14528  4457 30482 33612 14655
   928 11044 40858 25614 19328 33812 11068 40280 19369 19322 11631 11632
 43804 34059 48288 15961 28898 25657 48270 11565 33860 14700 28778 48750
 48245 24940 24938 25648 28956 40182 41016 25317  4024 24804 25466 16162
 48638 15924  4502 41094 48512 33506 40654  3986 49090 11338 19040 24798
 48020 10831 15913 33484  5216   846 49139 11949 49143 14756 43604 48594
  5155 15975 41060 49070 25294 49067 33572 25297 34615 25817  5132 25854
 19502 10894 45185 30574 24820 33552 17592  2901 11110 19934  2764  9098
 30968 36714 42020 22830  5638 46991 12967  8942 40527 35068  7259 45654
 14653  8883   584 48649  5632 42078 12934 22690 21204 22679 25086 38408
 15188 38370  1400 43898 15280 29712 37834 22278  5122 31608 49066 40144
 10884 22212 22200 22199 16957  1920 37704 42854 37552 49164 37502 21952
 19590 37469 49026 34676 28183 14008 29594 46088 13752 42866 46024 19360
 13538 14796  8761  1404  5536  8744 27320 32018  5000  7606 31000  8680
 31958 24501 31934 17234 17216 37050 19404  7798 24587  2158 28944 37140
 31736 42667 14804  5035 13428 31806 37966 27522 39875 37952 12830 24742
  4158 23710 24310 12416  9379  3998  9376 36268 38680 11688 20918  3136
 12651  4122 43670 12377 30810 36141  5876 41254 38700  3811 47290 26504
 16298 40874  6755 28534 20171  6771 12153 43454 41066 30142   418   340
 41366 44098 32880 48356 14582  4362  5928  9552 26748 30811 20522 48006
 40720 40704 43732 18196 17747  4150   854 18557 39662 48538 43592  9727
 30452 41464 49904 28844  3907  4153 17777 11639 12744  1239 20976   934
   547 35787 35324 39378 45332 12325  9661 15114  7009 17792 13709 26376
 45154 30962 36836 24250   966  3696  5103  1512 24052 25790 49285 39184
 29711 16560 46316 24059 12183  4129  8480  5110 34048 41334 21900 34660
 22083 34750 49890 41226 37468 34740 23911 37427 41218 45917 10048 14764
 16678 16676 45048 18514  1604 33412 44456 34758 43852 24684 15402  6269
 29573 16130 18609  9932 13662  4030  4028 28652 20350  1058 18405 33304
 46122 13735 43850 35616 18423 10750 14972 31576 13719 30356  1121 22779
 47217  1341 36730 30925 40752  2720 47237 46857 13026   588  6885 18864
 43109 25156  1374 47288 48811 47291 32573 13138 36686  1330 21124 26940
  7146 16376 47094 16030 38401  7080 32507 25318 12752 38359  5656 38339
 25306  7270  3030  4460 49624 11298 32108 22643 47340 30751  3390 48975
 26471 10256 41540 21598 13452 20663 26398 26358 24560 47600  5912 11734
 43658  2148 14715 14072 48966  8670 33752 26515 15486 39804 35246 28525
 27292 22607 19808  9433 28533 18220 38772  5821 48933 24198  8729 48945
 36982 26560 36120 10994 41718 49910]
[   12    25    28 ... 49955 49960 49985]
* Critical examples for prunings = 2637
* Critical examples for training = 954
* PrAC images = 3087
******************************************
pruning state 14
* remain parameters = 11761.0
******************************************
* remain weight =  4.398148148148151 %
0.1
* training images number = 3087
Epoch: [0][0/25]	Loss 1.1654 (1.1654)	Accuracy 58.594 (58.594)	Time 0.07
train_accuracy 70.327
Test: [0/79]	Loss 1.8176 (1.8176)	Accuracy 43.750 (43.750)
Test: [50/79]	Loss 1.9063 (1.7858)	Accuracy 32.812 (46.415)
valid_accuracy 46.820
Test: [0/79]	Loss 1.7657 (1.7657)	Accuracy 39.062 (39.062)
Test: [50/79]	Loss 1.7396 (1.7947)	Accuracy 56.250 (43.061)
valid_accuracy 44.810
Epoch #0: Train Accuracy: 70.3271784780865, Validation Accuracy: 46.82, Test Accuracy: 44.81
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/25]	Loss 0.8863 (0.8863)	Accuracy 65.625 (65.625)	Time 0.06
train_accuracy 74.441
Test: [0/79]	Loss 0.3248 (0.3248)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.2953 (0.3012)	Accuracy 90.625 (92.632)
valid_accuracy 92.950
Test: [0/79]	Loss 0.3102 (0.3102)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.1787 (0.3043)	Accuracy 96.094 (92.402)
valid_accuracy 92.840
Epoch #1: Train Accuracy: 74.44120502873535, Validation Accuracy: 92.95, Test Accuracy: 92.84
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.11291556805372238
0.1
Epoch: [2][0/25]	Loss 0.7535 (0.7535)	Accuracy 75.000 (75.000)	Time 0.06
train_accuracy 75.283
Test: [0/79]	Loss 0.3309 (0.3309)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.4011 (0.4194)	Accuracy 88.281 (86.811)
valid_accuracy 87.270
Test: [0/79]	Loss 0.5909 (0.5909)	Accuracy 84.375 (84.375)
Test: [50/79]	Loss 0.2283 (0.4132)	Accuracy 95.312 (87.255)
valid_accuracy 86.390
Epoch #2: Train Accuracy: 75.28344669966084, Validation Accuracy: 87.27, Test Accuracy: 86.39
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08332625031471252
0.1
Epoch: [3][0/25]	Loss 0.5887 (0.5887)	Accuracy 78.125 (78.125)	Time 0.06
train_accuracy 78.134
Test: [0/79]	Loss 0.5673 (0.5673)	Accuracy 79.688 (79.688)
Test: [50/79]	Loss 0.5741 (0.6554)	Accuracy 78.906 (77.298)
valid_accuracy 77.610
Test: [0/79]	Loss 0.3765 (0.3765)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.5720 (0.6007)	Accuracy 77.344 (78.631)
valid_accuracy 78.560
Epoch #3: Train Accuracy: 78.134110787172, Validation Accuracy: 77.61, Test Accuracy: 78.56
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0700620710849762
0.1
Epoch: [4][0/25]	Loss 0.5598 (0.5598)	Accuracy 75.000 (75.000)	Time 0.12
train_accuracy 76.871
Test: [0/79]	Loss 0.4255 (0.4255)	Accuracy 82.812 (82.812)
Test: [50/79]	Loss 0.2732 (0.3456)	Accuracy 89.844 (88.480)
valid_accuracy 88.790
Test: [0/79]	Loss 0.3586 (0.3586)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.2137 (0.3922)	Accuracy 93.750 (86.152)
valid_accuracy 87.350
Epoch #4: Train Accuracy: 76.87074827460513, Validation Accuracy: 88.79, Test Accuracy: 87.35
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06530056893825531
0.010000000000000002
Epoch: [5][0/25]	Loss 0.5778 (0.5778)	Accuracy 78.125 (78.125)	Time 0.06
train_accuracy 79.397
Test: [0/79]	Loss 0.1592 (0.1592)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.1309 (0.1635)	Accuracy 97.656 (95.374)
valid_accuracy 95.330
Test: [0/79]	Loss 0.1853 (0.1853)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0876 (0.1645)	Accuracy 97.656 (95.558)
valid_accuracy 95.740
Epoch #5: Train Accuracy: 79.3974732503097, Validation Accuracy: 95.33, Test Accuracy: 95.74
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.011223535053431988
0.010000000000000002
Epoch: [6][0/25]	Loss 0.5316 (0.5316)	Accuracy 78.906 (78.906)	Time 0.06
train_accuracy 82.021
Test: [0/79]	Loss 0.1404 (0.1404)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1071 (0.1409)	Accuracy 98.438 (96.170)
valid_accuracy 96.160
Test: [0/79]	Loss 0.1620 (0.1620)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0616 (0.1417)	Accuracy 99.219 (96.415)
valid_accuracy 96.690
Epoch #6: Train Accuracy: 82.02137996820636, Validation Accuracy: 96.16, Test Accuracy: 96.69
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0095229996368289
0.010000000000000002
Epoch: [7][0/25]	Loss 0.5172 (0.5172)	Accuracy 79.688 (79.688)	Time 0.07
train_accuracy 82.799
Test: [0/79]	Loss 0.1311 (0.1311)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1042 (0.1351)	Accuracy 98.438 (96.293)
valid_accuracy 96.220
Test: [0/79]	Loss 0.1464 (0.1464)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0611 (0.1384)	Accuracy 98.438 (96.461)
valid_accuracy 96.690
Epoch #7: Train Accuracy: 82.7988337945274, Validation Accuracy: 96.22, Test Accuracy: 96.69
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007822464220225811
* remain weight =  3.5185933105965406 %
* best SA=96.69
* find early bird tickets at epoch 8
Remove Pruning
Pruning with custom mask
* remain weight =  3.5185933105965406 %
* record size = (60000, 8)
zero all unforgettable images out, rest number =  1324
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  4.398148148148151 %
Test: [0/391]	Loss 0.1016 (0.1016)	Accuracy 97.656 (97.656)
Test: [50/391]	Loss 0.1288 (0.1277)	Accuracy 96.875 (96.783)
Test: [100/391]	Loss 0.1440 (0.1313)	Accuracy 96.094 (96.589)
Test: [150/391]	Loss 0.0933 (0.1313)	Accuracy 98.438 (96.544)
Test: [200/391]	Loss 0.1078 (0.1294)	Accuracy 97.656 (96.560)
Test: [250/391]	Loss 0.1398 (0.1297)	Accuracy 96.094 (96.495)
Test: [300/391]	Loss 0.1423 (0.1299)	Accuracy 96.094 (96.480)
Test: [350/391]	Loss 0.1680 (0.1302)	Accuracy 96.094 (96.483)
valid_accuracy 96.472
[37328  8676 41408 ... 26471 47290 36006]
[   35    47    48 ... 49945 49950 49960]
* Critical examples for prunings = 1977
* Critical examples for training = 1324
* PrAC images = 2738
******************************************
pruning state 15
* remain parameters = 9409.0
******************************************
* remain weight =  3.5185933105965406 %
0.1
* training images number = 2738
Epoch: [0][0/22]	Loss 1.6371 (1.6371)	Accuracy 34.375 (34.375)	Time 0.08
train_accuracy 60.738
Test: [0/79]	Loss 2.1623 (2.1623)	Accuracy 17.969 (17.969)
Test: [50/79]	Loss 2.1774 (2.1087)	Accuracy 15.625 (22.610)
valid_accuracy 23.270
Test: [0/79]	Loss 2.0774 (2.0774)	Accuracy 23.438 (23.438)
Test: [50/79]	Loss 2.0771 (2.1024)	Accuracy 24.219 (22.978)
valid_accuracy 23.600
Epoch #0: Train Accuracy: 60.73776479181885, Validation Accuracy: 23.27, Test Accuracy: 23.6
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/22]	Loss 0.8247 (0.8247)	Accuracy 68.750 (68.750)	Time 0.06
train_accuracy 68.700
Test: [0/79]	Loss 0.9948 (0.9948)	Accuracy 67.969 (67.969)
Test: [50/79]	Loss 1.0857 (0.9606)	Accuracy 64.844 (70.450)
valid_accuracy 70.590
Test: [0/79]	Loss 1.0039 (1.0039)	Accuracy 68.750 (68.750)
Test: [50/79]	Loss 0.8766 (0.9795)	Accuracy 74.219 (69.485)
valid_accuracy 69.800
Epoch #1: Train Accuracy: 68.69978086194303, Validation Accuracy: 70.59, Test Accuracy: 69.8
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.10394303500652313
0.1
Epoch: [2][0/22]	Loss 0.7701 (0.7701)	Accuracy 75.000 (75.000)	Time 0.12
train_accuracy 69.759
Test: [0/79]	Loss 0.3632 (0.3632)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.3757 (0.3493)	Accuracy 86.719 (87.531)
valid_accuracy 87.900
Test: [0/79]	Loss 0.3532 (0.3532)	Accuracy 87.500 (87.500)
Test: [50/79]	Loss 0.2153 (0.4206)	Accuracy 94.531 (85.355)
valid_accuracy 86.110
Epoch #2: Train Accuracy: 69.75894813732651, Validation Accuracy: 87.9, Test Accuracy: 86.11
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.09288978576660156
0.1
Epoch: [3][0/22]	Loss 0.7588 (0.7588)	Accuracy 68.750 (68.750)	Time 0.06
train_accuracy 70.380
Test: [0/79]	Loss 0.3655 (0.3655)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.4331 (0.4538)	Accuracy 80.469 (83.563)
valid_accuracy 84.500
Test: [0/79]	Loss 0.4502 (0.4502)	Accuracy 86.719 (86.719)
Test: [50/79]	Loss 0.3170 (0.4858)	Accuracy 89.062 (82.123)
valid_accuracy 83.360
Epoch #3: Train Accuracy: 70.37983929875821, Validation Accuracy: 84.5, Test Accuracy: 83.36
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08204910159111023
0.010000000000000002
Epoch: [4][0/22]	Loss 0.5233 (0.5233)	Accuracy 81.250 (81.250)	Time 0.07
train_accuracy 73.302
Test: [0/79]	Loss 0.1575 (0.1575)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1262 (0.1479)	Accuracy 98.438 (96.952)
valid_accuracy 97.270
Test: [0/79]	Loss 0.1639 (0.1639)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0598 (0.1668)	Accuracy 100.000 (96.507)
valid_accuracy 97.080
Epoch #4: Train Accuracy: 73.30168005843682, Validation Accuracy: 97.27, Test Accuracy: 97.08
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.013603996485471725
0.010000000000000002
Epoch: [5][0/22]	Loss 0.5554 (0.5554)	Accuracy 79.688 (79.688)	Time 0.07
train_accuracy 75.931
Test: [0/79]	Loss 0.1044 (0.1044)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1039 (0.1259)	Accuracy 98.438 (97.442)
valid_accuracy 97.770
Test: [0/79]	Loss 0.1412 (0.1412)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0462 (0.1451)	Accuracy 100.000 (96.798)
valid_accuracy 97.370
Epoch #5: Train Accuracy: 75.93133674214755, Validation Accuracy: 97.77, Test Accuracy: 97.37
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007652248255908489
0.010000000000000002
Epoch: [6][0/22]	Loss 0.7545 (0.7545)	Accuracy 68.750 (68.750)	Time 0.06
train_accuracy 76.662
Test: [0/79]	Loss 0.1075 (0.1075)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0991 (0.1138)	Accuracy 99.219 (97.702)
valid_accuracy 98.020
Test: [0/79]	Loss 0.1291 (0.1291)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0417 (0.1308)	Accuracy 100.000 (97.120)
valid_accuracy 97.670
Epoch #6: Train Accuracy: 76.6617969320672, Validation Accuracy: 98.02, Test Accuracy: 97.67
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009352747350931168
0.0010000000000000002
Epoch: [7][0/22]	Loss 0.6560 (0.6560)	Accuracy 75.781 (75.781)	Time 0.05
train_accuracy 77.356
Test: [0/79]	Loss 0.0996 (0.0996)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0886 (0.1060)	Accuracy 99.219 (97.855)
valid_accuracy 98.120
Test: [0/79]	Loss 0.1262 (0.1262)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0357 (0.1221)	Accuracy 100.000 (97.426)
valid_accuracy 97.910
Epoch #7: Train Accuracy: 77.35573411249086, Validation Accuracy: 98.12, Test Accuracy: 97.91
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0004251248901709914
0.0010000000000000002
Epoch: [8][0/22]	Loss 0.5631 (0.5631)	Accuracy 78.125 (78.125)	Time 0.13
train_accuracy 78.123
Test: [0/79]	Loss 0.0988 (0.0988)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0861 (0.1044)	Accuracy 99.219 (97.901)
valid_accuracy 98.160
Test: [0/79]	Loss 0.1270 (0.1270)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0351 (0.1196)	Accuracy 100.000 (97.610)
valid_accuracy 98.030
Epoch #8: Train Accuracy: 78.1227173119065, Validation Accuracy: 98.16, Test Accuracy: 98.03
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0012753746705129743
* remain weight =  2.8147998563992083 %
* best SA=98.03
* find early bird tickets at epoch 9
Remove Pruning
Pruning with custom mask
* remain weight =  2.8147998563992083 %
* record size = (60000, 9)
zero all unforgettable images out, rest number =  1456
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0174 (0.0174)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0606 (0.0462)	Accuracy 97.656 (98.591)
Test: [100/391]	Loss 0.0313 (0.0462)	Accuracy 99.219 (98.461)
Test: [150/391]	Loss 0.0569 (0.0450)	Accuracy 99.219 (98.567)
Test: [200/391]	Loss 0.0981 (0.0450)	Accuracy 96.875 (98.535)
Test: [250/391]	Loss 0.0255 (0.0454)	Accuracy 98.438 (98.543)
Test: [300/391]	Loss 0.0558 (0.0452)	Accuracy 97.656 (98.539)
Test: [350/391]	Loss 0.0827 (0.0455)	Accuracy 97.656 (98.544)
valid_accuracy 98.518
Pruning with custom mask
* remain weight =  3.5185933105965406 %
Test: [0/391]	Loss 0.1274 (0.1274)	Accuracy 96.094 (96.094)
Test: [50/391]	Loss 0.0981 (0.1061)	Accuracy 98.438 (97.963)
Test: [100/391]	Loss 0.1289 (0.1062)	Accuracy 97.656 (98.113)
Test: [150/391]	Loss 0.0845 (0.1059)	Accuracy 99.219 (98.148)
Test: [200/391]	Loss 0.0995 (0.1055)	Accuracy 98.438 (98.084)
Test: [250/391]	Loss 0.0953 (0.1067)	Accuracy 98.438 (98.020)
Test: [300/391]	Loss 0.1731 (0.1076)	Accuracy 95.312 (98.022)
Test: [350/391]	Loss 0.1344 (0.1067)	Accuracy 96.875 (98.077)
valid_accuracy 98.044
[ 8202  8200 37198 ...  2426 20016 16130]
[   25    28    67 ... 49946 49955 49960]
* Critical examples for prunings = 1082
* Critical examples for training = 1456
* PrAC images = 1979
Algorithm finished in 770.4388523101807 seconds
forgetting numbers:  [2695, 1828, 1143, 867, 726, 672, 649, 595, 663, 713, 855, 986, 860, 954, 1324, 1456]
pie num:  [0, 694, 677, 924, 838, 952, 1081, 1275, 1158, 1582, 1659, 1125, 1346, 2637, 1977, 1082]
model sparsities:  [20.00014958415605, 36.000044875246815, 48.799961108119426, 59.04011847065159, 67.23209477652127, 73.78575061329505, 79.02867528271406, 83.22301501824927, 86.57856159875546, 89.26284927900437, 91.41012983904744, 93.12810387123795, 94.50240830491235, 95.60185185185185, 96.48140668940346, 97.1852001436008]
