Namespace(arch='res20s', batch_size=128, checkpoint=None, core_set_method='both', data='data/mnist', data_prune='zero_out', data_rate=0.2, dataset='mnist', decreasing_lr='91,136', eb_eps=0.08, epochs=182, gpu=0, lr=0.1, momentum=0.9, print_freq=50, prune_type='rewind_lt', pruning_times=16, queue_length=5, rate=0.2, resume=False, rewind_epoch=2, save_dir='mnist_with_bars', seed=None, split_file='npy_files/mnist-train-val.npy', threshold=0, warmup=0, weight_decay=0.0001)
return train dataset
build model: resnet20
######################################## Start Standard Training Iterative Pruning ########################################
******************************************
pruning state 0
* remain parameters = 267408
******************************************
* remain weight =  100.0 %
0.1
* training images number = 50000
Epoch: [0][0/391]	Loss 4.1372 (4.1372)	Accuracy 7.812 (7.812)	Time 0.11
Epoch: [0][50/391]	Loss 0.6584 (1.6386)	Accuracy 79.688 (50.797)	Time 2.15
Epoch: [0][100/391]	Loss 0.3035 (1.0635)	Accuracy 89.062 (66.955)	Time 2.13
Epoch: [0][150/391]	Loss 0.4032 (0.8178)	Accuracy 92.188 (74.462)	Time 2.06
Epoch: [0][200/391]	Loss 0.2634 (0.6699)	Accuracy 96.094 (79.058)	Time 1.98
Epoch: [0][250/391]	Loss 0.1095 (0.5710)	Accuracy 96.875 (82.159)	Time 2.16
Epoch: [0][300/391]	Loss 0.2623 (0.5066)	Accuracy 92.969 (84.180)	Time 2.03
Epoch: [0][350/391]	Loss 0.1553 (0.4545)	Accuracy 95.312 (85.835)	Time 2.03
train_accuracy 86.930
Test: [0/79]	Loss 0.2646 (0.2646)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.2240 (0.1850)	Accuracy 92.188 (94.133)
valid_accuracy 94.440
Test: [0/79]	Loss 0.1143 (0.1143)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0328 (0.1938)	Accuracy 99.219 (93.260)
valid_accuracy 93.750
Apply Unstructured L1 Pruning
Remove Pruning
0.1
Epoch: [1][0/391]	Loss 0.1166 (0.1166)	Accuracy 96.094 (96.094)	Time 0.04
Epoch: [1][50/391]	Loss 0.0588 (0.1193)	Accuracy 97.656 (96.124)	Time 2.21
Epoch: [1][100/391]	Loss 0.0810 (0.1218)	Accuracy 97.656 (96.101)	Time 2.14
Epoch: [1][150/391]	Loss 0.1454 (0.1201)	Accuracy 93.750 (96.156)	Time 2.00
Epoch: [1][200/391]	Loss 0.0753 (0.1140)	Accuracy 96.875 (96.401)	Time 2.06
Epoch: [1][250/391]	Loss 0.0945 (0.1113)	Accuracy 96.094 (96.473)	Time 1.95
Epoch: [1][300/391]	Loss 0.0705 (0.1076)	Accuracy 97.656 (96.569)	Time 1.97
Epoch: [1][350/391]	Loss 0.0757 (0.1056)	Accuracy 98.438 (96.650)	Time 1.97
train_accuracy 96.718
Test: [0/79]	Loss 0.0273 (0.0273)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.2030 (0.1142)	Accuracy 92.969 (96.186)
valid_accuracy 96.660
Test: [0/79]	Loss 0.1240 (0.1240)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0207 (0.1425)	Accuracy 99.219 (95.558)
valid_accuracy 96.440
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.08672889322042465
0.1
Epoch: [2][0/391]	Loss 0.0871 (0.0871)	Accuracy 97.656 (97.656)	Time 0.04
Epoch: [2][50/391]	Loss 0.0551 (0.0738)	Accuracy 98.438 (97.442)	Time 2.00
Epoch: [2][100/391]	Loss 0.0809 (0.0769)	Accuracy 96.875 (97.440)	Time 2.12
Epoch: [2][150/391]	Loss 0.1063 (0.0790)	Accuracy 95.312 (97.392)	Time 2.29
Epoch: [2][200/391]	Loss 0.1123 (0.0773)	Accuracy 96.875 (97.481)	Time 2.05
Epoch: [2][250/391]	Loss 0.0901 (0.0792)	Accuracy 96.875 (97.454)	Time 2.15
Epoch: [2][300/391]	Loss 0.0371 (0.0799)	Accuracy 98.438 (97.438)	Time 2.08
Epoch: [2][350/391]	Loss 0.0476 (0.0794)	Accuracy 98.438 (97.465)	Time 2.01
train_accuracy 97.502
Test: [0/79]	Loss 0.0295 (0.0295)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0893 (0.0615)	Accuracy 96.094 (97.947)
valid_accuracy 98.160
Test: [0/79]	Loss 0.0574 (0.0574)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0265 (0.0706)	Accuracy 99.219 (97.672)
valid_accuracy 98.050
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.07045413553714752
0.1
Epoch: [3][0/391]	Loss 0.1022 (0.1022)	Accuracy 94.531 (94.531)	Time 0.04
Epoch: [3][50/391]	Loss 0.0102 (0.0621)	Accuracy 100.000 (98.100)	Time 2.03
Epoch: [3][100/391]	Loss 0.0425 (0.0623)	Accuracy 99.219 (97.989)	Time 2.03
Epoch: [3][150/391]	Loss 0.0354 (0.0639)	Accuracy 98.438 (97.998)	Time 2.00
Epoch: [3][200/391]	Loss 0.0750 (0.0640)	Accuracy 96.875 (97.991)	Time 1.98
Epoch: [3][250/391]	Loss 0.0592 (0.0644)	Accuracy 97.656 (97.952)	Time 1.98
Epoch: [3][300/391]	Loss 0.0390 (0.0656)	Accuracy 98.438 (97.931)	Time 1.95
Epoch: [3][350/391]	Loss 0.0897 (0.0651)	Accuracy 96.875 (97.961)	Time 1.94
train_accuracy 98.000
Test: [0/79]	Loss 0.0308 (0.0308)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0541 (0.0874)	Accuracy 99.219 (97.518)
valid_accuracy 97.570
Test: [0/79]	Loss 0.1127 (0.1127)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0229 (0.0916)	Accuracy 98.438 (97.181)
valid_accuracy 97.580
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.06073116883635521
0.1
Epoch: [4][0/391]	Loss 0.0577 (0.0577)	Accuracy 96.875 (96.875)	Time 0.04
Epoch: [4][50/391]	Loss 0.0278 (0.0588)	Accuracy 99.219 (97.978)	Time 2.20
Epoch: [4][100/391]	Loss 0.0652 (0.0604)	Accuracy 98.438 (98.089)	Time 2.05
Epoch: [4][150/391]	Loss 0.0431 (0.0585)	Accuracy 97.656 (98.148)	Time 1.99
Epoch: [4][200/391]	Loss 0.0966 (0.0578)	Accuracy 98.438 (98.177)	Time 2.01
Epoch: [4][250/391]	Loss 0.0091 (0.0589)	Accuracy 100.000 (98.145)	Time 1.95
Epoch: [4][300/391]	Loss 0.0584 (0.0582)	Accuracy 97.656 (98.144)	Time 1.95
Epoch: [4][350/391]	Loss 0.1339 (0.0575)	Accuracy 96.094 (98.166)	Time 1.96
train_accuracy 98.156
Test: [0/79]	Loss 0.0273 (0.0273)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0611 (0.0612)	Accuracy 97.656 (98.039)
valid_accuracy 98.180
Test: [0/79]	Loss 0.0528 (0.0528)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0080 (0.0739)	Accuracy 100.000 (97.610)
valid_accuracy 97.990
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.06029737368226051
0.1
Epoch: [5][0/391]	Loss 0.0994 (0.0994)	Accuracy 96.875 (96.875)	Time 0.04
Epoch: [5][50/391]	Loss 0.0482 (0.0632)	Accuracy 98.438 (98.208)	Time 2.02
Epoch: [5][100/391]	Loss 0.0274 (0.0571)	Accuracy 99.219 (98.236)	Time 2.01
Epoch: [5][150/391]	Loss 0.0145 (0.0574)	Accuracy 100.000 (98.168)	Time 2.01
Epoch: [5][200/391]	Loss 0.0563 (0.0556)	Accuracy 98.438 (98.235)	Time 2.00
Epoch: [5][250/391]	Loss 0.1069 (0.0561)	Accuracy 96.094 (98.198)	Time 1.98
Epoch: [5][300/391]	Loss 0.0376 (0.0569)	Accuracy 99.219 (98.201)	Time 1.97
Epoch: [5][350/391]	Loss 0.0358 (0.0561)	Accuracy 98.438 (98.206)	Time 1.97
train_accuracy 98.208
Test: [0/79]	Loss 0.0235 (0.0235)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0842 (0.0758)	Accuracy 97.656 (97.595)
valid_accuracy 97.870
Test: [0/79]	Loss 0.0281 (0.0281)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0043 (0.0643)	Accuracy 100.000 (97.794)
valid_accuracy 98.110
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.05880901217460632
0.1
Epoch: [6][0/391]	Loss 0.0523 (0.0523)	Accuracy 97.656 (97.656)	Time 0.05
Epoch: [6][50/391]	Loss 0.0651 (0.0495)	Accuracy 98.438 (98.529)	Time 2.06
Epoch: [6][100/391]	Loss 0.0095 (0.0504)	Accuracy 100.000 (98.352)	Time 2.00
Epoch: [6][150/391]	Loss 0.0243 (0.0540)	Accuracy 99.219 (98.231)	Time 1.97
Epoch: [6][200/391]	Loss 0.1221 (0.0536)	Accuracy 96.094 (98.259)	Time 1.96
Epoch: [6][250/391]	Loss 0.0772 (0.0537)	Accuracy 96.094 (98.241)	Time 1.96
Epoch: [6][300/391]	Loss 0.0708 (0.0529)	Accuracy 96.875 (98.290)	Time 2.01
Epoch: [6][350/391]	Loss 0.0143 (0.0516)	Accuracy 99.219 (98.324)	Time 2.00
train_accuracy 98.352
Test: [0/79]	Loss 0.0100 (0.0100)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0454 (0.0386)	Accuracy 98.438 (98.744)
valid_accuracy 98.850
Test: [0/79]	Loss 0.0291 (0.0291)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0077 (0.0374)	Accuracy 100.000 (98.637)
valid_accuracy 98.890
Apply Unstructured L1 Pruning
Remove Pruning
* current-mask-distance is = 0.05689433589577675
* remain weight =  100.0 %
* best SA=98.89
* find early bird tickets at epoch 7
Pruning with custom mask
* remain weight =  79.99985041584395 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  2775
******************************************
pruning state 1
* remain parameters = 213926.0
******************************************
* remain weight =  79.99985041584395 %
0.1
* training images number = 2775
Epoch: [0][0/22]	Loss 0.5836 (0.5836)	Accuracy 78.125 (78.125)	Time 0.06
train_accuracy 74.703
Test: [0/79]	Loss 0.3655 (0.3655)	Accuracy 85.938 (85.938)
Test: [50/79]	Loss 0.4277 (0.4376)	Accuracy 85.938 (86.213)
valid_accuracy 86.810
Test: [0/79]	Loss 0.3880 (0.3880)	Accuracy 89.844 (89.844)
Test: [50/79]	Loss 0.2458 (0.4524)	Accuracy 89.844 (86.198)
valid_accuracy 87.000
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/22]	Loss 0.7811 (0.7811)	Accuracy 70.312 (70.312)	Time 0.05
train_accuracy 72.649
Test: [0/79]	Loss 0.1626 (0.1626)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2059 (0.2204)	Accuracy 94.531 (93.735)
valid_accuracy 94.020
Test: [0/79]	Loss 0.2361 (0.2361)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.1510 (0.2596)	Accuracy 98.438 (92.479)
valid_accuracy 92.720
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.044099360704422
0.1
Epoch: [2][0/22]	Loss 0.7210 (0.7210)	Accuracy 72.656 (72.656)	Time 0.04
train_accuracy 75.604
Test: [0/79]	Loss 0.1247 (0.1247)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1282 (0.1202)	Accuracy 94.531 (96.844)
valid_accuracy 97.160
Test: [0/79]	Loss 0.0830 (0.0830)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0300 (0.1283)	Accuracy 100.000 (96.109)
valid_accuracy 96.540
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.039219167083501816
0.1
Epoch: [3][0/22]	Loss 0.6525 (0.6525)	Accuracy 75.781 (75.781)	Time 0.04
train_accuracy 76.973
Test: [0/79]	Loss 0.0818 (0.0818)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1124 (0.1099)	Accuracy 95.312 (96.339)
valid_accuracy 96.750
Test: [0/79]	Loss 0.0988 (0.0988)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.0285 (0.1108)	Accuracy 100.000 (96.599)
valid_accuracy 97.080
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.036610789597034454
0.1
Epoch: [4][0/22]	Loss 0.4770 (0.4770)	Accuracy 85.156 (85.156)	Time 0.10
train_accuracy 78.631
Test: [0/79]	Loss 0.0343 (0.0343)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0666 (0.0624)	Accuracy 98.438 (98.300)
valid_accuracy 98.340
Test: [0/79]	Loss 0.0523 (0.0523)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0431 (0.0639)	Accuracy 99.219 (98.116)
valid_accuracy 98.350
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.033965013921260834
0.010000000000000002
Epoch: [5][0/22]	Loss 0.4402 (0.4402)	Accuracy 82.031 (82.031)	Time 0.05
train_accuracy 83.568
Test: [0/79]	Loss 0.0289 (0.0289)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0412 (0.0452)	Accuracy 98.438 (98.759)
valid_accuracy 98.840
Test: [0/79]	Loss 0.0436 (0.0436)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0087 (0.0494)	Accuracy 100.000 (98.729)
valid_accuracy 99.010
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005487878806889057
0.010000000000000002
Epoch: [6][0/22]	Loss 0.4042 (0.4042)	Accuracy 85.938 (85.938)	Time 0.05
train_accuracy 85.874
Test: [0/79]	Loss 0.0276 (0.0276)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0429 (0.0442)	Accuracy 99.219 (98.836)
valid_accuracy 98.900
Test: [0/79]	Loss 0.0346 (0.0346)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0104 (0.0480)	Accuracy 99.219 (98.713)
valid_accuracy 98.960
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.004805400036275387
0.0010000000000000002
Epoch: [7][0/22]	Loss 0.5441 (0.5441)	Accuracy 76.562 (76.562)	Time 0.05
train_accuracy 86.739
Test: [0/79]	Loss 0.0256 (0.0256)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0413 (0.0416)	Accuracy 99.219 (98.790)
valid_accuracy 98.890
Test: [0/79]	Loss 0.0331 (0.0331)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0068 (0.0455)	Accuracy 100.000 (98.698)
valid_accuracy 98.930
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00043005525367334485
0.0010000000000000002
Epoch: [8][0/22]	Loss 0.3800 (0.3800)	Accuracy 85.938 (85.938)	Time 0.04
train_accuracy 86.811
Test: [0/79]	Loss 0.0263 (0.0263)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0400 (0.0407)	Accuracy 99.219 (98.897)
valid_accuracy 98.950
Test: [0/79]	Loss 0.0329 (0.0329)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0061 (0.0443)	Accuracy 100.000 (98.729)
valid_accuracy 98.980
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00045810232404619455
0.0010000000000000002
Epoch: [9][0/22]	Loss 0.3384 (0.3384)	Accuracy 89.062 (89.062)	Time 0.05
train_accuracy 87.568
Test: [0/79]	Loss 0.0266 (0.0266)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0398 (0.0407)	Accuracy 99.219 (98.866)
valid_accuracy 98.930
Test: [0/79]	Loss 0.0328 (0.0328)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0058 (0.0440)	Accuracy 100.000 (98.698)
valid_accuracy 98.970
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00033656496088951826
* remain weight =  63.999955124753185 %
* best SA=98.98
* find early bird tickets at epoch 10
Remove Pruning
Pruning with custom mask
* remain weight =  63.999955124753185 %
* record size = (60000, 10)
zero all unforgettable images out, rest number =  1477
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  79.99985041584395 %
Test: [0/391]	Loss 0.0186 (0.0186)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0355 (0.0306)	Accuracy 98.438 (99.234)
Test: [100/391]	Loss 0.0251 (0.0308)	Accuracy 100.000 (99.242)
Test: [150/391]	Loss 0.0149 (0.0312)	Accuracy 100.000 (99.193)
Test: [200/391]	Loss 0.0274 (0.0313)	Accuracy 99.219 (99.203)
Test: [250/391]	Loss 0.0238 (0.0314)	Accuracy 99.219 (99.231)
Test: [300/391]	Loss 0.0645 (0.0320)	Accuracy 96.875 (99.229)
Test: [350/391]	Loss 0.0436 (0.0318)	Accuracy 98.438 (99.248)
valid_accuracy 99.234
[48432 37028 34430 ... 36006 46857  7146]
[  178   212   238   547   588   778   854   902   966  1001  1032  1047
  1049  1058  1143  1330  1404  1674  2078  2250  2410  2488  2622  2676
  2707  2931  2958  3136  3390  3456  3524  3532  3756  3811  3998  4024
  4039  4150  4362  4446  4460  4562  4692  5084  5103  5174  5314  5718
  5738  5868  6236  6304  6428  6578  6636  6658  6670  7058  7259  7270
  7293  7347  7606  7638  7732  7768  7784  7898  7920  8072  8104  8200
  8468  8470  8488  8532  8552  8613  8680  8729  8731  8772  8799  9471
  9489  9876 10215 10243 10256 10297 10524 10831 10884 11288 11759 11797
 11894 11949 12078 12377 12651 12692 12729 12764 12940 13138 13198 13370
 13658 13662 13673 13831 13850 13854 14246 14341 14607 14754 14756 14764
 14765 14790 14804 14819 15190 15280 15402 15434 15450 15500 15642 15749
 15766 15826 15827 15862 15924 15948 16072 16173 16192 16676 16684 16722
 17112 17314 17382 17608 17645 17648 17739 17940 18056 18378 18382 18397
 18598 18930 19244 19386 19502 19714 19934 19960 20016 20226 20322 20350
 20532 20652 20709 20792 20816 20882 20962 21164 21351 21355 21445 21952
 22200 22470 22607 22643 22675 22690 23136 23458 23546 23886 24036 24059
 24101 24210 24310 24526 24587 24760 24798 24845 24870 25159 25262 25294
 25546 25657 25678 25683 25688 25799 25801 25948 26052 26061 26079 26358
 26376 26392 26504 26515 26613 26749 26756 26760 26842 26896 27085 27121
 27204 27248 27293 27859 27964 28260 28357 28567 28574 28710 28898 28984
 29180 29434 29573 29712 29803 29880 29930 30041 30184 30250 30356 30450
 30751 30833 30881 30925 31100 31185 31200 31275 31456 31461 31474 31576
 31600 31672 31738 31840 31895 31954 31962 32110 32280 32323 32394 32417
 32421 32507 32510 32511 32747 32918 32934 33362 33364 33506 33612 33768
 34048 34328 34542 34660 34676 34740 34758 34771 35234 35401 35574 35622
 35688 35710 35777 35882 35883 35916 36055 36446 36620 36714 36730 36982
 37038 37118 37135 37396 37468 37588 37589 37606 37834 37966 37974 38052
 38082 38359 38365 38374 38397 38399 38401 38650 39074 39327 39378 39397
 39488 39708 39734 39801 39875 40280 40386 40498 40527 40654 40824 40948
 41113 41284 41377 41396 41464 41538 41540 41618 41843 42140 42234 42312
 42359 42566 42604 42772 42854 43198 43510 43826 43852 44046 44262 44456
 45266 45408 45532 45654 45761 45888 45917 45944 45968 46298 46316 46432
 46906 47008 47217 47288 47376 47387 47421 47486 47600 47689 47759 48091
 48228 48245 48356 48472 48486 48856 48945 48968 48975 49026 49062 49082
 49090 49109 49139 49143 49212 49285 49416 49488 49543 49573 49904]
* Critical examples for prunings = 419
* Critical examples for training = 1477
* PrAC images = 1612
******************************************
pruning state 2
* remain parameters = 171141.0
******************************************
* remain weight =  63.999955124753185 %
0.1
* training images number = 1612
Epoch: [0][0/13]	Loss 1.1357 (1.1357)	Accuracy 64.844 (64.844)	Time 0.06
train_accuracy 62.221
Test: [0/79]	Loss 0.1971 (0.1971)	Accuracy 91.406 (91.406)
Test: [50/79]	Loss 0.1642 (0.2070)	Accuracy 96.094 (94.026)
valid_accuracy 94.280
Test: [0/79]	Loss 0.2533 (0.2533)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.0970 (0.2306)	Accuracy 98.438 (93.505)
valid_accuracy 94.230
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/13]	Loss 0.9327 (0.9327)	Accuracy 64.062 (64.062)	Time 0.05
train_accuracy 61.849
Test: [0/79]	Loss 0.1096 (0.1096)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1621 (0.1431)	Accuracy 94.531 (96.140)
valid_accuracy 96.500
Test: [0/79]	Loss 0.1064 (0.1064)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0562 (0.1662)	Accuracy 99.219 (94.991)
valid_accuracy 96.050
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04319245368242264
0.010000000000000002
Epoch: [2][0/13]	Loss 0.8296 (0.8296)	Accuracy 64.844 (64.844)	Time 0.04
train_accuracy 64.578
Test: [0/79]	Loss 0.0788 (0.0788)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0869 (0.0808)	Accuracy 99.219 (98.254)
valid_accuracy 98.370
Test: [0/79]	Loss 0.0572 (0.0572)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0276 (0.0890)	Accuracy 99.219 (98.024)
valid_accuracy 98.420
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005749645177274942
0.010000000000000002
Epoch: [3][0/13]	Loss 0.8009 (0.8009)	Accuracy 67.188 (67.188)	Time 0.05
train_accuracy 71.030
Test: [0/79]	Loss 0.0622 (0.0622)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0783 (0.0667)	Accuracy 97.656 (98.637)
valid_accuracy 98.720
Test: [0/79]	Loss 0.0530 (0.0530)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0214 (0.0755)	Accuracy 100.000 (98.330)
valid_accuracy 98.630
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005387370474636555
0.0010000000000000002
Epoch: [4][0/13]	Loss 0.6894 (0.6894)	Accuracy 69.531 (69.531)	Time 0.04
train_accuracy 71.712
Test: [0/79]	Loss 0.0564 (0.0564)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0740 (0.0642)	Accuracy 97.656 (98.729)
valid_accuracy 98.790
Test: [0/79]	Loss 0.0472 (0.0472)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0198 (0.0720)	Accuracy 99.219 (98.438)
valid_accuracy 98.740
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0005258821765892208
* remain weight =  51.200038891880574 %
* best SA=98.74
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  51.200038891880574 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  916
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  63.999955124753185 %
Test: [0/391]	Loss 0.0627 (0.0627)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.0574 (0.0582)	Accuracy 98.438 (98.729)
Test: [100/391]	Loss 0.0645 (0.0583)	Accuracy 99.219 (98.886)
Test: [150/391]	Loss 0.0387 (0.0593)	Accuracy 100.000 (98.877)
Test: [200/391]	Loss 0.0477 (0.0591)	Accuracy 98.438 (98.912)
Test: [250/391]	Loss 0.0677 (0.0597)	Accuracy 98.438 (98.923)
Test: [300/391]	Loss 0.0966 (0.0599)	Accuracy 96.875 (98.931)
Test: [350/391]	Loss 0.0668 (0.0596)	Accuracy 98.438 (98.949)
valid_accuracy 98.950
[20110 37609 37606 22470 49818 27085 20784 29121 19808 32417 13538 20628
 33088 10211 16716 25232  5908 26626 48676 26629 40498 13604 42854 13673
 15778  8552 26515 39397 36214 10044 48402  2720 23588 15766 48681 15660
  6755  9534 12325 32835 48397 41540 26516 30049 27204 46298 12898  4712
 47659 15243  7885 26622 46394 18190 36055  4562 37966 28072 23494 42715
 46288 36268 35401 12940 32918  6808 44262  5821 25657 18056  7638 37680
 12764  5422 14740 19888 16676  1978 23422 13850 24059 40752 34830 17602
 15528 34829 31576 27248 37062 23962  7917  8347 49844 39482 24101 48486
 27700 12891 44243  6578  5912 20903 35324  2707 39526 26636  4741 24526
  2901  7907  1963 46274 17045 17592 25648 15642  7898  4428 47421 27810
 19040 17608 17542 30262 32814 26392 23336 14819 44939 23385 34010 39419
 42112 42973  2824 42772 17645 29206 49890 22400 20226 19089 24560 13735
 49501 24609  5866 39940  6540 42810 46154   498 48638 29311 25466 20652
 38798  1784 27898 48432  2052 12422 14896 13854  7768 37198  4460 35480
 29238 20816 48524 16862 42964  3290 29226 15070 42234 20204 13719 25294
 46316  8488 29659 41594 27968 12302 39424 12967 44090 44110 39423  9450
 48649 27930  4476  7732  4692 24084 46887 44046 41618 49514 26398  9471
 49426 15748  3268 38770  8532 47646 30041 39467 37552 37468  8384   544
 37140 31456 47538 46300 22436   547 34234 19714 27026 39801 10048 10616
  7789  2078 30156 36141 17001 26418 31608  6636  9583 17663 19866 32323
 37469  9627 34964 13658 49910 32280 30168 41718 24589   340 37672 36104
 13662 42943 31374  7867 28014 40824 32880 40527 41562 37551 28534 33812
  1032 36522 30968 39708 21355  9104  9876 37450 41113  1356 29712 17879
  7058 23040 18598 18405 49153 32604 26740 12000 21351 22802 24990 21164
 18423 49139 10944 10319  8072 16173 49143 43610 47220  1049  1047 18416
 28688  7989 48102 10315  3998 43368  1341 21850  1001 12018 19960  1404
 20522 25868 33304 33612 14072    28 38526 42604  6347 39256 29573 44338
 33296 33622 23089 46524 30566 37557 29055  4066  7293 48966 40308  7347
 31000 22200 46682 34676 18609 46538 38230  2426  7374 49164  5162   974
  3756 20029 10982 31854 14084   966 48968 11786 45792 47237 42482 45615
  2342 10894 43510 28574 45617 20460 22183 35774 38399 38401 11894 26052
 18514 27472 38339 25994 35777 37588  5084 27502  2338 30856  5068 28533
 38381 49088  9814 49090 49083 44690 33448  6269 48006 37396   212 10884
 47290 36642 47291 45606 23770 27501  5684 11949 30881  5035  3118  7270
  1097 30925 34532 41276 43575  6246 16376 28453 17188 31884 25948 12651
  8104 35164 17811 16190 20016 23730  5000  6236  1291 33388 30770 37521
  1143  6304  5706 31954 25979 14307 41912  8927 24310 25975 30751 27514
 31958  1276 36698 34542 42501 37589 19502 16072 47340 19360 48856 20322
 17728 27293 27292 23762 31200 10251 33170 13428 32519 28175 41464 47741
 17071   180 31216 20962  5806 36982 15924 10253  8680   778 44484 15948
  2958  6418   770 24722  5790  7530 32511 33182 43852 42359 47152 31760
 22591 17079 38650 28956 23212 47759 22130  2148 38050  7599  5306 24432
 26646 25683 19666 38700 26940  2576 49285 47710 35916 32499  8617 48811
 13053  8613 27252 13452  6448 25678  5638 18247 17420 38672 12181 30356
 41473  4221 25156 45143 32510 46432 18776 25159  1618  6848 42334 40434
  7584 38680 13067 29588 23946  8154 39311 17739 47159 22690 47828 25817
 10282  9727 23136 28282 15500 28279   886  5216 22675 19328 17120 27602
  1495 21042  9932 25801 18322 17121 19934 35851 33656 41358 48945 17326
  4909  8797 12692 30544 12076 17772  6974  5624 39625 23868 39088 33262
 42415 19345 48930 15280 17336 10287 25799 41016 15328 25797 13970 28898
 19306 47376 14582 36934 41424  8719 48242 46466 43818 21982 40386 22169
 35882 48881  3634 35883 41795 32776 16560 35873  4150 26228 11044 35864
 40076 23886  5774 30482 42648 26896  1514   832 39074 46476 15991 22643
 39297  8729 37574 48228 45930 29803 44642 15434   238 29713 25854 31252
  6670 23458  1988 31591 31596 31600  5174  6524 20663 31672 20641  2158
 17093 46486 13370 23406 31850 23361 31335 49067 36425 45868 36370 36363
 19497 31185 45968 16678 20918 47940 41284 16684 13831  9376 47912 48053
 27526 17209 32405 12830 49416 32934 49464 17763 12416 26600 49543  5738
  5602  3184  5704 39734 26766 32394 47798 24630  5972  2410 46689 17314
 39260 41438 12153 49264  2554  6102 41478 46806 39342 12196  6034 39425
 39488 45761  3136 23038 14764 11438  8898 11693 36760 28710 44078  4153
 48245  4165 40977 14468  8799   500 48512 45306 11104   846  8731 43826
 11631   558 14804 45154 43898 40510 37050 14790 48469 25546  8772 34404
   588  7259 24997 48594  4542 14172 38439  4030 18557  4129 45616 41156
 38397  9098 45654 14230 28407 44133 36620 19369 38365 30811 11734  7920
  8940  8468 21302 22278 26376 41538 26471 40654 34048  5430 25306 12377
 11298 17958 47486 49573 26560 34758 47689 22062 41218 33412 19390  5656
  5110 48975 10994 34660 25863 40144 41334  3536 10800 24798 34740 34750
 40976 33506 40948 24730 12183 34328 40514 22083  5314 25790 37347 13138
 47288 28672 28652 36714 36686 38370  9009  1222 21900  1058 43454 38408
  1330  7080 21124 14078  7010  1444 23194  7146 45917 22779 21382  7842
   376 37834 37838 44870 44098  7758   494 43658   584 21598 37044  8670
 22607 16030   902 45352 30604 15826  1604 43109  9344 15402 20171  8226
 12836 24198 12752 32507 10256 32342 47160 20048 32573 35246 10297 35234
  3030 17817 17819 17747 49960  2622 46857 15450 28078  9396 27912  1940
 16956  2014 36006 27706  2250 42566 27503 39184 29930 35616 32108 17382
 15190 42312 26748 25412]
[   28    80   132   180   212   238   498   544   588   832   854   886
   892   902   966   974  1032  1047  1049  1058  1143  1330  1341  1356
  1404  1612  1662  1920  1978  2078  2158  2184  2410  2488  2576  2765
  2892  2931  2958  3118  3136  3390  3456  3524  3532  3692  3718  3756
  3811  4024  4150  4153  4362  4460  4562  4692  5068  5084  5103  5174
  5216  5306  5314  5536  5602  5638  5684  5704  5738  5798  5868  5896
  5912  6236  6246  6304  6387  6418  6428  6466  6578  6658  6974  7005
  7058  7114  7233  7259  7270  7293  7347  7606  7638  7732  7768  7784
  7898  7917  8072  8200  8468  8470  8480  8532  8613  8617  8680  8704
  8731  8799  8902  8966  9098  9104  9330  9396  9471  9489  9627  9727
  9876 10215 10243 10277 10524 10756 10831 10884 10894 11288 11298 11306
 11631 11693 11734 11759 11781 11797 11949 12000 12236 12302 12651 12692
 12729 12764 12836 13138 13370 13428 13572 13604 13658 13673 13735 13831
 14008 14308 14341 14607 14650 14653 14708 14754 14756 14762 14764 14765
 14790 14804 14819 15016 15280 15450 15642 15749 15766 15826 15827 15870
 15893 15924 15948 15975 16072 16173 16192 16676 16684 16722 16799 16959
 17001 17012 17093 17112 17209 17220 17645 17739 18056 18220 18378 18397
 18416 18423 18510 18557 18598 18834 18930 19040 19089 19244 19328 19386
 19502 19714 19888 19934 20018 20048 20110 20226 20322 20350 20522 20628
 20652 20663 20709 20746 20784 20792 20882 20903 20918 20962 21130 21164
 21351 21355 21445 21952 22200 22217 22470 22607 22675 22679 22690 23136
 23458 23546 23594 23868 23962 24036 24059 24310 24526 24587 24760 24798
 24870 24997 25159 25262 25294 25457 25546 25657 25678 25683 25688 25799
 25801 25854 25948 26052 26061 26079 26358 26376 26392 26442 26515 26516
 26613 26636 26756 26760 26766 26842 26896 27026 27085 27121 27243 27248
 27251 27472 27522 27602 27706 27859 27898 27912 27964 28357 28407 28413
 28471 28567 28574 28710 28780 28898 28954 28984 29180 29238 29434 29573
 29672 29880 29930 30049 30184 30250 30356 30450 30566 30692 30751 30770
 30833 30856 30925 31024 31100 31185 31197 31200 31275 31335 31461 31474
 31546 31576 31591 31596 31600 31650 31738 31840 31854 31895 31934 31962
 32248 32280 32297 32323 32344 32348 32394 32405 32417 32421 32510 32511
 32747 32888 32918 32934 33088 33768 34328 34404 34542 34660 34676 34703
 34758 34771 35051 35062 35068 35234 35401 35574 35622 35688 35710 35882
 35916 36141 36172 36620 36698 36714 36730 36982 37038 37050 37062 37118
 37135 37468 37551 37588 37589 37601 37606 37680 37834 37962 37966 37974
 38359 38365 38374 38397 38399 38401 38408 38798 38974 39074 39256 39289
 39327 39378 39397 39425 39488 39734 39875 39896 40280 40386 40498 40527
 40636 40654 40824 40886 40948 41156 41284 41366 41377 41396 41464 41538
 41540 41594 41618 41843 41894 42042 42140 42234 42262 42312 42359 42667
 42772 42866 42897 43198 43852 43898 44046 44078 44262 44378 44456 44758
 45143 45266 45408 45532 45615 45654 45761 45888 45944 45968 46088 46316
 46476 46906 47217 47219 47288 47291 47376 47387 47486 47538 47659 47689
 47759 47798 48091 48245 48344 48356 48402 48469 48472 48538 48556 48811
 48856 48881 48927 48933 48945 48975 49026 49062 49082 49090 49132 49143
 49153 49212 49264 49416 49488 49543 49844 49890 49892 49904 49910]
* Critical examples for prunings = 539
* Critical examples for training = 916
* PrAC images = 1145
******************************************
pruning state 3
* remain parameters = 136913.0
******************************************
* remain weight =  51.200038891880574 %
0.1
* training images number = 1145
Epoch: [0][0/9]	Loss 1.3798 (1.3798)	Accuracy 55.469 (55.469)	Time 0.08
train_accuracy 56.070
Test: [0/79]	Loss 0.1765 (0.1765)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.2672 (0.2140)	Accuracy 90.625 (94.899)
valid_accuracy 95.340
Test: [0/79]	Loss 0.1809 (0.1809)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1390 (0.2383)	Accuracy 96.094 (93.689)
valid_accuracy 94.430
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/9]	Loss 1.1653 (1.1653)	Accuracy 56.250 (56.250)	Time 0.11
train_accuracy 53.450
Test: [0/79]	Loss 0.1432 (0.1432)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1880 (0.1951)	Accuracy 95.312 (94.256)
valid_accuracy 94.650
Test: [0/79]	Loss 0.1185 (0.1185)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0856 (0.2016)	Accuracy 97.656 (93.474)
valid_accuracy 93.850
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.046102266758680344
0.010000000000000002
Epoch: [2][0/9]	Loss 1.1259 (1.1259)	Accuracy 58.594 (58.594)	Time 0.05
train_accuracy 57.991
Test: [0/79]	Loss 0.0717 (0.0717)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0957 (0.0965)	Accuracy 98.438 (98.177)
valid_accuracy 98.270
Test: [0/79]	Loss 0.0878 (0.0878)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0279 (0.0945)	Accuracy 100.000 (98.055)
valid_accuracy 98.310
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.006442047189921141
0.0010000000000000002
Epoch: [3][0/9]	Loss 0.9072 (0.9072)	Accuracy 59.375 (59.375)	Time 0.05
train_accuracy 61.921
Test: [0/79]	Loss 0.0680 (0.0680)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0877 (0.0895)	Accuracy 99.219 (98.361)
valid_accuracy 98.470
Test: [0/79]	Loss 0.0869 (0.0869)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0268 (0.0893)	Accuracy 100.000 (98.346)
valid_accuracy 98.550
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0007011751877143979
* remain weight =  40.95988152934841 %
* best SA=98.55
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  40.95988152934841 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  709
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  51.200038891880574 %
Test: [0/391]	Loss 0.0660 (0.0660)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.0904 (0.0809)	Accuracy 97.656 (98.713)
Test: [100/391]	Loss 0.1020 (0.0845)	Accuracy 98.438 (98.654)
Test: [150/391]	Loss 0.0662 (0.0850)	Accuracy 99.219 (98.603)
Test: [200/391]	Loss 0.0715 (0.0838)	Accuracy 98.438 (98.678)
Test: [250/391]	Loss 0.0863 (0.0851)	Accuracy 96.875 (98.606)
Test: [300/391]	Loss 0.1116 (0.0857)	Accuracy 96.875 (98.591)
Test: [350/391]	Loss 0.0887 (0.0854)	Accuracy 97.656 (98.611)
valid_accuracy 98.578
[ 1988 20016 31608  6304 46486 27026 47288 25994  9534 37838 48402  7606
 25466 48356 49818 35916 16799 23730 30156 32507  6347  6236 20816 46316
  4024 44939 32511 47376 39074 11104 37140 10282 13572 25975 31650 24560
 35873 15528 47291 46466  4030 42112 14819  2892 31596  6269 37198 27898
 17763 14896  5536 44243 42964 27912  7584 31591  7638 10253  4542 22470
 35883  1940 24526  9489  1978 13452 48397 20029 27085 39708 42042 20784
 17772 23770  1920 35882 21900 13428 37834 25457 42973 47152 39088  5972
 39327 37601 29672 46806 39342  2338 37606 24084  2342 48681 42359 37589
 37588    80 27526  9814 42334 48556 29880 31958 12830 32323 19040 49960
 24310 12836   132 20460 42312 17220 44758  7842 27292 22278 27293 15328
 22130 27522 13067 27514 37551 11306    28 25648  5821 25683  5868  5866
 39423 39424 48638 12967 40824 37521 35480 17382 20322 46887  8072 12940
  7920 34010 37468 27503 15280 39488 17314 29712 29713  7907 17326  5896
 11298  2410 22217 37557 27472  7917 15190 20663 37396 42648 17645  2765
 21982  2078 20110 25854 12729 27810  9627 17079  7732 40977 40976 44870
 48469 47160 17608 42810 33768 49844 22436  8200 47237 21952 20652 41016
 17663 46524 18776 17001 13370 20641 12692 44378 47219 47217  2052   238
 48472 47159 49892 25801 37672 29588 23962 20522 42262 20171 35688  5684
 42667 27248 44484 27251 29930 31934 22062  9727  2184 10048 27700 37680
 17592 31850 15016 33812  2148 48486  2158   212  4150 18834  4428  6102
 17120 17121 44456 29573 40948 31884 19960 12000 33622  7293 26622  6808
 22802 26626 15893 26629  8552 19390 49090  3290 29055 47689 49082 26646
 41718 28471 10894 21164 14008 40076 41334 28567 43510 41358  7259 31000
  5103  6848  5110 33388 31024 28534 28533 34771 26613 49416 26228 36425
 45868 48091 30482 14072 15870  3756 23336 18514 28413 45968 14582 28407
 41276 11781 18056 23361  5216 49501 29121  8488 31185 40434 23385 38672
 12302 41284 32934  7347  8532  3268 28453 45930  5174  1784 21130 36363
  6755 49062 34829 33448 19360 45306 36934 25232 45944 11797 49464 31100
 32888 25159 15948  4909 40280 23040 36642  8797  1058  8799 30811 45617
 49285 38365 41438 45654 24997 33170 34542 43818 28898 14230 26358 45616
 49264 14308 47940 26376 41478 10756  7114 30751  3536 36714  8729 41473
 47912  8772 30770 16072 14341 45532 30692 33262 38374 41366  8680 43826
 10831 36522 45761 47798 41618 33088  5068   832 34703 41377 45792 23194
 43898 49132 30566 14078 28984 45408  8617   846 28954 26516  1097 33304
 38381 34532 18378  5000 48006 41562  6974 43852 49164 14172  1143 28672
 47828 41594   854  8898 43575   902 45266 49067 22183 37050 35051 13735
 29226 26079 26842 35062 19866 12416 48228 36698 36172 46274 14764 18598
 29206 44110 23494  6578 38770  5306  3118 49573 35068 48930 15660 32814
 17817 48927 28279 14765 17958 39896   558 14740 26896 40510 26052 24630
  9330 48945  8384 45143 39875 48966 23546 26061 20918  6540  6466 30262
 40498 19934 45154 32747  6524 10319  9344 13719 49514 48242 47486   498
 10315 48968 30356 13662 26756 32835  6658 46298 10297  5430  6670 25262
  6636 44078 22690 49543 17811 14650 23406 36982 15778 13850 21042 18557
  9376 16560 23422 19328  1514 28282 38700  8468 28072 31216 24722 12377
 22679  8480 32880 44090 46288  6428 23458  6418 24760 29311 15766 46088
 11734 29180  3136 46300 31275 26740 21302 11631  3998  3030 31760 38408
 48538 37966 16684 42566 38650 33296 37962  7005 32918  2576 36104  5908
 32394 42897 31672 31576 35324 22591  1612 38359  8347 26766 42234 39256
 47710 44690 22675 39260 30544 20350 29803 17747  9396 26940  1047 24101
 41424 22607 23588 38339 45615 43109 23038 17819 10800 36006 15642 46857
 36686 25678 39482 39801 22169 38370  5912 43658 41540 16030 34234 47340
 37574  7080 40527 37469 41538 26398 24198 28078  7010 21382  2554  7146
  4476 44262 22779 39940  7768 44098 26636  1341 41218 25294 47538 25546
 33412 15402 27706 45352 12651 43454 25799 45917 21598 32405  5638 28357
 30049 38050 35234 40654  1444  5602 35246 24798  9098 47646 15450  1404
 19808 33506 24730 12752 42866  9009   584  1604 40752 49139 18190 11693
 32573 40144 13138 35616 28652 30604 18220 17739 25412 38401 26471 34660
 40514 10994 34048 16676 26560 20048 25790 25306  7270 38526 34328 34758
 32342   886 12183  2250 34740 47290 14468 22643 25156   376 36141 34750
 22083]
[   80   132   172   178   180   517   544   547   588   854   892   902
   966  1032  1049  1117  1121  1239  1453  1512  1518  1552  1604  1612
  1634  2044  2158  2258  2488  2661  2707  2901  2928  2931  2958  3136
  3273  3290  3456  3524  3532  3543  3644  3718  3808  3810  3811  4028
  4122  4129  4150  4158  4362  4460  4506  4529  4542  4562  4625  4692
  4694  5026  5084  5103  5174  5177  5216  5430  5536  5602  5638  5656
  5704  5738  5821  5868  5896  5912  5936  5972  6092  6236  6269  6347
  6418  6428  6578  6582  6746  6808  6848  6942  6962  6974  7005  7270
  7299  7347  7530  7584  7606  7638  7732  7758  7768  7784  7842  7880
  7917  7989  8072  8104  8154  8200  8226  8347  8468  8470  8488  8670
  8690  8715  8719  8730  8761  8772  8799  8854  8867  8883  9114  9220
  9266  9330  9396  9450  9471  9489  9661  9876 10172 10207 10215 10243
 10277 10282 10484 10524 10746 10774 10800 10831 10884 10894 11104 11196
 11210 11288 11688 11734 11759 11775 11781 11797 11885 11950 11953 12000
 12076 12317 12692 12729 12744 12764 12836 12840 13002 13031 13138 13242
 13394 13428 13524 13658 13673 13703 13766 13831 13854 14078 14191 14222
 14260 14292 14341 14368 14607 14650 14655 14700 14740 14754 14756 14760
 14762 14764 14765 14790 14796 14872 14886 14896 14982 15076 15280 15412
 15434 15450 15486 15728 15749 15766 15827 15924 15942 15948 15949 15988
 15991 16072 16084 16116 16173 16192 16242 16376 16549 16658 16676 16678
 16684 16722 16862 16933 16957 16963 17001 17082 17192 17326 17540 17648
 17654 17728 17739 17756 17811 17817 17890 18056 18130 18228 18378 18382
 18397 18405 18440 18508 18557 18598 18732 19244 19306 19386 19412 19494
 19714 19808 19866 20018 20048 20050 20110 20116 20157 20171 20196 20226
 20363 20534 20614 20663 20709 20784 20792 20882 20918 20962 20976 21130
 21164 21351 21355 21382 21392 21445 21598 21726 22453 22470 22483 22607
 22675 22679 22690 22719 22838 22989 23086 23136 23195 23336 23384 23458
 23546 23710 23824 23886 23962 24042 24059 24101 24310 24526 24560 24587
 24760 24798 24870 24887 25156 25262 25294 25318 25412 25546 25648 25657
 25678 25688 25790 25801 25829 25861 25863 25948 25949 26034 26046 26135
 26228 26354 26358 26398 26418 26442 26444 26471 26515 26516 26613 26629
 26636 26756 26760 26766 26852 26896 27024 27085 27118 27121 27241 27243
 27248 27323 27502 27514 27687 27706 27859 27964 27968 28116 28149 28150
 28175 28231 28262 28326 28357 28422 28471 28491 28512 28530 28534 28574
 28654 28710 28898 28900 29026 29109 29145 29180 29181 29189 29238 29286
 29335 29410 29431 29434 29462 29712 29731 29780 29880 29930 30041 30049
 30262 30450 30452 30566 30751 30770 30833 30903 30962 31185 31200 31275
 31335 31461 31474 31576 31591 31596 31600 31606 31622 31650 31652 31706
 31710 31736 31738 31786 31840 31850 31895 31934 31946 31958 31962 31981
 32002 32018 32176 32276 32323 32348 32372 32394 32421 32507 32510 32511
 32571 32573 32747 32814 32918 32934 33038 33288 33406 33572 34010 34048
 34274 34328 34422 34532 34660 34678 34771 34829 34841 35062 35068 35102
 35234 35401 35480 35574 35622 35710 35787 35883 35991 36141 36172 36452
 36606 36642 36651 36698 36714 36730 36783 36808 36866 36934 36984 37038
 37050 37118 37124 37135 37140 37275 37352 37374 37469 37552 37574 37588
 37606 37672 37704 37750 37778 37834 37962 37966 37974 38252 38256 38345
 38365 38374 38397 38399 38401 38680 38848 39031 39307 39355 39378 39397
 39405 39423 39482 39516 39625 39734 39752 39864 40022 40066 40280 40386
 40466 40527 40571 40595 40654 40948 41109 41156 41218 41284 41305 41377
 41396 41464 41538 41540 41562 41618 41843 41882 42042 42112 42140 42234
 42262 42312 42428 42482 42604 42624 42687 42793 42828 42854 42897 43109
 43198 43208 43510 43544 43592 43670 43826 43898 44046 44078 44174 44340
 44355 44456 44624 44736 44822 44954 44961 45011 45048 45056 45532 45761
 45772 45868 45888 45917 45944 45968 45992 46066 46088 46118 46122 46316
 46336 46476 46689 46724 46828 46938 47217 47288 47297 47376 47387 47401
 47486 47538 47540 47669 47689 47759 47781 47828 47844 47926 48065 48070
 48102 48242 48330 48356 48402 48460 48472 48512 48538 48556 48752 48824
 48945 49026 49062 49064 49082 49094 49109 49153 49212 49233 49264 49386
 49416 49487 49488 49493 49525 49543 49563 49579 49844 49928 49960]
* Critical examples for prunings = 707
* Critical examples for training = 709
* PrAC images = 1160
******************************************
pruning state 4
* remain parameters = 109530.0
******************************************
* remain weight =  40.95988152934841 %
0.1
* training images number = 1160
Epoch: [0][0/10]	Loss 1.2165 (1.2165)	Accuracy 54.688 (54.688)	Time 0.04
train_accuracy 57.931
Test: [0/79]	Loss 0.2728 (0.2728)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.3705 (0.3030)	Accuracy 86.719 (90.732)
valid_accuracy 91.070
Test: [0/79]	Loss 0.2540 (0.2540)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2880 (0.3134)	Accuracy 91.406 (90.671)
valid_accuracy 90.970
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/10]	Loss 1.1261 (1.1261)	Accuracy 55.469 (55.469)	Time 0.05
train_accuracy 55.172
Test: [0/79]	Loss 0.2023 (0.2023)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2286 (0.2120)	Accuracy 92.188 (93.382)
valid_accuracy 94.230
Test: [0/79]	Loss 0.2812 (0.2812)	Accuracy 91.406 (91.406)
Test: [50/79]	Loss 0.0707 (0.2338)	Accuracy 99.219 (93.107)
valid_accuracy 93.830
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.061718251556158066
0.010000000000000002
Epoch: [2][0/10]	Loss 1.0030 (1.0030)	Accuracy 64.062 (64.062)	Time 0.04
train_accuracy 60.000
Test: [0/79]	Loss 0.1286 (0.1286)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1230 (0.1145)	Accuracy 97.656 (97.304)
valid_accuracy 97.570
Test: [0/79]	Loss 0.1389 (0.1389)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0431 (0.1331)	Accuracy 99.219 (96.982)
valid_accuracy 97.380
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00692047830671072
0.0010000000000000002
Epoch: [3][0/10]	Loss 0.8583 (0.8583)	Accuracy 65.625 (65.625)	Time 0.11
train_accuracy 63.534
Test: [0/79]	Loss 0.1217 (0.1217)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1177 (0.1067)	Accuracy 96.875 (97.641)
valid_accuracy 97.870
Test: [0/79]	Loss 0.1268 (0.1268)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0346 (0.1252)	Accuracy 99.219 (97.120)
valid_accuracy 97.530
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008399525540880859
* remain weight =  32.767905223478735 %
* best SA=97.53
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  32.767905223478735 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  677
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  40.95988152934841 %
Test: [0/391]	Loss 0.0801 (0.0801)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.1178 (0.1034)	Accuracy 97.656 (97.901)
Test: [100/391]	Loss 0.1411 (0.1058)	Accuracy 94.531 (97.749)
Test: [150/391]	Loss 0.0596 (0.1073)	Accuracy 99.219 (97.749)
Test: [200/391]	Loss 0.0872 (0.1071)	Accuracy 97.656 (97.734)
Test: [250/391]	Loss 0.1075 (0.1073)	Accuracy 96.875 (97.719)
Test: [300/391]	Loss 0.1601 (0.1083)	Accuracy 94.531 (97.664)
Test: [350/391]	Loss 0.1146 (0.1073)	Accuracy 96.875 (97.703)
valid_accuracy 97.678
[45930 46122 25854 16376  1341 23588 26613 25466 21130 32835 35068 17811
 11298 46288 10319 25863 43198 28072 31100 33768 15486  6466 46298  7907
 30049 28453 29713 45917 40076 37574  3030  8200  9330  7989 19808 35051
 31474 49416 40066 34010 29712 21164 44954 49514  7917 37521 22470 28530
 31981  4562  4028  9220  4024 28534  9727  9376 24887 43510 49573 34740
 47540 47538 12967 23962 35688 26515  5704 33088 37838 39260 48638 40595
  2488 10048 31000 46938 46724 48402 31024  1634 22130 13766  5868 29731
 22436 14008 28491  5866 45868 48397 32814 14896 32002 20962 24870 27248
 25657 31335   376 20460 14872 23194  6428 37834 11104 24560 22453 18130
 37557  4542 49386 20816 47689 23422 20322 35574 37672  4428 29930 24730
 48512 44484 39397 46887   172 34048 17314 46066 31200 16549 32934 32880
 29672 31185 38650 47646 39516  9098 13673  7784 26842 20918  8104 32276
 13067 29588 13662 46336 38672 31216  1514  6670 44758 40752  1512 16676
 15328 16678 39378  2410  6524 27472 24630  7842 47486 46857 23361 12302
 32888  6658 18834 16684 23385 32918 24198 49501 38680 29880 13831 38700
 39488 24101  5174 29573 13703   238 33812  6755 48469  5972 43109 27292
 27526 36172  9876 18056 37589 25683  7732 26629 34829  4476  2338 21952
 35616 37750  5177 36104 10524 25829  3118 26626 46806 18776  3290  1518
  1404 27502 15280 39342 27503 11196 16658 24059 37606 28279 44456 37601
 12416 23494 25790 15402   212 40976 23406  4362 40977 42359  4460  4150
 46088 25799 45968 36363 34841  6636 20226 23546 49464 39875 26852 26646
 39256 35883 38397 31884 14368 41464 43898 41377 49062  9489 31760 47159
 16933 47217  1988 42973 47237 16030   588 17082 33262 49285  7005 38256
 45408 31622  1978 30751 44110 25159 48242 26046 33572 14260 39708 31606
 29238 28654   544 48966 10253   547 45154  8854 43826 37962 33170   846
 37966   558 11693 36934 46486 17739 43818 14172 37974 32421 41562 32511
  1121 28652 42793 42964 21355  3524  8799 22675 28984 17608 30811 30770
 30544  2901 38345 47912 46476 33506  2892  3810 45266 20016 11775 44046
 36982  8772  1047  4909  2765  8488 31650 44078 30833  1097 20641 37038
 42234 47940 16862 10243 13452 30566 19494 38381 23886 14655 20018 43658
 15827 14650 26079 49892 27118 20110 38374 41366 21392 20709 47291  2250
 36866 19390 49064  2184 39625 18190 17763 18382 11631 31576 14790 31934
 49164 49082 29311 35324 49928 31738 17540 36006  8729 34532 32394  8898
 48927  8719 49844 41424 24526 47828 17663 46689 15870 14796 47376  3756
   886 31958 35102 14819 39074 28567 45048 10884 47798 10282 48330 13394
 41473 44243  9396 43544 24310 36808 16072 10277 20784 18378 30962 32507
 15991 41618  6974 41156 10207 35873 16242 27964  6102 31596  2707 14740
 36783 15893 12692 31600 49067   854 23730 20522 47219 30482  8617  6962
 34678 44174 29055 22838 18598 30903 26442 45143 46524 15660 16963  8532
 32405 20663 15642 43575  6092 42262  7530 14765 14764 20029 19412 25994
 13524 40280 14762  3644 17120 30262 10894 14308 27968 14760 34542  7114
 14754 20652 33622 14756 33406 19040 39031 17326 14582 15924 15948 49818
 43592  4506 19866  6236 28413 26052 31591 44690 13370 26061 37552  2158
 16957 37140 16173 28357  3532 39940 33296 41540 32372 34234 15766  6578
 42648 36522 45888 30604 23770 37551 42854 35916 22083 32573 49090 27026
 27085 22169  7146 10800 37468 37469 16560  7270 47290 17958 20048 33412
 14468 28175 37588 13719  2052 26756 25678   832 41334 17382  7920  8670
  1604 22183 39482 40144  8552 12836 43454 17817 44939 25318 17819 42604
  9344  9009 35062 18220 25306 42334 28078 34328 10994 22607 14700 38401
 28672 15450 12752 34660 13138  8226 21900 46300  7584 44262 14078  5084
  6418 26940 13572 20171 34750  4030 10297   584 49960 25412 42042 47759
 38526 26471 45761 32342  5110 12183 12830 26560 34758 49212 47160 47340
  8480 45615 26376 41284 40948  7080 36686 36141  6304  8072  5602  2554
 35246 36714 23824 22062 33448 21598 19360  5912 20350 35234 45352 27912
  5216 38050 38339  8154 41218 44870  4692 27293 27522 18557 17079  7010
 24798 22643  4129 25801 44098  3268  2148 41538 28710 38370 20882 38365
 49264 22679 24997 27706  1444]
[   80   212   238 ... 49910 49960 49991]
* Critical examples for prunings = 1132
* Critical examples for training = 677
* PrAC images = 1538
******************************************
pruning state 5
* remain parameters = 87624.0
******************************************
* remain weight =  32.767905223478735 %
0.1
* training images number = 1538
Epoch: [0][0/13]	Loss 1.6975 (1.6975)	Accuracy 45.312 (45.312)	Time 0.08
train_accuracy 61.704
Test: [0/79]	Loss 0.5395 (0.5395)	Accuracy 81.250 (81.250)
Test: [50/79]	Loss 0.6631 (0.5414)	Accuracy 80.469 (82.690)
valid_accuracy 83.500
Test: [0/79]	Loss 0.4874 (0.4874)	Accuracy 78.906 (78.906)
Test: [50/79]	Loss 0.2124 (0.5885)	Accuracy 92.188 (80.775)
valid_accuracy 82.410
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/13]	Loss 1.1227 (1.1227)	Accuracy 60.156 (60.156)	Time 0.05
train_accuracy 54.616
Test: [0/79]	Loss 2.2053 (2.2053)	Accuracy 46.094 (46.094)
Test: [50/79]	Loss 2.2078 (2.2438)	Accuracy 46.094 (49.280)
valid_accuracy 49.420
Test: [0/79]	Loss 1.8981 (1.8981)	Accuracy 56.250 (56.250)
Test: [50/79]	Loss 2.4198 (2.4489)	Accuracy 45.312 (47.825)
valid_accuracy 48.080
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.11177302896976471
0.010000000000000002
Epoch: [2][0/13]	Loss 1.2610 (1.2610)	Accuracy 53.906 (53.906)	Time 0.04
train_accuracy 58.127
Test: [0/79]	Loss 0.2172 (0.2172)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.2913 (0.2382)	Accuracy 93.750 (93.765)
valid_accuracy 94.200
Test: [0/79]	Loss 0.1993 (0.1993)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.1033 (0.2670)	Accuracy 97.656 (91.881)
valid_accuracy 93.330
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009517940692603588
0.010000000000000002
Epoch: [3][0/13]	Loss 0.9014 (0.9014)	Accuracy 67.188 (67.188)	Time 0.04
train_accuracy 64.499
Test: [0/79]	Loss 0.2339 (0.2339)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.3272 (0.2551)	Accuracy 91.406 (93.612)
valid_accuracy 93.850
Test: [0/79]	Loss 0.2443 (0.2443)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.1295 (0.2812)	Accuracy 98.438 (91.575)
valid_accuracy 92.830
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007828905247151852
0.0010000000000000002
Epoch: [4][0/13]	Loss 0.8423 (0.8423)	Accuracy 69.531 (69.531)	Time 0.04
train_accuracy 64.369
Test: [0/79]	Loss 0.1993 (0.1993)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.2711 (0.2217)	Accuracy 92.188 (94.746)
valid_accuracy 95.000
Test: [0/79]	Loss 0.2022 (0.2022)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.0982 (0.2452)	Accuracy 99.219 (93.153)
valid_accuracy 94.340
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006619191262871027
* remain weight =  26.214249386704957 %
* best SA=94.34
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  26.214249386704957 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  896
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  32.767905223478735 %
Test: [0/391]	Loss 0.1753 (0.1753)	Accuracy 96.094 (96.094)
Test: [50/391]	Loss 0.2422 (0.2157)	Accuracy 93.750 (94.654)
Test: [100/391]	Loss 0.2309 (0.2220)	Accuracy 93.750 (94.609)
Test: [150/391]	Loss 0.1865 (0.2253)	Accuracy 96.094 (94.407)
Test: [200/391]	Loss 0.1876 (0.2241)	Accuracy 93.750 (94.352)
Test: [250/391]	Loss 0.1990 (0.2290)	Accuracy 96.094 (94.108)
Test: [300/391]	Loss 0.3158 (0.2285)	Accuracy 86.719 (94.173)
Test: [350/391]	Loss 0.2373 (0.2278)	Accuracy 92.969 (94.222)
valid_accuracy 94.224
[25318 37588 22562 40976 37507 45154 48881 44078 37109 15862 25412 48330
 37589 14754 40654 25801 29930 18686 40874 22559 15870 44850 44294 34234
  4506 14650 45048 37546 44243 44098 15779 29226 21726  4028 43898  4292
 37557 21856  8470 30168  8104 40977 37750 14756 14655  7530 34430 29940
 21888 37521 44110  4039  8072 15328 30142 14762 44758  8532  4476 14764
 29713 38050 14765 29659  7638 38052   500  8480  8154 15420 15827 29180
 11693 30312 14819 29881 37135   391  7784   647 44340  7584 22364 48288
  4362 37276 19124 34406  8488 44174 40571 15826 25829  8552 48512 40386
 29868 25688 44690 15434 22470 29329  7606 14804 37834 33812  8560 45011
 37249 29880 25159 14740 34352 48937 11631  7768 15486 11283 22130 14700
 15500  4438 40728 34010 48245   238 48344  4542 29457 37491  4446 44923
 40948 37028  4150 44456 11292    80 37118 11378 22240 22453 37453 48290
 37117 41054 14708 29065  4562 44213 48472 48945 48927 25854 37469  7917
  4692 14796 25321 48402 44224   558 21900 49928  4024 41772  1920 19866
 35051 23494 32835 47475 26629 20784 26626 16865 39832 12416 47486 26613
 41739 36104 42973 24526 42964 27968 27964 31461 13572  3290  9436 49515
 13658 32880 20816 13650 32888 12377 32814 46300 35136 20663  5602 17890
 17005  3136  2052 47376 19934 41843 23588 39752  6466 13494 42793  6486
 16963 49573 31591  6524 41795 46314 31600 13564 16933 23538  1988 46336
 13524 47410 20709 17940  3118 49501  6658 13854 26458 18130 24630 31252
 24633 20976 47646 38680 36303 38672 45968 49386 19714 38640 45930 43198
 18190 16549 41538  6885  1512 28260 38700  1518 33088 36363 16572 47690
  9220 31185 23195 24533 31275 28141  5456 47540 32918  9396 49488 13719
 36172 28014 36174 39875 19810 20881 24560  5430 13760  8617 10521 13831
 46024 41618 43098 16684 12302 16658 24589 46066 31335 26504 28078 10524
 28072  9330 26756  6418 26760 27293 49863 27299 32405 47068 20322 12808
 39342 27320 32421 46806 49844 32112  6092 32104 27026  6102 35688 27428
 26978 23886  9876 17326 27292 35324 39289 20110 49818 17663 35632 24198
 24210 47159 12820 13026 46976 20226 32276 27180 39482 46938 49960 39486
 24052 27121 27118 20250 24121 12967 47012 32348 49892 32394 32391 12834
  6034 39378 12836 42334 39516 35402 39397 20171 27243 20172 49890 27085
 47160 42042 32507 23730 10277 31840 17808 47288 10297 47290 46524 47291
 17120 17819 39074 12585  2158 46486 31778  2148 19960 26766 35916 31738
 20628  5632 20016 42703 17073 39708 17079 10319 17082 12576 31760  6304
 42604 41962 17240 31954 42517 12692 31958 39625 20050  2931 27472 42501
  2361 10215 32002 32510 39212 28279  6236  2958  5704 47247 35234 20522
  2254 20029 27502 31884 17188 10253 17763  5718 12663 32571 47237 34829
 28066 33552 18398  4934 16242  1143  3756 34648 18405 10800 28898 22675
  1121 38345 30770 18416  5084 14308 43575 24887   846 30751 11949 22679
 28654 41156  3772 33378 30482  5110 14580 16302 33304 16298  8966 38381
 36620 45617  5126 15960 14230 45574 30846 47912 36934 12000 49014 18382
 43826  8942 11786  8680 45266 26158 38374  7233   854 14528 45408 30626
   966  5013 49090 14501 16030 22766 33506 28788 25990 40236  7347 49082
  8772 22750 10886 24990 30566 34532 49067  7374 49062 10894 30604 26052
   886 33426 41284 18557 21355  4955 36730 12181  5062 49139  8854  3810
  3811 19412 22838  1047 40280  8719  7270 22830 14368  8729 33406 22824
 21392 43658  1001 30875  8975 45352 28413 24820 21124 28357 21130 19360
 26304 28471 41396 22633 11767 28453 30962 15924  4909  7005 14078 25910
 36522 41424 47828  3998 45761 18609 20211 49264 40076 31005 21164 19609
 22607 12137 38082 33572 41377 15948 45815 14008 23136 49212  1444 12153
 11775 12151 14172 18220 28984 47759   749 26358 21232 48975 30881  3524
 28491 24730 46724 37552 14866 21351  1634 30262 46476 14341 12729 13394
 17209 16173 49070   718  2320 37050 41907 26442   172 32323 18930 29803
 31989 12940 26852 48242  6974 15280 40752 29055 31200 42648  2707 29238
 31216  2184 48091 36875 35873 17093 14653  1604  8731  8668 25683 30811
 31395 25797 38370 31596 43544 13735 10944 15975 18397 43818 13067 20882
 29391  6578  4153 42897 44262  5738 30041 35401 26646 37124 31622 35622
 45143 23385 34678 11759 19752 25657 14100 42140 42772 19808 18423 43454
 14607 40824 34740 35102 16961  5821 11298 14582 24101  8670  2576 36982
 10994 30356 25994 26940 46857  8200 22643 30450 15402 21598   902  8202
 23824  5868 42482 49904 15991 22083 34048 39256 35310 29712 46906 25863
  7490 25861  8701 13138  5798 32342 41113 32372  4030 49026 29573 15190
 46689 18598 12752 32511 24059  5912  2901 17382 20350 27248 48966 37468
  2488 35616 45868  7920 20048 18322  9344 30049 26515  7080 46088 25678
 46122 32934 30184 38408 45654 38401 38397 24798 26560 40144 40514 30870
 45615  7146  3268 28567 28574 43510 26622 27898 38359 41334   547 33296
 49543  1330 24587  7732 12183 26376 14896 33768 41540 23194 16560  5314
 45917 47689 11104  1404 38526 20918 49285 14072 15642 34328  7010 34758
  5216 49416 34750 26471 43109 16676  1341 44870 37838  9098 34660 35062
 17958 28710 13370  5656 37396 33412 21382  3030 26842 10282  8799 22200
 15450 37680  2250  7264 21952  8226 48933 41218 16084 12651 35246 42566
 32573 14468 27522 47217 31934 27503 12679 25546 35883   212 14760 46288
  1940 35068 31576 46298 38339 31606 36686 25306 14790 36006 11196 25799
 31650 37962 32747 28652  1058  5638 47340 37974 27706 20641 17739 28672
  2078 13452   588 37966   584 19502 43592 22062]
[   25    28    48 ... 49945 49955 49985]
* Critical examples for prunings = 2793
* Critical examples for training = 896
* PrAC images = 3314
******************************************
pruning state 6
* remain parameters = 70099.0
******************************************
* remain weight =  26.214249386704957 %
0.1
* training images number = 3314
Epoch: [0][0/26]	Loss 1.2229 (1.2229)	Accuracy 55.469 (55.469)	Time 0.08
train_accuracy 76.554
Test: [0/79]	Loss 0.1323 (0.1323)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1483 (0.1436)	Accuracy 96.094 (96.829)
valid_accuracy 97.000
Test: [0/79]	Loss 0.1446 (0.1446)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0468 (0.1561)	Accuracy 100.000 (96.308)
valid_accuracy 96.780
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/26]	Loss 0.5263 (0.5263)	Accuracy 80.469 (80.469)	Time 0.05
train_accuracy 82.740
Test: [0/79]	Loss 0.0727 (0.0727)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1622 (0.1167)	Accuracy 93.750 (96.860)
valid_accuracy 96.850
Test: [0/79]	Loss 0.0811 (0.0811)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0430 (0.1193)	Accuracy 99.219 (96.814)
valid_accuracy 96.920
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.062483061105012894
0.1
Epoch: [2][0/26]	Loss 0.4267 (0.4267)	Accuracy 83.594 (83.594)	Time 0.04
train_accuracy 83.675
Test: [0/79]	Loss 0.1339 (0.1339)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.2001 (0.1629)	Accuracy 92.969 (95.175)
valid_accuracy 95.340
Test: [0/79]	Loss 0.0734 (0.0734)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0310 (0.1711)	Accuracy 99.219 (94.455)
valid_accuracy 94.910
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.05292515084147453
0.1
Epoch: [3][0/26]	Loss 0.3633 (0.3633)	Accuracy 86.719 (86.719)	Time 0.04
train_accuracy 84.128
Test: [0/79]	Loss 0.1259 (0.1259)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1809 (0.1944)	Accuracy 92.969 (94.547)
valid_accuracy 94.930
Test: [0/79]	Loss 0.1410 (0.1410)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0819 (0.2062)	Accuracy 99.219 (94.715)
valid_accuracy 94.700
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04890226945281029
0.1
Epoch: [4][0/26]	Loss 0.3023 (0.3023)	Accuracy 89.844 (89.844)	Time 0.04
train_accuracy 85.003
Test: [0/79]	Loss 0.2094 (0.2094)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2115 (0.1918)	Accuracy 93.750 (92.953)
valid_accuracy 93.500
Test: [0/79]	Loss 0.2304 (0.2304)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.0854 (0.2177)	Accuracy 98.438 (92.479)
valid_accuracy 93.250
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04804633557796478
0.1
Epoch: [5][0/26]	Loss 0.3815 (0.3815)	Accuracy 87.500 (87.500)	Time 0.05
train_accuracy 85.456
Test: [0/79]	Loss 0.1846 (0.1846)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.2510 (0.1756)	Accuracy 92.969 (95.190)
valid_accuracy 95.410
Test: [0/79]	Loss 0.1849 (0.1849)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1279 (0.1780)	Accuracy 95.312 (95.190)
valid_accuracy 94.710
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.04582091048359871
0.010000000000000002
Epoch: [6][0/26]	Loss 0.5552 (0.5552)	Accuracy 80.469 (80.469)	Time 0.12
train_accuracy 87.025
Test: [0/79]	Loss 0.0456 (0.0456)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0765 (0.0684)	Accuracy 97.656 (98.392)
valid_accuracy 98.410
Test: [0/79]	Loss 0.0710 (0.0710)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0203 (0.0732)	Accuracy 99.219 (98.238)
valid_accuracy 98.440
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008331074379384518
0.010000000000000002
Epoch: [7][0/26]	Loss 0.3019 (0.3019)	Accuracy 89.844 (89.844)	Time 0.05
train_accuracy 88.986
Test: [0/79]	Loss 0.0429 (0.0429)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0746 (0.0598)	Accuracy 96.875 (98.621)
valid_accuracy 98.690
Test: [0/79]	Loss 0.0594 (0.0594)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0116 (0.0670)	Accuracy 100.000 (98.330)
valid_accuracy 98.600
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.005848871078342199
0.010000000000000002
Epoch: [8][0/26]	Loss 0.3180 (0.3180)	Accuracy 89.844 (89.844)	Time 0.05
train_accuracy 89.137
Test: [0/79]	Loss 0.0331 (0.0331)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0684 (0.0527)	Accuracy 96.875 (98.729)
valid_accuracy 98.750
Test: [0/79]	Loss 0.0517 (0.0517)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0107 (0.0571)	Accuracy 100.000 (98.591)
valid_accuracy 98.850
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0055635604076087475
0.0010000000000000002
Epoch: [9][0/26]	Loss 0.2745 (0.2745)	Accuracy 89.844 (89.844)	Time 0.04
train_accuracy 89.710
Test: [0/79]	Loss 0.0343 (0.0343)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0709 (0.0529)	Accuracy 96.875 (98.759)
valid_accuracy 98.780
Test: [0/79]	Loss 0.0533 (0.0533)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0114 (0.0578)	Accuracy 100.000 (98.545)
valid_accuracy 98.790
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0004564972477965057
0.0010000000000000002
Epoch: [10][0/26]	Loss 0.2884 (0.2884)	Accuracy 88.281 (88.281)	Time 0.05
train_accuracy 89.348
Test: [0/79]	Loss 0.0354 (0.0354)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0704 (0.0536)	Accuracy 96.875 (98.775)
valid_accuracy 98.800
Test: [0/79]	Loss 0.0529 (0.0529)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0112 (0.0587)	Accuracy 100.000 (98.560)
valid_accuracy 98.780
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0004564972477965057
0.0010000000000000002
Epoch: [11][0/26]	Loss 0.3515 (0.3515)	Accuracy 91.406 (91.406)	Time 0.05
train_accuracy 90.163
Test: [0/79]	Loss 0.0368 (0.0368)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0730 (0.0553)	Accuracy 96.875 (98.729)
valid_accuracy 98.750
Test: [0/79]	Loss 0.0549 (0.0549)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0116 (0.0607)	Accuracy 100.000 (98.468)
valid_accuracy 98.730
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006562147755175829
* remain weight =  20.971324717285945 %
* best SA=98.78
* find early bird tickets at epoch 12
Remove Pruning
Pruning with custom mask
* remain weight =  20.971324717285945 %
* record size = (60000, 12)
zero all unforgettable images out, rest number =  1194
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  26.214249386704957 %
Test: [0/391]	Loss 0.0261 (0.0261)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0560 (0.0450)	Accuracy 97.656 (98.943)
Test: [100/391]	Loss 0.0526 (0.0466)	Accuracy 99.219 (98.933)
Test: [150/391]	Loss 0.0247 (0.0479)	Accuracy 100.000 (98.919)
Test: [200/391]	Loss 0.0403 (0.0474)	Accuracy 99.219 (98.931)
Test: [250/391]	Loss 0.0407 (0.0469)	Accuracy 99.219 (98.970)
Test: [300/391]	Loss 0.0790 (0.0473)	Accuracy 96.875 (98.959)
Test: [350/391]	Loss 0.0316 (0.0470)	Accuracy 99.219 (99.001)
valid_accuracy 98.980
[35234    28 24557 ... 34750  7842 37469]
[  132   178   238   523   547   588   832   854   886   902   966  1032
  1047  1049  1143  1190  1291  1400  1404  1512  2158  2488  2576  2676
  2707  2855  2877  2892  2931  2958  3118  3136  3184  3388  3456  3524
  3532  3692  3718  3756  3811  4024  4028  4030  4039  4150  4152  4153
  4362  4562  4692  5000  5002  5103  5174  5278  5314  5422  5513  5602
  5718  5738  5821  5868  5954  6236  6254  6304  6448  6466  6636  6658
  6885  6962  6974  7009  7058  7112  7259  7270  7293  7347  7452  7498
  7606  7638  7732  7758  7768  7784  7833  7885  7907  7920  8104  8200
  8283  8470  8532  8613  8680  8701  8729  8730  8731  8752  8772  8832
  8883  8966  9330  9332  9471  9489  9627  9727  9814  9876 10156 10205
 10225 10241 10251 10301 10313 10524 10800 10831 10884 11044 11196 11288
 11631 11734 11759 11797 11894 11949 11953 12000 12236 12377 12416 12684
 12692 12729 12764 12830 12940 12957 13026 13031 13138 13650 13658 13662
 13673 13735 13831 13992 14072 14084 14246 14292 14307 14341 14501 14598
 14607 14650 14754 14756 14760 14765 14790 14819 14828 15190 15280 15324
 15402 15434 15468 15749 15766 15827 15975 16072 16173 16192 16676 16722
 16957 17045 17091 17093 17112 17136 17314 17382 17401 17648 17739 17789
 17811 17854 17890 17940 18378 18382 18397 18404 18405 18419 18473 18487
 18598 18723 18834 18950 19064 19124 19244 19328 19330 19345 19386 19497
 19666 19714 19808 19934 19960 20016 20018 20226 20350 20460 20522 20569
 20628 20652 20709 20746 20792 20816 20853 20882 20959 20962 21130 21351
 21355 21445 21791 22200 22470 22554 22607 22643 22666 22675 22690 22766
 22986 23136 23195 23336 23406 23422 23424 23538 23546 23588 23824 23962
 24036 24059 24084 24210 24432 24504 24526 24587 24760 24798 24870 24990
 25159 25262 25466 25546 25657 25678 25683 25688 25799 25829 25863 25868
 25948 26061 26079 26358 26376 26471 26504 26515 26516 26613 26756 26760
 26842 26896 27026 27085 27121 27248 27251 27258 27292 27501 27502 27514
 27602 27859 27964 28162 28260 28279 28357 28567 28574 28684 28710 28898
 28954 28984 29041 29180 29311 29434 29588 29712 29731 29760 29803 29881
 29930 30041 30184 30250 30262 30356 30450 30544 30604 30751 30770 30833
 30856 30870 31024 31100 31185 31200 31275 31461 31474 31481 31517 31576
 31591 31596 31600 31672 31706 31738 31840 31850 31854 31895 31954 31962
 32110 32248 32323 32344 32394 32421 32455 32507 32510 32511 32573 32747
 32888 32892 32918 33088 33506 33768 34542 34660 34676 34740 34771 34841
 34861 35401 35450 35544 35574 35616 35622 35688 35710 35777 35873 35883
 35887 35916 36268 36408 36425 36620 36714 36730 36822 36836 36900 37038
 37044 37062 37076 37118 37135 37347 37396 37441 37450 37453 37468 37551
 37552 37574 37601 37606 37609 37672 37834 37952 37974 38052 38256 38359
 38374 38397 38399 38401 38577 38582 38700 39031 39074 39327 39332 39378
 39397 39424 39488 39734 40280 40386 40498 40510 40654 40728 40824 40874
 40948 40977 41109 41218 41226 41284 41301 41334 41366 41377 41396 41464
 41473 41538 41618 41843 41894 42042 42140 42359 42566 42897 43048 43098
 43198 43658 44046 44078 44232 44245 44262 44456 44484 44852 44936 45048
 45408 45502 45532 45606 45615 45654 45761 45888 45944 45968 46015 46033
 46088 46298 46466 46642 46887 46906 47217 47288 47291 47376 47387 47421
 47458 47486 47538 47689 47759 47826 47912 47928 48006 48020 48091 48245
 48344 48402 48472 48486 48524 48614 48638 48811 48856 48881 48887 48945
 48975 49026 49062 49082 49090 49109 49132 49198 49212 49285 49398 49416
 49488 49499 49543 49904]
* Critical examples for prunings = 580
* Critical examples for training = 1194
* PrAC images = 1458
******************************************
pruning state 7
* remain parameters = 56079.0
******************************************
* remain weight =  20.971324717285945 %
0.1
* training images number = 1458
Epoch: [0][0/12]	Loss 0.9913 (0.9913)	Accuracy 62.500 (62.500)	Time 0.05
train_accuracy 61.317
Test: [0/79]	Loss 0.7876 (0.7876)	Accuracy 71.875 (71.875)
Test: [50/79]	Loss 0.7748 (0.7199)	Accuracy 74.219 (73.100)
valid_accuracy 73.570
Test: [0/79]	Loss 0.7512 (0.7512)	Accuracy 71.094 (71.094)
Test: [50/79]	Loss 0.5093 (0.7284)	Accuracy 83.594 (72.442)
valid_accuracy 73.520
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/12]	Loss 0.8434 (0.8434)	Accuracy 64.844 (64.844)	Time 0.04
train_accuracy 60.631
Test: [0/79]	Loss 0.1606 (0.1606)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1935 (0.1876)	Accuracy 96.875 (96.063)
valid_accuracy 96.380
Test: [0/79]	Loss 0.1564 (0.1564)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0890 (0.1918)	Accuracy 98.438 (95.604)
valid_accuracy 96.280
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06490843743085861
0.010000000000000002
Epoch: [2][0/12]	Loss 0.8816 (0.8816)	Accuracy 59.375 (59.375)	Time 0.05
train_accuracy 68.038
Test: [0/79]	Loss 0.0714 (0.0714)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0952 (0.0878)	Accuracy 97.656 (98.315)
valid_accuracy 98.480
Test: [0/79]	Loss 0.0800 (0.0800)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0331 (0.0898)	Accuracy 99.219 (98.238)
valid_accuracy 98.530
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008523690514266491
0.0010000000000000002
Epoch: [3][0/12]	Loss 0.7956 (0.7956)	Accuracy 71.094 (71.094)	Time 0.04
train_accuracy 71.193
Test: [0/79]	Loss 0.0525 (0.0525)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0772 (0.0733)	Accuracy 98.438 (98.545)
valid_accuracy 98.570
Test: [0/79]	Loss 0.0677 (0.0677)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0232 (0.0761)	Accuracy 100.000 (98.514)
valid_accuracy 98.790
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.000927263346966356
0.0010000000000000002
Epoch: [4][0/12]	Loss 0.7093 (0.7093)	Accuracy 72.656 (72.656)	Time 0.05
train_accuracy 70.302
Test: [0/79]	Loss 0.0510 (0.0510)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.0748 (0.0705)	Accuracy 97.656 (98.621)
valid_accuracy 98.600
Test: [0/79]	Loss 0.0637 (0.0637)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0207 (0.0741)	Accuracy 100.000 (98.529)
valid_accuracy 98.800
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008915993967093527
* remain weight =  16.77698498175073 %
* best SA=98.8
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  16.77698498175073 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  825
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  20.971324717285945 %
Test: [0/391]	Loss 0.0553 (0.0553)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.0593 (0.0634)	Accuracy 98.438 (98.820)
Test: [100/391]	Loss 0.0606 (0.0656)	Accuracy 100.000 (98.817)
Test: [150/391]	Loss 0.0486 (0.0666)	Accuracy 98.438 (98.846)
Test: [200/391]	Loss 0.0622 (0.0663)	Accuracy 97.656 (98.850)
Test: [250/391]	Loss 0.0554 (0.0669)	Accuracy 99.219 (98.867)
Test: [300/391]	Loss 0.1023 (0.0675)	Accuracy 95.312 (98.835)
Test: [350/391]	Loss 0.0749 (0.0671)	Accuracy 97.656 (98.831)
valid_accuracy 98.828
[15486 22436  4460   588 14700 40874  7842 37552 37606 11378 48856 33752
 45154 30123 30356 34406 40571 37601 11306 40886 37962  8283 48881 11632
 48887  4434  4428 40386  8029 48638 19322 34328 25801 37347 25799 25683
  4476 37521 37574 25797 41113 30184 19040 25829 37734 48975 37589  7530
 11693 40752   482 29434 11631 22130 40466 37198 15434 25262   500 48270
 18834  4362 40527 44232 37680 15402 11044 22240 37124   132  8202 30262
   376 29350 22200 37050 44078 48538 29843 45048  7768 44262 48706  7784
 37672 37276   544   340   547 22562 37070 48524 48512 30041  7917 14994
 25868 44245 29880 25318  4024 37396  7907 37076 15826   212 37468 37469
 29226 15642 40498 29311  7920 22169 14790  7584  7885 44758 11298   506
 30250 48486 48288 19244   650 29803 19124 14819   635 40514 37834 48945
 37838 19328 37109  7732  8154 11242  4694 14754 15190 40976 40977 48242
  8532 29906 15468 12940 34740 19360 26740 47421  9489 27898  1920 46298
 17940  9471 23546  6524 46287 24504 35062 16862 31591 20816 24526 39832
 27968 32835 13658 13662  6578 31481 47486  5456 28014 12416 36141 41772
 42964 31594 31596 26748 47321 42772 35916 39031  2078 42793 47340  9583
 17005 39734 24432 13494 41907  9552 31672 47376 16963 31608  5538  3136
 36006  6466 46354  9379 13538 19960  9534 42854  6448 16956 16957  3118
 10301  1784 26636 13831 10524 28162 23336 36268 28175 39940 28180 31275
 46024 10544 46015  5332 38680 49464 43198 18130 49416 28231 47646  5314
 31216 21042 34870 38640 16574 34861 41618  3432 18166  3390 19808 28149
 16676  6636 43048  3268 26629 46154 38798  5422 26622  6658 28066 47538
 28072 28078 23406  9332 46122 31374 16678 20959  3998 16684 46088 41718
  9376 31345 18056 38760 23385 43098 12377 49501 32934 20018 39708 10297
 49913 32372 46857 17401 27118 32394 49890  2488 42429  5821 12834 32110
 12830 39311 27323 32104 23962 39296  5806  6102 27426 39284 17316 42482
 35652  2824 17648 42501 39260 35324 39307  6014 17420 24036 24121  5908
 24101 12957  2676 35450 10044 12967 46938 39424 27180 27248 17542 27265
 42334 39482  5972 13031 13026 20226 27299 32202 46887 20460 24052 39486
 32344 24059 39397 47012 46906 20322 27026 32002 23886 17763 10243 31854
 31850 10251 10253 31840  6304 12651 17112 20048 17789 13370 39683 17093
  2158 47288 46466 13428 17045 46476 17819  9627 17136 17059 27706 10282
 17073 27696 20628 47290 35883 28282 35234 20569 42112 46689 27503 49818
 12752 23868 32510 27522 32511 42566 27526 31958  2892 20110  2901 17213
 17209  5684 31884 12692  2250  2254  9727 20572  6236 42604  5718 20522
  5722 17714 31934 10215 16549 23361 44852 30856 22986  5084  8704   886
 26079 49212   902 41396 14246 38365 18378 45617 38359 47912 33296 10774
  8731 36653  3692 33304 36620 14501 49090 36866   846  1239 10746  5110
 49233 43510 33362 38399 38397 28898 11894 28559 36875 16298  8942 28567
 36606  8690  5103 30544 30870 10756  7374   832 45408 38345 16198 28688
 30751 18419 36760 18423  5013  8799 34614 47966 38256 49153  1058 16173
  1032 11960 21392  1047 33364 22884 36730 33330 33388  7259 14307  8752
 47928 41284 18398  3810 14428  8761 30646  5062 28654 41276  1121 22830
 38230 14292  5000 18405  8867 41301 24722 30770 48006 12000 14172 38401
 30925 41156  9098 12181 21150 41540  1404  1400 18598 38082 12183 38526
 23038 14607 26376 19666 22675 34771 28413 28984 14072  8613 28422 36425
 18609 45930 10600 29065 47689 34841 15870 24730 18190 26442 31100 33088
  6885  7498 19702 21124 14650 14008 22633 45868  1444 38577 14598 15924
 36936 23089 43850  4909 26034  1291 28954 30983 36522 43852 30482  7005
 45266 15960 28471  7009 30962 14124  1330 30993 33506 18322  5174 14580
 23040 15948 22690 34758  9046 28512 48614 17772 15280 42648  7264 37551
 49904   854 38050 15991 13082 29712  4446 32276 11759 25466 43826 32280
 10894 15975 37557 39423 47217 42359 13198 22554 32348  8729 17645 19330
 37453 27502 32455  8730  8617  7452 40654 39289 37974 16072 20171 11786
 37588 27254 32248 21445 29731 32323 11734 19502 35616 26061  7606 25863
 49573 39801 48402 12153 31335 42897 25657  5536 20784 21164 14896 31622
 26766 38439 45606  2014 36446 20962  6755 14222 34676 28525 24560 47828
  9396 24589 36126 44850 29238 38403 16836  4542  6540 34234 46288   266
 34678  6974 12184 14866  8883 38672 20016 18776 25294  3718 14804 45968
 31736 43234 14308 47291 40510 33361 26896  5656  4692 13970 41366  8347
 17001 29206  6418 40948  2052 21598 30049 34750 49285 26471  9344  5912
 44870 49960  7010 22062  9009 43454 22083 38700 22607 16560 45917 26516
 25306 48966 34048 32342 23194 15324 10994 43109 27292 24798  4150 14078
  1604 26560 28260 41538  7758 48811  8480 17382  8226 38339 24990 14760
 39752 28652 33412 44098   584  5602 17739 37966  7270 36686 28672 32573
 35246 26940 15450  3030 10800 21382 39074 17079 41334 25412 43658  7146
 30604 16959 47160 37491 24198  8670 21900  4028 40144 25546  4030  7080
 17958 25678 13138 45352 41218  8701  1940 38370 34660 16030 22779 26756
 35310 33448 14468 23588 45615 44456 29930 31576  1512]
[   80   132   180   212   517   547   558   588   966  1047  1049  1058
  1304  1330  1356  1404  1612  1784  1824  1920  1978  2148  2158  2410
  2488  2576  2622  2676  2707  2824  2892  2931  2958  3136  3390  3456
  3524  3772  3811  4024  4028  4039  4129  4150  4153  4362  4446  4460
  4529  4542  4692  4908  5013  5035  5174  5216  5430  5538  5656  5684
  5704  5718  5738  5790  5868  5896  5912  5952  6130  6236  6428  6448
  6466  6578  6636  6658  6974  7005  7058  7112  7270  7293  7347  7374
  7498  7530  7606  7638  7768  7784  7879  7885  7898  7907  7920  8072
  8104  8154  8200  8202  8470  8552  8680  8729  8731  8799  8867  8883
  8945  8966  9302  9330  9396  9471  9814  9876 10146 10207 10251 10253
 10287 10297 10321 10524 10746 10831 10884 10894 10944 11288 11298 11438
 11631 11647 11734 11797 11949 11992 12000 12153 12302 12325 12377 12651
 12692 12729 12730 12764 12940 13031 13138 13198 13370 13538 13658 13662
 13673 13735 13831 14072 14501 14540 14607 14650 14700 14754 14756 14760
 14765 14790 14796 14886 14894 14896 15190 15324 15434 15450 15642 15660
 15749 15766 15827 15924 15948 16116 16173 16192 16676 16698 16722 16981
 17073 17112 17213 17216 17234 17244 17314 17316 17440 17592 17602 17608
 17811 17890 18056 18130 18356 18378 18382 18397 18398 18416 18557 18598
 18834 18930 19124 19306 19330 19345 19386 19494 19714 19866 19934 19960
 20018 20029 20048 20110 20196 20226 20287 20350 20534 20550 20628 20709
 20720 20882 20962 21058 21130 21302 21351 21355 21382 21445 21952 22607
 22643 22675 22690 22986 23038 23136 23336 23385 23422 23458 23588 23629
 23824 23886 23962 24059 24101 24166 24210 24432 24526 24587 24649 24722
 24760 24798 24803 24820 24870 24887 24990 25018 25159 25262 25294 25321
 25457 25546 25678 25683 25688 25799 25801 25829 25948 26079 26358 26376
 26392 26471 26504 26515 26613 26760 26842 26978 27026 27085 27121 27248
 27251 27252 27265 27292 27431 27502 27503 27706 27859 27898 27964 27968
 28014 28066 28260 28357 28407 28525 28530 28534 28574 28710 28786 28801
 28898 28954 29065 29180 29286 29434 29457 29659 29731 29803 29881 30041
 30356 30450 30751 30811 30833 30856 30903 31100 31185 31200 31275 31335
 31461 31474 31576 31591 31596 31600 31622 31650 31706 31738 31840 31895
 31934 31946 31962 32110 32152 32202 32276 32280 32297 32323 32344 32394
 32421 32510 32511 32571 32747 32870 32918 32934 33038 33120 33288 33768
 34422 34542 34614 34660 34740 34771 34841 34964 35051 35062 35068 35153
 35234 35401 35574 35622 35688 35710 35882 35916 36141 36172 36446 36620
 36714 36768 36875 36934 37038 37109 37118 37135 37160 37275 37396 37453
 37468 37507 37521 37551 37574 37589 37609 37672 37834 37962 37966 38050
 38256 38374 38397 38399 38401 38408 38409 38433 38459 38577 38680 39031
 39074 39198 39256 39327 39342 39377 39378 39397 39425 39600 39625 39734
 39752 39832 39875 40280 40386 40498 40654 40824 41218 41284 41290 41305
 41366 41377 41438 41464 41538 41540 41593 41618 41693 41808 41843 42049
 42112 42140 42234 42312 42429 42447 42566 42604 42660 42772 42832 42866
 42964 43014 43040 43109 43198 43212 43416 43439 43510 43592 43826 43852
 43874 44046 44262 44340 44424 44484 44642 44845 45408 45532 45606 45616
 45654 45761 45868 45888 45919 45944 45968 46076 46248 46298 46316 46331
 46365 46394 46466 46476 46524 46581 46770 46828 46887 46906 47217 47288
 47342 47376 47387 47486 47538 47689 47912 47926 48006 48091 48107 48245
 48402 48469 48472 48512 48527 48706 48811 48856 48887 48945 48975 49026
 49062 49082 49164 49212 49264 49416 49464 49488 49524 49525 49543 49904]
* Critical examples for prunings = 588
* Critical examples for training = 825
* PrAC images = 1135
******************************************
pruning state 8
* remain parameters = 44863.0
******************************************
* remain weight =  16.77698498175073 %
0.1
* training images number = 1135
Epoch: [0][0/9]	Loss 1.3161 (1.3161)	Accuracy 45.312 (45.312)	Time 0.08
train_accuracy 54.714
Test: [0/79]	Loss 1.8357 (1.8357)	Accuracy 32.812 (32.812)
Test: [50/79]	Loss 1.8102 (1.6965)	Accuracy 31.250 (39.461)
valid_accuracy 39.880
Test: [0/79]	Loss 1.6782 (1.6782)	Accuracy 38.281 (38.281)
Test: [50/79]	Loss 1.5681 (1.6899)	Accuracy 50.000 (38.480)
valid_accuracy 39.060
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/9]	Loss 0.9566 (0.9566)	Accuracy 58.594 (58.594)	Time 0.11
train_accuracy 58.062
Test: [0/79]	Loss 1.1571 (1.1571)	Accuracy 49.219 (49.219)
Test: [50/79]	Loss 1.1962 (1.1208)	Accuracy 39.844 (48.422)
valid_accuracy 48.600
Test: [0/79]	Loss 1.0397 (1.0397)	Accuracy 52.344 (52.344)
Test: [50/79]	Loss 1.0243 (1.1301)	Accuracy 55.469 (46.936)
valid_accuracy 47.430
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06036154553294182
0.010000000000000002
Epoch: [2][0/9]	Loss 0.8604 (0.8604)	Accuracy 56.250 (56.250)	Time 0.05
train_accuracy 57.445
Test: [0/79]	Loss 0.2688 (0.2688)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.2767 (0.2487)	Accuracy 93.750 (95.251)
valid_accuracy 95.610
Test: [0/79]	Loss 0.2246 (0.2246)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1516 (0.2585)	Accuracy 98.438 (95.021)
valid_accuracy 95.350
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008470231667160988
0.0010000000000000002
Epoch: [3][0/9]	Loss 0.9189 (0.9189)	Accuracy 60.156 (60.156)	Time 0.05
train_accuracy 62.996
Test: [0/79]	Loss 0.1445 (0.1445)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1576 (0.1408)	Accuracy 97.656 (97.748)
valid_accuracy 97.900
Test: [0/79]	Loss 0.1211 (0.1211)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.0667 (0.1497)	Accuracy 100.000 (97.258)
valid_accuracy 97.510
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0010253438958898187
* remain weight =  13.421438401244545 %
* best SA=97.51
* find early bird tickets at epoch 4
Remove Pruning
Pruning with custom mask
* remain weight =  13.421438401244545 %
* record size = (60000, 4)
zero all unforgettable images out, rest number =  658
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  16.77698498175073 %
Test: [0/391]	Loss 0.1309 (0.1309)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.1392 (0.1374)	Accuracy 96.875 (98.039)
Test: [100/391]	Loss 0.1542 (0.1394)	Accuracy 96.094 (98.066)
Test: [150/391]	Loss 0.1208 (0.1406)	Accuracy 97.656 (97.977)
Test: [200/391]	Loss 0.1222 (0.1386)	Accuracy 97.656 (98.014)
Test: [250/391]	Loss 0.1439 (0.1404)	Accuracy 96.094 (97.952)
Test: [300/391]	Loss 0.1783 (0.1408)	Accuracy 94.531 (97.950)
Test: [350/391]	Loss 0.1255 (0.1407)	Accuracy 96.094 (97.961)
valid_accuracy 97.950
[27085 30184 47342 16957 40571 20018   376 39752 48881 44262 10146 21900
 37834 21952 39074 31736 37198 16862 31608  9376  2901 43048  3998 42049
 46354 47291 31650 45048 44340 47376   340 46466 28014 20016 24526 17763
 17772 16836 32571 13538 39031  4028 47288 42112 31591 35153  2824 25466
 20784 35882 35883 36006  4542 41113 39708 30123  6304 47321 46394  5536
 42140  7584 47387 18776 26358 42866 44758 32297 24059  7842 39327 37601
 48706 27323 40886 32276 37453 42566 37588 24101 37468  5972  5738 39311
 29881 20226  6014 22083 35652  5718 40752 17209 27503  5722 11242 39296
 32002  8029 27292 35622 39307 19040 32248 47012 18930  5821 22169 32202
  7917 48638  7920 25678 42482 44642 20350 46938 27426 39424 39425 42447
 12967 11306  2254 25683 11298  5908 39482 32104 17314 48614 32110 39378
  7907  7885 24121 37491 13026  5806 39397 42501 24210 17316  9534  9727
  5704 49890 32455 42793 23886 49904 42234 14994 42772 31840 17073 27180
 39625 17079  9627 31854 48469  2078  9583  5602  4476 32511 39683 23824
 42854 35234 17001 15434 27118 17005 20110  4460 25863 47217 19124   266
 30041 25829 32421  4428   180  5684 37672 20522 11438 31934 35688 49960
 27248  8072  7784 29930 42312 48538 12830 31958 27252 27265  7768 20196
  2148 20569 20171 18834   212 35310 27706 44484 31884 20550 10044 17136
 35710 25801 37680  4150 48512  4129  6448  5430  1356 28471 38365 38577
 10524 43874 26629  8480 15827 15826 49464  7347 21164  6755 45930 45944
 26622 39801  5103 18130 49090 10800 38082 24870  8799 22884 26442 10894
 14008  1330 49082 11797 45888  3268 41284  1400 31185 41540   635 38403
 46015 19330 25262 31200 19328 44098 31216 38680  6658 14246 46024 21382
 44078 47646 14607  1404 49488 45968 41276 28422 38359 28413  7005 45266
 16173 33330 49501  7009 30811 28525 21392 36446 14428 33388  3692 18419
 22830 38439 43592 45408 41366 48091 15924 36866 18416 26504  5035 30925
 47828 41396 25159 49164 38408 15948 34676 33362 49153  8867 30903 41377
 28654 23089   832 10831  3432 33361  8883 41618 36522 40386 30993 28567
 26560 41334 38374 16198 28559   854 43510 12183 12184 49416 18405  8617
 30983  8613 23194 14501  7259 36875 26516  7264 34660 16298 15975 28710
  3390 33088 10600  5062 28954 12153 38526 18382 38399 13735 38798 40510
 14307 49026 20962  6540 45615 37124 48270 43826 31345 45616 20882 36760
 19494 39832 41907   544 29311 49212 31335 11693 47538 10297 35062 28898
 22633 35051 18322 45154 43198 38256  8730 30250 28162 36126 16722 47486
 30262 13662 14292   506 23546 39875 48006 26034 36141 46248 33288  6466
 16678 15660 28801   482 26842 12000 31374 40514 45606  8347 13673 19866
 22986 37962 36653 24722 14804 49573 48242  1047 26766 29206 31275 24730
 22554  1784 46316 16574  5422 25294 26756 21042 43850 41438 43852 38700
  1512   886  6636 26740 28282  8761  9344 12377 40498 15642 46287 30770
 36768  9332 24589  7530  6428 17940 33304 29226  1032  3118 32747   902
 32835 39940 47421 10287 46298  6578 37050 24587 34614 28260   966 14172
 49233 33364 11378 26896 17213 36268 37966 27502 46476 29803 37557 31100
 15190 17592 47928 34841 29843  4039 33448 48486 38345 29286   846 23385
  8942  2052 32372  8701  7010  5912 36714 38397 49285  1058 41538 38339
 36606 43658 36686 39342 49264 22200 38370  8670 46857 30604 22130 24198
  7146 40874  7080 16030 37469 11992 29712 44456 35616 29238  4030 20720
   558 47290 34328  7638 14700 22643  8200 25306 46122 17958  3136 16959
 41218   584 40654 26748 13831 16560 30356 30049  1940 25318 14896 18609
 26940 17819 28078 10251 20816 48945  9396 43109 23588 25412  3030 23406
  7498 48966 20029 17739 16676 48975 22607 10994  5314  8226 20048 32573
 40527 19808  9098 12302 34758 45352 34750 34740 14072 40948 14078 33412
 22062  7270 18190 23962 32348 32342  2622 42334   132 34048 28652 24036
 13138 28672 38409 26471 40144 38401 25546 22779  8154 35246 33506 23361
  1444 32934 23868 24798   650 21124 18056 15402 12752  7732 44870 47160
 45917 43454  5656 17112 36425  5110 45868  7758 47689 17382]
[   54   132   140 ... 49758 49844 49960]
* Critical examples for prunings = 1046
* Critical examples for training = 658
* PrAC images = 1393
******************************************
pruning state 9
* remain parameters = 35890.0
******************************************
* remain weight =  13.421438401244545 %
0.1
* training images number = 1393
Epoch: [0][0/11]	Loss 1.2554 (1.2554)	Accuracy 55.469 (55.469)	Time 0.08
train_accuracy 57.574
Test: [0/79]	Loss 1.4497 (1.4497)	Accuracy 41.406 (41.406)
Test: [50/79]	Loss 1.4412 (1.4044)	Accuracy 39.844 (42.065)
valid_accuracy 42.140
Test: [0/79]	Loss 1.3533 (1.3533)	Accuracy 38.281 (38.281)
Test: [50/79]	Loss 1.2756 (1.3997)	Accuracy 56.250 (42.816)
valid_accuracy 42.500
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/11]	Loss 1.0488 (1.0488)	Accuracy 61.719 (61.719)	Time 0.05
train_accuracy 60.302
Test: [0/79]	Loss 0.4437 (0.4437)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.4182 (0.4109)	Accuracy 93.750 (94.317)
valid_accuracy 94.660
Test: [0/79]	Loss 0.4142 (0.4142)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.3243 (0.4299)	Accuracy 92.969 (93.444)
valid_accuracy 94.170
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07400389760732651
0.010000000000000002
Epoch: [2][0/11]	Loss 0.8129 (0.8129)	Accuracy 70.312 (70.312)	Time 0.05
train_accuracy 66.906
Test: [0/79]	Loss 0.1598 (0.1598)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1779 (0.1669)	Accuracy 96.094 (97.656)
valid_accuracy 97.780
Test: [0/79]	Loss 0.1846 (0.1846)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1089 (0.1821)	Accuracy 98.438 (97.212)
valid_accuracy 97.650
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008414600044488907
0.0010000000000000002
Epoch: [3][0/11]	Loss 0.8618 (0.8618)	Accuracy 65.625 (65.625)	Time 0.10
train_accuracy 69.777
Test: [0/79]	Loss 0.1178 (0.1178)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1393 (0.1317)	Accuracy 96.094 (97.580)
valid_accuracy 97.730
Test: [0/79]	Loss 0.1521 (0.1521)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0733 (0.1443)	Accuracy 98.438 (97.381)
valid_accuracy 97.740
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.001114516519010067
0.0010000000000000002
Epoch: [4][0/11]	Loss 0.7328 (0.7328)	Accuracy 78.125 (78.125)	Time 0.05
train_accuracy 70.280
Test: [0/79]	Loss 0.1072 (0.1072)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1294 (0.1238)	Accuracy 95.312 (97.518)
valid_accuracy 97.650
Test: [0/79]	Loss 0.1445 (0.1445)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.0639 (0.1350)	Accuracy 98.438 (97.381)
valid_accuracy 97.700
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.001058790716342628
* remain weight =  10.737150720995636 %
* best SA=97.65
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  10.737150720995636 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  696
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  13.421438401244545 %
Test: [0/391]	Loss 0.1545 (0.1545)	Accuracy 97.656 (97.656)
Test: [50/391]	Loss 0.1606 (0.1690)	Accuracy 96.875 (97.672)
Test: [100/391]	Loss 0.1950 (0.1722)	Accuracy 96.875 (97.672)
Test: [150/391]	Loss 0.1432 (0.1747)	Accuracy 99.219 (97.651)
Test: [200/391]	Loss 0.1402 (0.1731)	Accuracy 99.219 (97.757)
Test: [250/391]	Loss 0.1777 (0.1745)	Accuracy 98.438 (97.753)
Test: [300/391]	Loss 0.1876 (0.1747)	Accuracy 97.656 (97.734)
Test: [350/391]	Loss 0.1721 (0.1744)	Accuracy 96.875 (97.748)
valid_accuracy 97.704
[37680 17940   482 11631 30142 19124 40886  7833  1404 30184 39940 26749
 48469 26748  9104 21124 48472   140 17426 44340  6428 25678 25688   506
 31596  8202 46015 26629 49501 14819 26740 23406 29573 17382 24036 19120
 20976 22470 41912 43198  8200 29930  1400 32934 25262  8154 10044  7920
 37124 32394 47597 39810 31608 39832 44133 39355 32002 27964 42359 18419
 25863 21130 40824 23336 25854 22278  9433 12808 31335 12184 38526 46857
 18190 27859 37256 13538 24494  5821 47036 26504 48811 24730  1920 32065
 46316  6466  5798 29880 18776 11576 45917 30062 28162 27426 25318 32104
 23588 41618 16574 28260 31185 41843 40571 14972 32348   180 26522 24722
 44870 45944 43254 28279 42447 26516  4150 39307 34841 48397 26613 49904
 21042 28282 32344 40948  3118 44262 32342  2676 48356  9147 17314 25294
  3136  9471 22436 11196 27118 26560 36268 44424  2410 23962 40514 25829
  6448 23546 15642 32918  7907  5314 32323 46269 17592 13138 40654 18056
 28357  9495 29311 20784  7719 19808 40977 20226 12834  7885 26766 47689
 29434 47421 45888 20962 49488 26622 12830 48706 32888 38577 15434 19934
 20816  1940 12302 49890 16678 16957 48270 10277   832 34614 41396 43852
 30770 30764 23458  6236 47288 49062 47912 18356  2901  5602  8072 10894
 36653 39423 38798 21351 22128  8680 20569 49082 30751 15987  7347 47217
 36126  5035 17120 22130 28944 29672 37551 15948  5065 43574 30811 45615
 36606 49026  5684  3634  2078 10287 49226 36599 30833 21164 26034 43898
 30450 38339 20882 24587 46466 42772 10211 43592 45593 49212 26061 37557
 24589 39031 25508 23824 10756  5430  9344  5656 20016 33362  7259 33364
 21445 16072  7264 22838 27706 34542 46524  7270 36714 27251 29711 32248
 36104 49153 11953  1784 47486 27248 39074 12651  2148 24990 30659 18416
 13370 27700 29705  5422  8785 40240 22824 42049 30604 10243 41284  1058
  9376 42042 41276 21368 17093 42312 43804 47940  5908 40752 33288  5026
 48594 43048  8029 23730 47966 38256 30692 45408 25546 17763  8729 21382
  9627 26138  6578 33304  9379 30574 20048 22884 22675 44382 37468 44758
 46394 42140 39734 22554 28471 48933   588 46906 30262 20709 43098 14756
 37609 37606 38452  6979 13662 38359 41113  7530 37974 37028 38439 48945
 46682 46122 39397 28491 37962 49285 44046  5738 14760 31946  2342  9534
 23140 13185  5174 13658 41540 36446 34740 40066 14804  1988 25462 46368
   517 42866 41062 14790 32421 32280  8470 11693 18930  1341  4030 23126
  5868 26392 25466 27503 11044   544 12153 19064  4039 42854  7005 17645
  1269  2062 17001 34660 19040  7058  4362 39875  9583 44642 28561 14650
 37450  5103 16298 37453 20110 45654  4460 35882 49233 41438 20522 33506
 13673 20663 38365 21290 38050 17005 48649 13735 20287 35883 40386 18598
 43510 30903 37588 48966 19330 31934 19328  9332 14692  9396 35688 28525
 11734 11288  6636 36522 17209 38408 15190 34048 34676 18609 32455 38403
 38397 24870 48975 16722 36982 38399 37552 29713 37601 23384 49543  7917
 49573 29803 11298 39482 16676 12967 11378 17608 29238  5704 26896 22779
 40008 23594 42793 27502 48881 40280  7638 27121 43874 11777 12000 40510
 17112 24479 14072 25861 20710 30983 46476 41907 12196 14230  6347 18166
  7584  4692 22643 33552  2052 18212   650 36730 38680 42234 26756 15500
 47828  6755 32747  3390 39801  5062 46828  2158  5536 20196 17079 20029
  1978 45616 33388 39311  8940 18405 22062  9727  4542 37574 37469 12752
 23868 22169 28078 10297 47160 42334 40144 39708 36141 38370 38052 11797
  7080 18322 35246 45266 47290 35234 20018 36686 16030 43658 41366  1047
 17739   886  8731 16130 19502  3756  8772 41334 32573  2622 26940 17958
 11992 33412 48638 41218  8883 47291 28652 28654  5110 22083 10215 12692
 28672  8670  7146 43826 45352 49090 33448  7498 37038 22607  2426 25799
 31576 34802 25306  4129 20171 14896   376 26471 31591 45868  5216  3456
 35616 23194  3268 15660  1444 47376  7732 34829 38700 25412  8226 47600
  7842  1604   266 32112 35062 31216 16862 24198 16560 48402 47646 36006
  7758 33768 21900 10994  9098 39256 26376 26842 16959 31958 15450 43454
 47340 17819 37427 27292 14700  3030  4476 17213 23886 49264  2576 47321
  7010  6658   584  3532 12183 33088 47094 34758 24798 43109 34750 12377
  8480 35310 41538 44456 44098 22200  4028 23911 44078 34328 18220 10251]
[   28    30    80 ... 49667 49904 49910]
* Critical examples for prunings = 1083
* Critical examples for training = 696
* PrAC images = 1468
******************************************
pruning state 10
* remain parameters = 28712.0
******************************************
* remain weight =  10.737150720995636 %
0.1
* training images number = 1468
Epoch: [0][0/12]	Loss 1.3128 (1.3128)	Accuracy 54.688 (54.688)	Time 0.06
train_accuracy 52.997
Test: [0/79]	Loss 2.3415 (2.3415)	Accuracy 19.531 (19.531)
Test: [50/79]	Loss 2.4268 (2.2189)	Accuracy 16.406 (20.803)
valid_accuracy 21.450
Test: [0/79]	Loss 2.2800 (2.2800)	Accuracy 16.406 (16.406)
Test: [50/79]	Loss 2.0452 (2.2105)	Accuracy 24.219 (20.864)
valid_accuracy 21.200
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/12]	Loss 0.9283 (0.9283)	Accuracy 64.062 (64.062)	Time 0.06
train_accuracy 61.989
Test: [0/79]	Loss 0.5600 (0.5600)	Accuracy 84.375 (84.375)
Test: [50/79]	Loss 0.6297 (0.5960)	Accuracy 78.906 (83.088)
valid_accuracy 83.790
Test: [0/79]	Loss 0.5861 (0.5861)	Accuracy 85.156 (85.156)
Test: [50/79]	Loss 0.4288 (0.6373)	Accuracy 95.312 (80.316)
valid_accuracy 82.240
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07662301510572433
0.010000000000000002
Epoch: [2][0/12]	Loss 1.0116 (1.0116)	Accuracy 67.188 (67.188)	Time 0.05
train_accuracy 65.123
Test: [0/79]	Loss 0.2514 (0.2514)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2284 (0.2373)	Accuracy 95.312 (94.914)
valid_accuracy 95.320
Test: [0/79]	Loss 0.2430 (0.2430)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.1183 (0.2564)	Accuracy 98.438 (93.827)
valid_accuracy 94.940
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.010030649602413177
0.0010000000000000002
Epoch: [3][0/12]	Loss 0.7649 (0.7649)	Accuracy 69.531 (69.531)	Time 0.05
train_accuracy 68.733
Test: [0/79]	Loss 0.1800 (0.1800)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1492 (0.1769)	Accuracy 96.875 (95.925)
valid_accuracy 96.260
Test: [0/79]	Loss 0.1908 (0.1908)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.0809 (0.1948)	Accuracy 98.438 (95.006)
valid_accuracy 96.040
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0012538312003016472
0.0010000000000000002
Epoch: [4][0/12]	Loss 0.8363 (0.8363)	Accuracy 65.625 (65.625)	Time 0.12
train_accuracy 68.597
Test: [0/79]	Loss 0.1559 (0.1559)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1301 (0.1587)	Accuracy 97.656 (96.216)
valid_accuracy 96.520
Test: [0/79]	Loss 0.1749 (0.1749)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.0680 (0.1756)	Accuracy 98.438 (95.312)
valid_accuracy 96.330
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0013931457651779056
* remain weight =  8.58987016095255 %
* best SA=96.33
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  8.58987016095255 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  781
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  10.737150720995636 %
Test: [0/391]	Loss 0.1180 (0.1180)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1533 (0.1489)	Accuracy 95.312 (96.798)
Test: [100/391]	Loss 0.1381 (0.1512)	Accuracy 98.438 (96.829)
Test: [150/391]	Loss 0.0993 (0.1541)	Accuracy 98.438 (96.668)
Test: [200/391]	Loss 0.1284 (0.1542)	Accuracy 96.875 (96.638)
Test: [250/391]	Loss 0.1703 (0.1577)	Accuracy 96.875 (96.501)
Test: [300/391]	Loss 0.1645 (0.1587)	Accuracy 96.875 (96.514)
Test: [350/391]	Loss 0.1613 (0.1576)	Accuracy 95.312 (96.583)
valid_accuracy 96.576
[47646 24684 12302 30142 26516 34847 44133 19714 25306 41618 29750 22130
 34841 18130 22436 39940 10894 19782  3184 11298 48649  4428 49573 47432
 14819 44078 37117 47421 35062 25508 39779  4438 45026 37109 48638 14804
 15280  5536 26740 29180 25466 26748  8470 10382  8072 34048  4460 39810
 45005 30184 29713 40589 40824 18056 11234  4562 29238 32888 19810 12377
 49488 24589 48538 39875 26613 29248  5422 37557  5430 26629  8104 40773
 25412 19064 37552    28 49524 47486 40752 49543   506 49501  4542  4712
 33038  3772  5035 19345 48975  3756  7920 47962 18416   180 15016 48966
  4024 18405 47940 11992  4028  4030 41366 25868 48242 18378 25150 33288
 33572 33290 48945 33622 12000 22217 40386 33615  8200 37373 18609  7842
 37680 24990 29930 26061 15500 49090  7867 44758 49083 48091 26034  7885
 18514 15486 48065 25863 49026 48038 11949 41290 29434 37672 41276 48006
 34542 22240  5013  8226 40338  5000 49121 22278   266 22203 25861 22200
 37453 33088 12196 26392 48837 44624 15642 37601 12184 12181 18776  3524
 26362  8154 37450 40948  4692 34802 29286 33812 37468 37588 34234 47689
 41540    80 40514  4221 40008 18190 24730 25262   418 14896 26358 12153
  5155 48881 44424  3638 34676 49233 47828 37256 33242  7954 10756 25854
  5103 11693 10750 37606 41438 18732 15190 22169 37609   376 40066 25232
 29573 41473 40977  4153 19244 19612  4150 44870 49285  5026 42042 47376
 46368 42799 30659 46377  9569 31652 22838 20663  2044 17005  2062 22824
   966  6387  8785 20641 35883 31738  8772 46476 17079 36730  1988 16957
 21382 16862  1058 36686 38884  1888 46269 42897 23546 31546 36006 23710
  1920 42866 28710 42854 23588 21368  6466 20709 31606 38256 31608 22884
 30604  9661  2148  2342 35688 46682 27472  9814 42482 32002   854 27426
  2410  6172 36866 28898 36870 42415   832  8680 15991 15987 39311 32104
 17382 22719 27930 14501 16030 39074  2158 13370 21445 17112 46524 17120
 17121 17129 23770 31958 31840 43742 20522  8731 17209 27522 45352  6197
 31934 31946 27502 30574 20784 31474  9433 38403 23194 31100 38397 30870
 43254 38390  8966  6848 28282 16488  1492 16560 45919 16298 28260 16574
 28574 28577 43198 31197 45617 43234  6798 28357  1404 28471  7005 28491
  1269 36522 30983 21164  6974 23106 45761 38408 43368 28407 23126 36446
 30903 21130 14172 21128 38526 45815 28525 14084 43823 45615 14230 28072
  8898 31366 30770 46118 30764 36651  6636 36653 16766  6670  9379 13703
  1784 23458 36104 28672 13673  6578 13662  8867 27963  8883 31200 28078
  7146 31216 38365  8940 36268 45606 36599  7112  6755 38359  1604 23384
 36606 46015 31275  1143  1634 13831 23336 16678 43098  1662 31335 20962
 43826 20534 22083   650 47008 35401 22633  2694 17706 32508 32510 43898
 22559 32511 38050 10215 22643 14700 39461 27180 22554 17728 15924 12692
 36936 42234 10243 10251  5684 10044 17763  2958 20029 37038 12752 20110
 39482 17608 39508 12834 46828 17592 42140 32394 17591  5821 29048 12808
 14653 47094 45154 12797 27118 29026 32455  8552  5868 49910  5790 22622
 12651 30332 17648 32344 17645 27121 32348 32349 49904 20016 32276 42255
 27248 17819 27251 10277  5624 26842 24059 22675  2576 41912  7606 39708
 41907  5602   788 12576 39734 39378 26785  2554 39363 27292 42334 17426
 17420 20322 26766 24006 30474 39397 14764 15948 20171 47288 10287 39423
 14760 10282 30262 14754 35480 41962 24360 10297 47260  5936 27243 39425
  5656 12522 32427 39801 11734  4476 44046 31461 16400 19934 19330 37966
 14756 30925 26504 40510 13658  4362 19124 15827 47538 29803 46088 11631
 14765 48356 12744  9147   588  9104 21290   132 23406  2901  9098 28162
  5718 17940 29711 28175 47597 39546 45888 45532  2488 18404 34520 26138
 17093 49153  6236  6347 38052 27700 27490 44262  8729 35882 20287 39031
  7259 39480 17001 34614 33328 12967 27722 27157 33448 17213 18423 20210
  5062 21392  7719 46887 32110 18322 20569 34660 43804 46316  5908 42312
 47036 49212 20710 33364  7833 17326 18598 14650 31600 42447 22679 14790
 21900 23824 37469 11196 20018 45917 43874  7080 45944 19502 17958 37574
 25678 25318 38370 24526  1047 47966  3030 43852 40654 14072 19040 40144
 13185   544 49264  8480 23962  8670  7010 26756 26749  2622 25546 46906
 49164 45868 34750  1444 48706 27602 41464 47290   584  7758 22062 24798
 21124 43454  7638 32573 17739 34758 32342 47600 43658 36126 25159 38798
 33506  7530  2014 28652 32934 35310 26560   140  6428 46122 11044 42793
 25975 34829 26471 12836 15450 22607 19328 36982 27085 43592 12830 31576
  1940 41113 31591 41062  7498 36714  3390 48288 34328 15402  2676 18220
  3268 12183 35246 23730 10994 33412 26940 35234 26376 20048 47759  5216
 41284 22779 44456  7732 27706 16676  6658 37427  7264 25799 37347 32323
  7270 11632 41218 47160  9344  8428 41538 19808 46078 38339 43109 45616
 24198]
[   42   126   132 ... 49928 49933 49960]
* Critical examples for prunings = 1664
* Critical examples for training = 781
* PrAC images = 2101
******************************************
pruning state 11
* remain parameters = 22970.0
******************************************
* remain weight =  8.58987016095255 %
0.1
* training images number = 2101
Epoch: [0][0/17]	Loss 1.8943 (1.8943)	Accuracy 37.500 (37.500)	Time 0.06
train_accuracy 58.115
Test: [0/79]	Loss 1.7980 (1.7980)	Accuracy 25.781 (25.781)
Test: [50/79]	Loss 1.7933 (1.6883)	Accuracy 26.562 (33.487)
valid_accuracy 34.120
Test: [0/79]	Loss 1.5879 (1.5879)	Accuracy 30.469 (30.469)
Test: [50/79]	Loss 1.6348 (1.6719)	Accuracy 39.062 (33.961)
valid_accuracy 33.700
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/17]	Loss 0.8625 (0.8625)	Accuracy 71.875 (71.875)	Time 0.05
train_accuracy 69.205
Test: [0/79]	Loss 0.5825 (0.5825)	Accuracy 83.594 (83.594)
Test: [50/79]	Loss 0.6095 (0.5809)	Accuracy 85.156 (85.034)
valid_accuracy 85.650
Test: [0/79]	Loss 0.4789 (0.4789)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.4236 (0.5760)	Accuracy 93.750 (85.463)
valid_accuracy 84.880
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07522855699062347
0.1
Epoch: [2][0/17]	Loss 0.8069 (0.8069)	Accuracy 67.188 (67.188)	Time 0.08
train_accuracy 68.729
Test: [0/79]	Loss 0.2107 (0.2107)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1861 (0.2214)	Accuracy 97.656 (95.619)
valid_accuracy 96.030
Test: [0/79]	Loss 0.2508 (0.2508)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.1175 (0.2302)	Accuracy 98.438 (95.512)
valid_accuracy 95.770
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06451893597841263
0.010000000000000002
Epoch: [3][0/17]	Loss 0.5906 (0.5906)	Accuracy 77.344 (77.344)	Time 0.04
train_accuracy 76.297
Test: [0/79]	Loss 0.1143 (0.1143)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1515 (0.1294)	Accuracy 94.531 (97.595)
valid_accuracy 97.820
Test: [0/79]	Loss 0.1479 (0.1479)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0555 (0.1396)	Accuracy 100.000 (97.335)
valid_accuracy 97.670
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008358728140592575
0.010000000000000002
Epoch: [4][0/17]	Loss 0.7365 (0.7365)	Accuracy 72.656 (72.656)	Time 0.05
train_accuracy 76.678
Test: [0/79]	Loss 0.1303 (0.1303)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1447 (0.1242)	Accuracy 96.094 (98.009)
valid_accuracy 98.110
Test: [0/79]	Loss 0.1645 (0.1645)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0583 (0.1333)	Accuracy 100.000 (97.595)
valid_accuracy 97.880
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00783630833029747
0.0010000000000000002
Epoch: [5][0/17]	Loss 0.5804 (0.5804)	Accuracy 77.344 (77.344)	Time 0.04
train_accuracy 77.059
Test: [0/79]	Loss 0.1193 (0.1193)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1352 (0.1172)	Accuracy 96.094 (97.993)
valid_accuracy 98.130
Test: [0/79]	Loss 0.1558 (0.1558)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0528 (0.1261)	Accuracy 100.000 (97.718)
valid_accuracy 97.980
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006965607171878219
0.0010000000000000002
Epoch: [6][0/17]	Loss 0.4977 (0.4977)	Accuracy 80.469 (80.469)	Time 0.05
train_accuracy 77.344
Test: [0/79]	Loss 0.1130 (0.1130)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1291 (0.1141)	Accuracy 96.094 (98.039)
valid_accuracy 98.180
Test: [0/79]	Loss 0.1509 (0.1509)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0507 (0.1228)	Accuracy 100.000 (97.748)
valid_accuracy 98.020
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009577709715813398
* remain weight =  6.871896128762045 %
* best SA=98.02
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  6.871896128762045 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  941
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  8.58987016095255 %
Test: [0/391]	Loss 0.1129 (0.1129)	Accuracy 97.656 (97.656)
Test: [50/391]	Loss 0.1143 (0.1171)	Accuracy 97.656 (97.932)
Test: [100/391]	Loss 0.1351 (0.1205)	Accuracy 96.094 (97.927)
Test: [150/391]	Loss 0.1034 (0.1215)	Accuracy 96.875 (97.920)
Test: [200/391]	Loss 0.1267 (0.1209)	Accuracy 96.094 (97.967)
Test: [250/391]	Loss 0.0921 (0.1208)	Accuracy 99.219 (97.996)
Test: [300/391]	Loss 0.1865 (0.1209)	Accuracy 94.531 (97.973)
Test: [350/391]	Loss 0.1305 (0.1203)	Accuracy 96.875 (97.999)
valid_accuracy 97.982
[45005 44845  5103 37038 21445  5062  4476 28898 48161 37050  8072   500
   376 18904 11632  8761 45011 48356 37921 11693 30041 37109 37117 14804
 29930 38050  5000   588 11734 29006 37974  7920 11759 11234   544 24887
 44090  8731 48856 29055  7867 22470 40752 34542  8729 30356 48242 21436
 25466 43804 15862  8867   788 30184  8027 34520 30262  4438 37966 48706
 37276 25508 30142 38052  5013  8785 19369   635 19386  5155 22217 11196
  7758 14819 30123 43742 45018 48837 24870 22400 40704  8552  8639 14740
 30250 15766 45048 44758 15942 48881 28954 14790 44870 23538 22130  6578
 32888 18056 13428 20522 27426 39031 17216 25232 42482 26405 42447 35916
 44456 15494 10544  5790 49425 32934  9814 49416  5771 27502 15500 17209
 47342 42428  2410 35246 35234 47260 26522 26516 27241 27251 27252  5876
 42334 49501  5868  3370 47288 37502 47291 48594 31962 31958  3390 21852
 27320 39656 39074 23546 35970 26362 27514 31738 31591 23336 31576 19810
 39779 19808  5684 13658 33060 41464 17005 36104 17001 44484 13673  3634
 46154 27696 18190 27706  8898 13703 13713  2014 41438 38884  5908 31596
 37469  6670 26358 42555  2250  3532 47421 31712 37481 23422  9778 13538
 47428 23406 39734  5738 18130 31652 31650 17079 42624  5718  2148 47479
 31608 31606 42660 31474  6418  2622 40514  6050 32421 17804 48538 37552
 39482 10215 41843 32394 26896 10241 47094 17645 37601 12967 23870 46887
 20210 15434 32349 32348  2892 42042 17648 10243 41962 37588 12834 12836
 29573 10287 26785 47036 32511 32510 32508 32507 41907 10282 47008 41912
 23962  6102 35401 39461 39424 32475 17728 20110 46976 40511 32455  6014
 46857 40526 17608 17890 49904 20304 35652 24101 42180 35288 20018  3268
 23730  2707 20016 12663 27121 27122  5936 23710 10048 47217 12651 13185
 42234 27157 35710 42255 12692 46635 49890 39546  3184 24059 21952 23824
 12730 12729 13031 39311 20050 20048 10382 26636 32248 47159 47160 26629
 17540 40530 46770 20286 20287  5972 35624 42140 26622 13093 18220 24360
  1978 21164 22779 12000 22767 22766 30770 11992 43368 34703 47940 40066
 36653 22729 25800 25321   266 36730  9104 28492 29830 21226 29832 11949
 36760 37373 38365 24633 25854 38439 28282  4028 40008  4030  1988  1404
 43234 14230 34758 21124 30882 38409 21130 38399 15190 22824 18506 45532
 25868  7347  4650 10894 30833 34740 28525 22884  1143  7498  8966 40144
 21351 48065 16150 21355 22559 34010 43592 40654  7638 36875 14607  8940
 30452 30450 22531  8927 36936 45154 21395   892 14650 14653  8942 16173
 48397 11870 22679 37574 44133 22675 18598 30638 28559 28574  7530 15164
 22643 48975 48402 25330  1058 24730 30574  1047 45266  1032  7584 40976
 28654 34614 36866 33572 28260 11306 11288 10756  9471 33242  5536 41305
 38640 31216 42973  7058 28014 45888 26066 24492 29713 41290  9433 12196
 47710 16908 24391 45815 48638 16722 33130 31252  8200 42838 12302 37680
 46002 31374 16862 20784 37453 42866 37450 31344  3756 26134 27898  6974
  1920 45944 19752 15528 47651 12244 31275 41324  9569 41276 12184  5638
 33361 45686 47798  1512 29711 41196 16560  1492  8488 42772 34802 36522
  1482 25294  5430 49107  5422 24587 31461 45622 43198 24589 45617 18228
  6885 28175 48649 16684 28072  7114 37704 37427 33304  9390 20962 20970
 15579  1604 29391 11298 24526 38796 20976 47759 49153 14084 34841  1568
 18404 18382 31395  8104   132 40527  8347 34234 29803 18930  4955 15183
 44262  8470 48524 29237 37609 29365  4694    28  8532 44078 37557 15642
 25159 44424 12830  1662 27472 30870 38408 22838 26376 49082  6658 31741
 49083 30903 27522 16488  7293 30925 41540 46300 47844 17121 17112 45615
 27602 26392 42501 25863 28357 36268 23588 49488 31934 28471 36714 49026
 41618 12522  3456 14172 30764 41594 17213 47376 28413 33506 43328 38359
  5798 36642 32918 31840 47962 49109 45654 12325 44046 49285  5624 36363
 31197 38772 41396 47597 26079 49212 47600  3811 46015 10750  5554 41377
 36238 31366 42854 41366 45800  3682 16678 39810 25948 12416 16574 47475
 35062 47486 34829 13662  5679 27700 30983 18405 27722 36446 39801 34847
 47538  6848  9379 49164 33296  1634 18397  4153 31335 27254 39480 14896
  4446 37135  4428  3136 46906 43823   650 40824 43874 15948  8799 18776
  6197 15987  2901 14765 24036 14760 14756 26504 32372  8854 19330  4460
  4986   391  4562 10277 39423 26740 29180 10251 44753 15016 19328 11631
 26842  3118 39397 32427  8680 37198 32417 15826 40280   547  6130 24830
 12982 26940 27118 48930 48933 24722   966 35688 25688 46682 28710  6387
 48945 10038 13198 16192 16198 26560 49543  4221  7606  2694 27101  6428
 17592 38256  6246   832  7719 13026  6466 35574 46806  8883 35622 42312
  6448 11044 39256 49910 19494 48006 17420 33615 13831 13735 49573 27085
  6172 23770 17817 27026 31710 46288 17093 27292  9330 39875 22633 16084
 24812 30604 25975   506 12181 16698   180 14764 22203   886 24798 10297
 15450 43658 41904 21382 35310 36982  5174 44098 46828 14700 17591 29286
  6347 13138 16130 17958 17763  5912 48966 22607 10994 40977 49960 28652
 28672 34328  2676 25678 40948  5216  7732 26748 32323 37606 21900 35480
 32573  8670 20171 24006 30049 19244  3030 48288  4542 39425 25412 10256
 22169 23911 17739   584 16030 32342 33768 17747 15991  7784 25546 17819
 15402  8772 22200 10205 23886  7842 43826 12752 17382 18609 38700 33088
  7146  8226 14072  8480 43109  9344 46122 12377 38526 41218 49121 20641
 33412  7259  1444 22062  7264 36141 39940 49233  7010 45917  8202  5602
 23194 49264  5656 18322 45868 47689  7080  1940 12183 28078 16676 20882
  2576 36006  7270 25318 47340 18557  2426 24198 25799 26471 30751 22719
 24684 47290 45352 43454 34660 27248  2554  9098 41538 16376 38339 49090
 34048  8154 46316 37396 22083 39708 36686 25306  6636 36606 34750 38397
  1330 38370 14341 33448 24990]
[   48    67    80   172   178   180   282   394   500   544   558   588
   601   718   749   778   826   832   974  1021  1049  1148  1235  1259
  1330  1404  1492  1524  1586  1612  1824  1826  1920  1963  1999  2014
  2231  2338  2488  2554  2576  2694  2707  2764  2901  2924  2931  2958
  3136  3166  3447  3524  3570  3696  3810  3811  3998  4024  4028  4122
  4149  4156  4362  4428  4446  4529  4618  4625  4656  4692  4694  4752
  4822  4908  4955  5062  5065  5084  5262  5278  5314  5332  5340  5390
  5430  5482  5513  5536  5538  5624  5691  5758  5790  5798  5800  5821
  5868  5876  5912  5972  6027  6034  6050  6130  6142  6172  6197  6226
  6254  6269  6272  6362  6418  6466  6636  6905  6974  7005  7022  7058
  7078  7112  7233  7347  7530  7584  7599  7606  7638  7719  7768  7798
  7828  7891  7907  7920  7995  8031  8122  8200  8202  8287  8347  8480
  8552  8729  8731  8761  8847  8883  8884  8934  8945  8966  9002  9322
  9332  9344  9396  9471  9489  9550  9583  9627  9876  9932 10146 10156
 10203 10205 10207 10215 10221 10268 10277 10282 10297 10301 10321 10382
 10458 10524 10746 10756 10800 10884 10886 10944 11196 11288 11342 11504
 11631 11632 11643 11647 11702 11734 11759 11775 11781 11797 11882 11949
 11991 11992 12060 12078 12153 12236 12318 12325 12377 12576 12729 12764
 12768 12791 12834 12836 12868 12886 12898 12932 13026 13031 13138 13198
 13350 13394 13558 13604 13658 13662 13673 13719 13752 13829 13854 13996
 14084 14119 14142 14161 14222 14230 14281 14307 14314 14317 14440 14501
 14561 14574 14607 14650 14653 14698 14754 14756 14765 14790 14819 14866
 14896 14972 14982 15016 15094 15105 15121 15190 15239 15244 15412 15434
 15450 15574 15728 15749 15827 15848 15893 15948 15987 15988 15991 16173
 16192 16298 16408 16676 16760 16799 16919 16939 16951 16981 17005 17012
 17079 17112 17121 17130 17209 17234 17384 17494 17540 17591 17608 17645
 17648 17707 17739 17763 17817 17854 18056 18130 18166 18174 18190 18196
 18202 18212 18220 18222 18247 18322 18378 18382 18468 18482 18598 18684
 18832 18930 19096 19124 19146 19168 19211 19244 19302 19306 19322 19386
 19396 19412 19546 19630 19666 19714 19782 19804 19934 19960 20036 20054
 20110 20146 20196 20226 20322 20348 20350 20363 20522 20628 20663 20709
 20746 20773 20784 20962 21042 21058 21124 21130 21164 21302 21351 21355
 21445 21766 21791 21948 22183 22199 22200 22203 22217 22263 22320 22470
 22505 22597 22607 22622 22675 22719 22779 22838 22845 23128 23136 23140
 23340 23385 23406 23422 23424 23458 23538 23546 23614 23660 23762 23813
 23819 23824 23868 23962 24006 24014 24036 24059 24166 24210 24497 24526
 24557 24560 24587 24609 24613 24684 24760 24776 24820 24887 24990 24997
 25262 25287 25310 25327 25406 25466 25546 25648 25683 25799 25801 25854
 25868 25948 26029 26079 26275 26358 26444 26504 26515 26560 26613 26636
 26722 26740 26749 26756 26760 26842 26896 27026 27068 27085 27121 27143
 27157 27180 27426 27440 27472 27501 27502 27503 27522 27658 27722 27739
 27859 27964 27968 28066 28072 28149 28175 28183 28262 28279 28357 28368
 28391 28530 28561 28574 28640 28654 28684 28710 28844 28898 29041 29180
 29189 29238 29286 29350 29377 29434 29589 29594 29712 29713 29731 29750
 29803 29810 29821 29828 29868 29895 29905 29930 29940 30041 30085 30184
 30356 30446 30572 30598 30692 30751 30764 30794 30810 30811 30833 30856
 30903 30925 30962 31024 31100 31185 31198 31200 31216 31252 31262 31275
 31301 31366 31461 31474 31546 31576 31579 31591 31593 31596 31600 31650
 31652 31662 31710 31738 31840 31854 31895 31917 32110 32152 32256 32276
 32280 32372 32394 32409 32417 32421 32445 32475 32508 32510 32523 32573
 32747 32892 32908 32918 33088 33162 33182 33226 33242 33250 33304 33362
 33388 33506 33552 33615 33680 33906 33974 34048 34090 34122 34274 34330
 34402 34414 34496 34542 34660 34665 34771 34830 34847 34920 35051 35062
 35066 35128 35164 35234 35246 35360 35401 35410 35421 35480 35508 35622
 35688 35694 35710 35721 35777 35827 35873 35887 35916 36046 36065 36104
 36238 36282 36348 36357 36370 36606 36620 36653 36714 37023 37038 37048
 37062 37118 37124 37135 37247 37249 37276 37374 37396 37427 37450 37453
 37468 37507 37521 37551 37588 37589 37601 37609 37672 37719 37816 37838
 37900 37962 37966 37974 38050 38230 38256 38359 38365 38399 38401 38408
 38433 38439 38452 38459 38513 38680 38770 38772 38774 39031 39074 39160
 39256 39284 39297 39311 39327 39331 39355 39377 39378 39397 39423 39425
 39464 39488 39546 39582 39625 39683 39734 39801 39848 39875 39896 40076
 40104 40240 40386 40434 40636 40824 40909 40948 40976 41110 41116 41135
 41156 41199 41205 41218 41226 41270 41276 41284 41334 41366 41377 41396
 41424 41464 41538 41594 41616 41744 41894 41897 41907 41962 42042 42049
 42112 42140 42359 42384 42437 42566 42604 42606 42673 42687 42696 42702
 42711 42716 42772 42793 42802 42866 42897 42964 42968 43034 43174 43198
 43207 43230 43416 43484 43592 43790 43874 44054 44338 44340 44424 44432
 44484 44598 44618 44650 44758 44822 44915 44948 45348 45498 45529 45598
 45761 45772 45888 45917 45944 45962 45968 46056 46078 46146 46238 46240
 46247 46316 46423 46432 46435 46466 46476 46689 46780 46806 46814 46828
 46842 46860 46906 47155 47220 47234 47237 47247 47264 47387 47446 47488
 47538 47540 47603 47628 47646 47651 47680 47689 47720 47759 47836 47912
 47924 47928 47966 47979 48006 48091 48112 48154 48245 48288 48344 48402
 48454 48472 48538 48603 48618 48647 48706 48775 48795 48811 48856 48887
 48912 48927 48933 48975 49002 49014 49026 49062 49070 49082 49088 49090
 49121 49153 49164 49212 49226 49233 49285 49416 49464 49488 49543 49890
 49904 49960]
* Critical examples for prunings = 926
* Critical examples for training = 941
* PrAC images = 1527
******************************************
pruning state 12
* remain parameters = 18376.0
******************************************
* remain weight =  6.871896128762045 %
0.1
* training images number = 1527
Epoch: [0][0/12]	Loss 1.2638 (1.2638)	Accuracy 46.875 (46.875)	Time 0.08
train_accuracy 54.093
Test: [0/79]	Loss 2.5292 (2.5292)	Accuracy 11.719 (11.719)
Test: [50/79]	Loss 2.5951 (2.4255)	Accuracy 11.719 (15.211)
valid_accuracy 15.380
Test: [0/79]	Loss 2.4059 (2.4059)	Accuracy 17.969 (17.969)
Test: [50/79]	Loss 2.3756 (2.3976)	Accuracy 17.188 (15.640)
valid_accuracy 15.790
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/12]	Loss 0.9951 (0.9951)	Accuracy 55.469 (55.469)	Time 0.04
train_accuracy 58.612
Test: [0/79]	Loss 1.2580 (1.2580)	Accuracy 50.781 (50.781)
Test: [50/79]	Loss 1.3563 (1.2334)	Accuracy 44.531 (54.534)
valid_accuracy 55.060
Test: [0/79]	Loss 1.2303 (1.2303)	Accuracy 45.312 (45.312)
Test: [50/79]	Loss 1.0536 (1.2377)	Accuracy 68.750 (53.983)
valid_accuracy 54.610
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08652590215206146
0.010000000000000002
Epoch: [2][0/12]	Loss 0.9367 (0.9367)	Accuracy 70.312 (70.312)	Time 0.04
train_accuracy 63.261
Test: [0/79]	Loss 0.3977 (0.3977)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.4502 (0.4024)	Accuracy 88.281 (90.257)
valid_accuracy 90.160
Test: [0/79]	Loss 0.3876 (0.3876)	Accuracy 91.406 (91.406)
Test: [50/79]	Loss 0.2400 (0.4141)	Accuracy 96.094 (88.879)
valid_accuracy 89.710
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009904222562909126
0.010000000000000002
Epoch: [3][0/12]	Loss 0.7840 (0.7840)	Accuracy 70.312 (70.312)	Time 0.04
train_accuracy 63.982
Test: [0/79]	Loss 0.1928 (0.1928)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.2372 (0.2189)	Accuracy 94.531 (95.159)
valid_accuracy 95.080
Test: [0/79]	Loss 0.2041 (0.2041)	Accuracy 94.531 (94.531)
Test: [50/79]	Loss 0.0991 (0.2260)	Accuracy 99.219 (94.455)
valid_accuracy 95.050
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009468872100114822
0.0010000000000000002
Epoch: [4][0/12]	Loss 0.9154 (0.9154)	Accuracy 58.594 (58.594)	Time 0.10
train_accuracy 66.274
Test: [0/79]	Loss 0.1408 (0.1408)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1807 (0.1722)	Accuracy 94.531 (96.063)
valid_accuracy 96.080
Test: [0/79]	Loss 0.1640 (0.1640)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.0765 (0.1809)	Accuracy 100.000 (95.404)
valid_accuracy 96.110
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0009795385412871838
* remain weight =  5.497591695087656 %
* best SA=96.11
* find early bird tickets at epoch 5
Remove Pruning
Pruning with custom mask
* remain weight =  5.497591695087656 %
* record size = (60000, 5)
zero all unforgettable images out, rest number =  837
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  6.871896128762045 %
Test: [0/391]	Loss 0.1435 (0.1435)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1741 (0.1708)	Accuracy 94.531 (96.385)
Test: [100/391]	Loss 0.1630 (0.1719)	Accuracy 96.094 (96.473)
Test: [150/391]	Loss 0.1198 (0.1713)	Accuracy 97.656 (96.352)
Test: [200/391]	Loss 0.1502 (0.1697)	Accuracy 96.875 (96.323)
Test: [250/391]	Loss 0.1507 (0.1721)	Accuracy 97.656 (96.234)
Test: [300/391]	Loss 0.1680 (0.1722)	Accuracy 99.219 (96.234)
Test: [350/391]	Loss 0.1903 (0.1722)	Accuracy 93.750 (96.267)
valid_accuracy 96.232
[14740 14866 15579 22607 37062 37551 44098   601 29189  7498  8488 44424
 37050 15190 37900 37453 14760 37574 37048 44845  7920  8027  8552 15728
 44338 37276 14756 29594  7907 37672  7584 37921 15164 37038 37481 44484
  8072  7891 14790 15749 37198   718 22470 37507 29434 45011 44046 15239
 36982   547   266    28   558  7732 22183 14804 14896 15893 21766   178
  7530  7719 30142 37468 37124 22531 37502 38052  7842 22203 37521 21852
 37962 22200 15016 15494 15183 22199  7995 15500 29391  8532 37557 45048
 29803 15826 29711 38050 30356 37118 37117   376   544 22062 43874 24360
 47290 41912 39683 10321 10301  5684 12651 10282 47220  5718 35234 17747
 12692 35246 20054 10243 17645 24198 32445 32455 32475  2892 12576 42042
 10221 12730 32507 47159 26940 10241 10215 47340 47342  3118 19808 32908
 24560 47538 10524 32918 19810 18056 32934 24589 26522 26516 12325 26504
  5422 32421 32888 47479  3136 26722 32747 47387  5554 35066 39848  5536
 47428 35051 39801 49543 24492 24497 47421 49425 32417 17608 39378 24014
  2576 20286 42312 24006 13031 39355 46828 42334  6050 46806 20322 42359
 23962  2488  9932 27502 27501 31958 42501 42482 13198  6014 13185 42447
  2426 39284  6102 46770 42384 46689 27254 27251 27248 49910 49904 12898
 22622 42180 27118 39461 39482 32342 47008 32349  5876 32372 47036 35401
 12834 17540 20210 39397 46887 12982  2622 35480 12967 24101 24059 17494
 32248 20226  2676 32280  5936 10038  3390 47597 47603 11632 40976  4153
 48887 25799 48912 11044  4822 48245 48927 19306 48930 48945 25854 19328
 19330 25863 25975 41156 18557 48112 11797 11781 11631  3998 11759  4024
  4028 33615 11734 25868 33572 40434 19244 48856 18904 48524 40752  4562
 11288 11298 19124 25406  4438 34010 11306 40704 40654  4476  4529 24997
 25330 11196 48344 33768 48356  4221 48811 25683  4625  4692 40511 34234
 40526  4656  4650 25294 48397 48091 24990 34520 26358 19666 18220 12181
 33130 24722 26362 24730 18247 12153 34771 41464 33182 34750 47759 47798
  3524 41540 41618 12302 39940 24633 18130 41594 34829 47646 18166  5314
 34841 47689 34830 26392  5332 27503 41438 47844 49153 41284 41276  3810
 49107 33448 33361  5000 10894 11870 41196 48065  4986 49070 11882 40076
  5084 18405 33242 10750 49233 24812 49226  3682 49164 24830 34660 33304
 18382  5103 24870 11992 49212 23819 27122 27522 16574 28260 28262 36357
 28282 45868 36363 31100 21058 45815 23140 45800 28357 43207  1404 36446
 43328 28391 45761 21130  9104 28413 30983  7005 21164 30925 16376 38439
 38526 30903 38640 45944 13719 28014 13735 23406  6670 31374  9396 23385
  9379 28066 31335 28078 20882 31185  9344 16684  9330 16678 31275 46002
  1634  1612 45968 28183 31216 20970 31200 20976 23336 45654 21226 38401
 22824 30638 38230   966 14440  8799 22779 30574 22766 45352  8772 45348
 21436 36730 21445   886 16030  8731 36866 22675 43823 30452 43826   788
   778 15948 22643 36936 43742  8854  7264  7259 30856 14222 14230 28561
 16298 28574 30811 36606  7146 38359 30794  8942 30770 38339 30764  8927
 36653 30751 45532 28654 14317  8898 22884 45498 43592 16198  8884  1058
 16173 23424 46154 16560 42973 27898  6418 39031 31710 35887 13658  6428
  1963 46432 27696  6387 31738  2148 46466 27706 13394  1920 31546 31600
  1988 16951 31579 31606 46288 35916  9583 16981 20663 31650 31652  6448
 20641  1940 46476 31596 23770  1826 42606 20522 42566 13350 27968  6254
  6246 17112 42604  6272 17093  1824 31854 17216 10800 42772 14172  5972
 35694 25508 35688 10746 18322  8122 48538  7270 21952  7058 26134 30049
 21124 47720 37974 47710  8639  6974  2694 15942  8680 39423 34614 41305
 37588 25801 21355 11702 38256 42687  8031 39311  6347 18598 19396  2554
 14307 13428 43804 23730 36642 29055 36620 49090 44133 26066 38365   132
 27252  8966 47966 40530 27739 22838 48402 46857 38409  5908 12663 10277
 16908 31197  5868 38680 12416 23458 26636 28175 24587 31461 19960  5538
 29940 35128 17648 30262  5821 17890 16698  5602 42897 29365  9390 20050
 37109 36006 27121 13703 23422 31576 27514 44456 28710 48649 39708  5771
  8347 20110 38772 17382 48242  1032 11693 23824 37552 46078 47288 25412
 22083 17079 22633 48594 36686  4460 18609 22130  9433 47160 36714 28672
  4030 10994  7758  4446 15766 37469 47094 45154 48975  8670 17817 39582
 46122 40514 29713 29712 40527 38796 46682 23868  5790  8729 25306 28898
 17763 22679 12752 48706 25546 20048 32573 17819 20018 39256 44262 34328
  3030 43658 21382 26842 35310  6658  2410 30604 10297 26749  6130 17739
 13138 20016 29238 25678 20350 27292 27700 49026  2014 23538 42793 49285
   180 34758 45917  5216 44870  1330 10544 34740 32348 49264 24798 30184
 20171  7080 43454  2901   635  7010 42234 15450 46316  5912 37966 26471
  8480 32323 49416 14072 41538 18190 26376 47600 14764 24609 13538 44078
 34802 12184 12183 26515 23588 38408 28525 49960   588 33412 17958 10884
 41218 12830 43109  7638  1143 27722 35062  6578 20784 14700 13662 37450
 29573 47376 33506  4955 28652 26748 15402 17420 48006 31252 38397 40144
 45615 27085 41334 26560 12000 22169   650 12377   584 24526 21900 37427
 47486 38370 33362  8202  3268  1604 12836 26079 34048]
[   42    96   132 ... 49955 49960 49986]
* Critical examples for prunings = 1914
* Critical examples for training = 837
* PrAC images = 2359
******************************************
pruning state 13
* remain parameters = 14701.0
******************************************
* remain weight =  5.497591695087656 %
0.1
* training images number = 2359
Epoch: [0][0/19]	Loss 1.6169 (1.6169)	Accuracy 28.125 (28.125)	Time 0.08
train_accuracy 62.399
Test: [0/79]	Loss 2.0127 (2.0127)	Accuracy 28.906 (28.906)
Test: [50/79]	Loss 1.9860 (1.9086)	Accuracy 24.219 (31.510)
valid_accuracy 31.820
Test: [0/79]	Loss 1.8440 (1.8440)	Accuracy 34.375 (34.375)
Test: [50/79]	Loss 1.8891 (1.9035)	Accuracy 39.062 (31.143)
valid_accuracy 32.150
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/19]	Loss 0.6723 (0.6723)	Accuracy 72.656 (72.656)	Time 0.04
train_accuracy 70.793
Test: [0/79]	Loss 0.5797 (0.5797)	Accuracy 84.375 (84.375)
Test: [50/79]	Loss 0.5451 (0.5364)	Accuracy 88.281 (89.798)
valid_accuracy 90.170
Test: [0/79]	Loss 0.5079 (0.5079)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.4286 (0.5500)	Accuracy 95.312 (88.220)
valid_accuracy 89.130
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.09183049201965332
0.1
Epoch: [2][0/19]	Loss 0.7830 (0.7830)	Accuracy 72.656 (72.656)	Time 0.05
train_accuracy 71.089
Test: [0/79]	Loss 0.2912 (0.2912)	Accuracy 89.062 (89.062)
Test: [50/79]	Loss 0.2532 (0.3141)	Accuracy 92.188 (90.763)
valid_accuracy 91.170
Test: [0/79]	Loss 0.2396 (0.2396)	Accuracy 95.312 (95.312)
Test: [50/79]	Loss 0.1950 (0.3350)	Accuracy 99.219 (90.165)
valid_accuracy 90.360
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.075913205742836
0.1
Epoch: [3][0/19]	Loss 0.6879 (0.6879)	Accuracy 72.656 (72.656)	Time 0.04
train_accuracy 72.743
Test: [0/79]	Loss 0.2687 (0.2687)	Accuracy 89.844 (89.844)
Test: [50/79]	Loss 0.3750 (0.3296)	Accuracy 82.031 (88.434)
valid_accuracy 89.230
Test: [0/79]	Loss 0.3732 (0.3732)	Accuracy 88.281 (88.281)
Test: [50/79]	Loss 0.2543 (0.3537)	Accuracy 90.625 (88.143)
valid_accuracy 88.400
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06434936821460724
0.010000000000000002
Epoch: [4][0/19]	Loss 0.6684 (0.6684)	Accuracy 72.656 (72.656)	Time 0.05
train_accuracy 76.558
Test: [0/79]	Loss 0.1397 (0.1397)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1695 (0.1823)	Accuracy 97.656 (96.538)
valid_accuracy 96.600
Test: [0/79]	Loss 0.1758 (0.1758)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1123 (0.1892)	Accuracy 100.000 (96.400)
valid_accuracy 96.390
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.009523161686956882
0.010000000000000002
Epoch: [5][0/19]	Loss 0.4934 (0.4934)	Accuracy 84.375 (84.375)	Time 0.11
train_accuracy 77.279
Test: [0/79]	Loss 0.1398 (0.1398)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1588 (0.1764)	Accuracy 98.438 (96.814)
valid_accuracy 96.830
Test: [0/79]	Loss 0.1718 (0.1718)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1034 (0.1842)	Accuracy 100.000 (96.523)
valid_accuracy 96.500
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.007890620268881321
0.0010000000000000002
Epoch: [6][0/19]	Loss 0.5267 (0.5267)	Accuracy 81.250 (81.250)	Time 0.04
train_accuracy 78.720
Test: [0/79]	Loss 0.1351 (0.1351)	Accuracy 100.000 (100.000)
Test: [50/79]	Loss 0.1566 (0.1717)	Accuracy 98.438 (96.875)
valid_accuracy 96.860
Test: [0/79]	Loss 0.1692 (0.1692)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1013 (0.1800)	Accuracy 100.000 (96.615)
valid_accuracy 96.710
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008162710582837462
* remain weight =  4.398148148148151 %
* best SA=96.71
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  4.398148148148151 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  1025
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  5.497591695087656 %
Test: [0/391]	Loss 0.1809 (0.1809)	Accuracy 96.094 (96.094)
Test: [50/391]	Loss 0.1806 (0.1793)	Accuracy 96.875 (96.446)
Test: [100/391]	Loss 0.1962 (0.1817)	Accuracy 94.531 (96.450)
Test: [150/391]	Loss 0.1590 (0.1819)	Accuracy 96.094 (96.461)
Test: [200/391]	Loss 0.1821 (0.1807)	Accuracy 95.312 (96.471)
Test: [250/391]	Loss 0.1431 (0.1806)	Accuracy 98.438 (96.536)
Test: [300/391]	Loss 0.2091 (0.1809)	Accuracy 96.094 (96.558)
Test: [350/391]	Loss 0.2009 (0.1812)	Accuracy 92.188 (96.532)
valid_accuracy 96.528
[ 1920 44925 35234 ... 37962 17958  3030]
[    3    67    80 ... 49946 49955 49960]
* Critical examples for prunings = 1647
* Critical examples for training = 1025
* PrAC images = 2295
******************************************
pruning state 14
* remain parameters = 11761.0
******************************************
* remain weight =  4.398148148148151 %
0.1
* training images number = 2295
Epoch: [0][0/18]	Loss 1.4258 (1.4258)	Accuracy 45.312 (45.312)	Time 0.07
train_accuracy 62.789
Test: [0/79]	Loss 2.6014 (2.6014)	Accuracy 10.156 (10.156)
Test: [50/79]	Loss 2.6801 (2.5191)	Accuracy 7.031 (10.662)
valid_accuracy 10.880
Test: [0/79]	Loss 2.5540 (2.5540)	Accuracy 11.719 (11.719)
Test: [50/79]	Loss 2.4746 (2.4923)	Accuracy 11.719 (11.458)
valid_accuracy 11.500
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/18]	Loss 0.7520 (0.7520)	Accuracy 67.969 (67.969)	Time 0.04
train_accuracy 67.843
Test: [0/79]	Loss 0.8536 (0.8536)	Accuracy 75.781 (75.781)
Test: [50/79]	Loss 0.8986 (0.8506)	Accuracy 73.438 (75.046)
valid_accuracy 75.330
Test: [0/79]	Loss 0.8751 (0.8751)	Accuracy 67.969 (67.969)
Test: [50/79]	Loss 0.6108 (0.8539)	Accuracy 92.188 (74.418)
valid_accuracy 74.750
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.09778080135583878
0.1
Epoch: [2][0/18]	Loss 0.6912 (0.6912)	Accuracy 74.219 (74.219)	Time 0.05
train_accuracy 69.978
Test: [0/79]	Loss 0.3399 (0.3399)	Accuracy 90.625 (90.625)
Test: [50/79]	Loss 0.4921 (0.4574)	Accuracy 86.719 (85.493)
valid_accuracy 85.670
Test: [0/79]	Loss 0.4779 (0.4779)	Accuracy 85.156 (85.156)
Test: [50/79]	Loss 0.2084 (0.4466)	Accuracy 92.969 (85.616)
valid_accuracy 86.220
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.07482356578111649
0.1
Epoch: [3][0/18]	Loss 0.7496 (0.7496)	Accuracy 67.188 (67.188)	Time 0.04
train_accuracy 71.416
Test: [0/79]	Loss 0.3234 (0.3234)	Accuracy 87.500 (87.500)
Test: [50/79]	Loss 0.2597 (0.2697)	Accuracy 90.625 (89.782)
valid_accuracy 90.280
Test: [0/79]	Loss 0.2446 (0.2446)	Accuracy 93.750 (93.750)
Test: [50/79]	Loss 0.1214 (0.2652)	Accuracy 95.312 (90.456)
valid_accuracy 90.720
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.06308986991643906
0.010000000000000002
Epoch: [4][0/18]	Loss 0.7782 (0.7782)	Accuracy 71.094 (71.094)	Time 0.11
train_accuracy 73.246
Test: [0/79]	Loss 0.1138 (0.1138)	Accuracy 99.219 (99.219)
Test: [50/79]	Loss 0.1811 (0.1543)	Accuracy 95.312 (96.170)
valid_accuracy 96.210
Test: [0/79]	Loss 0.1286 (0.1286)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.0599 (0.1561)	Accuracy 100.000 (96.140)
valid_accuracy 96.670
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.00935294572263956
0.010000000000000002
Epoch: [5][0/18]	Loss 0.6733 (0.6733)	Accuracy 69.531 (69.531)	Time 0.04
train_accuracy 75.251
Test: [0/79]	Loss 0.1207 (0.1207)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1472 (0.1415)	Accuracy 97.656 (96.829)
valid_accuracy 96.930
Test: [0/79]	Loss 0.1231 (0.1231)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.0405 (0.1424)	Accuracy 100.000 (96.921)
valid_accuracy 97.360
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008332625031471252
0.0010000000000000002
Epoch: [6][0/18]	Loss 0.6499 (0.6499)	Accuracy 75.781 (75.781)	Time 0.06
train_accuracy 76.383
Test: [0/79]	Loss 0.1143 (0.1143)	Accuracy 98.438 (98.438)
Test: [50/79]	Loss 0.1497 (0.1388)	Accuracy 97.656 (96.798)
valid_accuracy 96.970
Test: [0/79]	Loss 0.1209 (0.1209)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.0387 (0.1405)	Accuracy 100.000 (97.120)
valid_accuracy 97.500
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0006802142597734928
* remain weight =  3.5185933105965406 %
* best SA=97.5
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  3.5185933105965406 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  1120
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  4.398148148148151 %
Test: [0/391]	Loss 0.1111 (0.1111)	Accuracy 99.219 (99.219)
Test: [50/391]	Loss 0.1046 (0.1277)	Accuracy 96.875 (97.488)
Test: [100/391]	Loss 0.1023 (0.1307)	Accuracy 98.438 (97.370)
Test: [150/391]	Loss 0.0794 (0.1325)	Accuracy 100.000 (97.279)
Test: [200/391]	Loss 0.1252 (0.1320)	Accuracy 96.875 (97.213)
Test: [250/391]	Loss 0.1092 (0.1341)	Accuracy 98.438 (97.168)
Test: [300/391]	Loss 0.1402 (0.1340)	Accuracy 96.094 (97.171)
Test: [350/391]	Loss 0.1334 (0.1337)	Accuracy 97.656 (97.231)
valid_accuracy 97.192
[ 2764 17542  8670 ... 13138 27522 23588]
[   28   132   138 ... 49923 49933 49960]
* Critical examples for prunings = 1424
* Critical examples for training = 1120
* PrAC images = 2084
******************************************
pruning state 15
* remain parameters = 9409.0
******************************************
* remain weight =  3.5185933105965406 %
0.1
* training images number = 2084
Epoch: [0][0/17]	Loss 1.5333 (1.5333)	Accuracy 39.844 (39.844)	Time 0.08
train_accuracy 53.743
Test: [0/79]	Loss 2.3204 (2.3204)	Accuracy 11.719 (11.719)
Test: [50/79]	Loss 2.3283 (2.2583)	Accuracy 10.938 (14.599)
valid_accuracy 14.870
Test: [0/79]	Loss 2.2212 (2.2212)	Accuracy 16.406 (16.406)
Test: [50/79]	Loss 2.2649 (2.2416)	Accuracy 17.188 (15.395)
valid_accuracy 15.290
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
0.1
Epoch: [1][0/17]	Loss 0.9585 (0.9585)	Accuracy 63.281 (63.281)	Time 0.04
train_accuracy 59.693
Test: [0/79]	Loss 1.3908 (1.3908)	Accuracy 55.469 (55.469)
Test: [50/79]	Loss 1.4382 (1.3234)	Accuracy 52.344 (61.229)
valid_accuracy 61.430
Test: [0/79]	Loss 1.3703 (1.3703)	Accuracy 63.281 (63.281)
Test: [50/79]	Loss 1.2077 (1.3363)	Accuracy 67.188 (60.723)
valid_accuracy 61.100
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.10330534726381302
0.1
Epoch: [2][0/17]	Loss 1.0136 (1.0136)	Accuracy 61.719 (61.719)	Time 0.05
train_accuracy 62.476
Test: [0/79]	Loss 0.3716 (0.3716)	Accuracy 92.188 (92.188)
Test: [50/79]	Loss 0.4067 (0.3618)	Accuracy 89.844 (92.999)
valid_accuracy 93.430
Test: [0/79]	Loss 0.3643 (0.3643)	Accuracy 92.969 (92.969)
Test: [50/79]	Loss 0.2297 (0.3736)	Accuracy 99.219 (92.877)
valid_accuracy 93.520
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.08268678933382034
0.010000000000000002
Epoch: [3][0/17]	Loss 0.7916 (0.7916)	Accuracy 66.406 (66.406)	Time 0.05
train_accuracy 66.795
Test: [0/79]	Loss 0.1780 (0.1780)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.2217 (0.2045)	Accuracy 94.531 (95.803)
valid_accuracy 96.030
Test: [0/79]	Loss 0.2051 (0.2051)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1549 (0.2139)	Accuracy 96.094 (95.312)
valid_accuracy 95.760
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.011265809647738934
0.010000000000000002
Epoch: [4][0/17]	Loss 0.8597 (0.8597)	Accuracy 62.500 (62.500)	Time 0.04
train_accuracy 67.850
Test: [0/79]	Loss 0.1687 (0.1687)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.2069 (0.1752)	Accuracy 96.094 (96.124)
valid_accuracy 96.470
Test: [0/79]	Loss 0.1806 (0.1806)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1148 (0.1921)	Accuracy 98.438 (95.833)
valid_accuracy 96.250
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.008927622810006142
0.0010000000000000002
Epoch: [5][0/17]	Loss 0.8414 (0.8414)	Accuracy 68.750 (68.750)	Time 0.05
train_accuracy 68.186
Test: [0/79]	Loss 0.1608 (0.1608)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1950 (0.1663)	Accuracy 96.094 (96.706)
valid_accuracy 97.010
Test: [0/79]	Loss 0.1777 (0.1777)	Accuracy 96.094 (96.094)
Test: [50/79]	Loss 0.1084 (0.1820)	Accuracy 98.438 (96.140)
valid_accuracy 96.640
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008502497803419828
0.0010000000000000002
Epoch: [6][0/17]	Loss 0.7518 (0.7518)	Accuracy 69.531 (69.531)	Time 0.05
train_accuracy 69.098
Test: [0/79]	Loss 0.1589 (0.1589)	Accuracy 97.656 (97.656)
Test: [50/79]	Loss 0.1908 (0.1632)	Accuracy 96.094 (97.013)
valid_accuracy 97.240
Test: [0/79]	Loss 0.1753 (0.1753)	Accuracy 96.875 (96.875)
Test: [50/79]	Loss 0.1046 (0.1779)	Accuracy 98.438 (96.415)
valid_accuracy 96.870
Apply Unstructured L1 Pruning
Remove Pruning
Pruning with custom mask
* current-mask-distance is = 0.0008502497803419828
* remain weight =  2.8147998563992083 %
* best SA=96.87
* find early bird tickets at epoch 7
Remove Pruning
Pruning with custom mask
* remain weight =  2.8147998563992083 %
* record size = (60000, 7)
zero all unforgettable images out, rest number =  1220
Setting up mnist dataset
build model: resnet20
Test: [0/391]	Loss 0.0053 (0.0053)	Accuracy 100.000 (100.000)
Test: [50/391]	Loss 0.0541 (0.0300)	Accuracy 97.656 (98.958)
Test: [100/391]	Loss 0.0151 (0.0314)	Accuracy 100.000 (98.994)
Test: [150/391]	Loss 0.0178 (0.0311)	Accuracy 99.219 (98.991)
Test: [200/391]	Loss 0.0388 (0.0311)	Accuracy 97.656 (98.989)
Test: [250/391]	Loss 0.0259 (0.0308)	Accuracy 98.438 (98.967)
Test: [300/391]	Loss 0.0654 (0.0310)	Accuracy 97.656 (98.975)
Test: [350/391]	Loss 0.0358 (0.0307)	Accuracy 98.438 (99.012)
valid_accuracy 99.006
Pruning with custom mask
* remain weight =  3.5185933105965406 %
Test: [0/391]	Loss 0.1543 (0.1543)	Accuracy 98.438 (98.438)
Test: [50/391]	Loss 0.1482 (0.1620)	Accuracy 96.875 (96.768)
Test: [100/391]	Loss 0.1912 (0.1685)	Accuracy 96.094 (96.689)
Test: [150/391]	Loss 0.1504 (0.1688)	Accuracy 96.875 (96.772)
Test: [200/391]	Loss 0.1978 (0.1672)	Accuracy 94.531 (96.914)
Test: [250/391]	Loss 0.1425 (0.1676)	Accuracy 99.219 (96.962)
Test: [300/391]	Loss 0.2109 (0.1673)	Accuracy 95.312 (97.002)
Test: [350/391]	Loss 0.1681 (0.1668)	Accuracy 96.094 (97.035)
valid_accuracy 96.974
[ 5896 47597 32747 ...  2426  5638 22531]
[   10    30   172 ... 49946 49955 49985]
* Critical examples for prunings = 1406
* Critical examples for training = 1220
* PrAC images = 2145
forgetting numbers:  [2775, 1477, 916, 709, 677, 896, 1194, 825, 658, 696, 781, 941, 837, 1025, 1120, 1220]
pie num:  [0, 419, 539, 707, 1132, 2793, 580, 588, 1046, 1083, 1664, 926, 1914, 1647, 1424, 1406]
model sparsities:  [20.00014958415605, 36.000044875246815, 48.799961108119426, 59.04011847065159, 67.23209477652127, 73.78575061329505, 79.02867528271406, 83.22301501824927, 86.57856159875546, 89.26284927900437, 91.41012983904744, 93.12810387123795, 94.50240830491235, 95.60185185185185, 96.48140668940346, 97.1852001436008]
